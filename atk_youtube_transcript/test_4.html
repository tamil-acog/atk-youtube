

points between cohere and openai.

<h1><strong>What are Cohere embeddings?</strong></h1>

<p>Today, we're going to take a look at Cohere's multilingual embedding model. For those of you that are not aware of Cohere, they are kind of similar to OpenAI in that they are essentially a service provider of large language models and all of the services that come with that. Now, right now they are not as well known as OpenAI, which is understandable. OpenAI has been around for a bit longer, but Cohere is actually a really good company that offers a lot of really good tooling that is actually very much comparable to what OpenAI offers. And that's actually the first thing I want to look at here. I just want to show you a few comparison points between Cohere and OpenAI.</p>
<a href="https://www.youtube.com/watch?v=1aequYq5yTo&t=7s&t=0.0s"> What are Cohere embeddings</a>


<h1><b>Cohere v OpenAI on cost</b></h1>

<p>Points between Cohere and OpenAI in terms of embedding models. Okay, so we're going to first take a look at the costs between these two opening eyes. So, Premiere embedding model right now is auto 002 comes out to this much per 1000 tokens. Cohere doesn't have a like per 1000 tokens for the cost, it actually goes with one dollar per 1000 embeddings. What does one embedding mean? Well, basically every call or every chunk of text that you ask out here to embed that is one embedding. So, upon embedding the maximum size of that is actually just over four thousand tokens. So, if you're maxing out every embedding is in you are sending 4 000 tokens to every embedding call then that means you would be getting this comparable price here which is actually half price which is pretty good. Now, if we kind of translate this into something that's a bit more understandable we have like 13 paragraphs is roughly about a thousand tokens these are the prices all right. So, with order with OpenAI it's one dollar per thirty two and a half thousand pairs so here is actually one dollar per 65 000 paragraphs which is really good but there is obviously a catch which is you know this thing up here or this one dollar per 1000 embeddings right the chances are you're probably not going to use four thousand embeddings with every call to Cohere so 2000 tokens well that's probably like 26 paragraphs if you're embedding 26 paragraphs at a time realistically you're probably gonna do much less right. So, if let's say you're going for more like a thousand tokens which I think is more realistic then obviously the price of go here is actually double the price of OpenAI in this instance so it kind of depends on what you're doing there as to whether you are throwing a load of text into your embeddings or not so I think the costs are pretty comparable Cohere can be cheaper but it can also be more expensive according to this logic anyway.</p>

<p>Okay so one thing I miss very quickly is the on-prem solution that Cohere offers so we have it here essentially you can run your own AWS instance and in the time that it would take you this is assuming you're running at 100 and the time date would tell you to encode 1 billion paragraphs if you use coheres on-prem solution you would end up paying two and a half thousand dollars it's also a lot quicker and you know there are all the other benefits as well but I thought when we're talking about cost we should definitely include that in there so you know it depends essentially embedding size actually you know this is a good indicator of how much it's going to cost you so it's actually under costs the higher your embedding size the more storage you need to store all of your embeddings after you've created them right so the embedding size smaller is cheaper soccer here is half the size of open AI in this case so you know long term you would probably actually be saving money with cohere with this embedding size if you're storing a lot of vectors so you know that's definitely something to consider if you consider this with the embedding cost initially you know maybe you're actually saving money with cohere even if you're just embedding like a thousand tokens or even 500 tokens at a time.</p>
<a href="https://www.youtube.com/watch?v=1aequYq5yTo&t=7s&t=48.84s"> Cohere v OpenAI on cost</a>


not.

<h1><b>Cohere v OpenAI on performance</b></h1>

<p>Long term, you're probably going to end up saving money now performance. So, this is kind of hard to judge because this is a single benchmark that knows where iron is put together and okay. I mean, cohere for sure is coming out on top here. It's kind of hard to say it again like whether this is representative across a board or not, but nonetheless, the two models that are comparable here. Okay, here's multilingual model and open ai's auto 002 model which is English and this is an English search task. So, it's pretty interesting that opening eyes best English language model is comparable to his multilingual model because his English model is better. And then there's a coheri rankers. This is an embedding model. It's at like a imagine you retrieve all of your items or you get two chunks of text and you feed them into like a transform model and compare them directly. It takes basically a lot slower but generally speaking it will be more accurate. So, I think they are pretty interesting results. It seems like they're kind of on par like open Ai and cohere are very on par but it seems like cohere at least from you know what I've seen here is slightly ahead of open AI in terms of performance on that single benchmark which is not the best comparison in all fairness but also slightly cheaper in the long run because of the embedding size but again like the everything here is so close that it's going to depend a lot on on your particular use case so it's not that Co here is better than open AI it's just that in some cases they probably are better and in some cases they're not.</p>
<a href="https://www.youtube.com/watch?v=1aequYq5yTo&t=7s&t=293.699s"> Cohere v OpenAI on performance</a>


<h1><b>Implementing Cohere Multilingual Model</b></h1>


<p>Probably cheaper as well, so that's definitely something to consider. Now, how do we actually use Cohere for embeddings? So, we're going to be focusing on the Cohere multilingual model, and in this example, we're going to be running through. It's not really my example. I've taken this example from Nose Rhymers based on a webinar that we are doing together. He's basically put all this together, and I've just kind of reformatted it in a way so that I can show you how it works and also show you kind of focus on the multilingual search component of Cohere and show you how it works. Let's, you know, let's just jump straight into it, right?</p>

<p>So, the first thing we need to do is our pip installs. So, we have hook and face datasets here. We're getting data from there. Go here and Pinecone client. We're using the GRPC client so that we can upset things faster. We'll see how to use that soon. Now, I actually have a couple of notes here. So, what a couple of things to point out with Cohere's multilingual model is that it supports more than 100 languages. I think the benchmarks that they've tested it on covered 16 of those languages or something around there.</p>
<a href="https://www.youtube.com/watch?v=1aequYq5yTo&t=7s&t=416.58s"> Implementing Cohere multilingual model</a>


<h1><b>Data prep and embedding</b></h1>

<p>Of course, you can create embeddings for longer terms of text, okay? And this is the data that we're going to be using. It's some straight data from Wikipedia that Nils put together, I believe, and just hosted under coher on I can face datasets. So let's have a look at these. For now, we're just going to look at the English and Italian, and we're going to see how we would put those and create a search with them. And then what I'm going to do is switch across to an example where we have way more data in the database, and that covers, I think, nine languages, but it's pretty interesting. So this is what day looks like. We just have like some text in the middle that's what we're going to be encoding, right? So if we're embedding these chunks one at a time, maybe it would be more expensive using cohere, but I think in reality, we could put a lot more of these together. So we could put together like five of these chunks or more, and it should work pretty well. So okay, let's go down here. You need a go here API key. So to get that, you would go to here. So you type in dashboard dot coher.ai, okay? And you'll probably have to log in if you haven't already logged in to go here, and then you go over to the left here, and you will find some API keys. From there, you take your API key, and you just put it in here. Okay, I have my API key stored already in a variable called cohere API key. Cool. Then this is how you would embed something, right? So we have a list of text that we would like to embed, and we just pass them to this Co dot embed. So Co is just a client that we've initialized up here. So co.embed text, and then you have your model. This is the only multilingual model that cohere offers at the moment, but I mean, like, if you compare that to open AI right now, they just offer English models. So I think they've taken the lead with that, which is pretty cool. Pull embeddings from response. So okay, we create our embedding six gives us like response, and it has a lot of information in there, but all we need are the embeddings, right? So we're just starting those out, and obviously, dimensionality of those embeddings, which is going to be 768. So that's the dimensionality, and then we have two of those Vector embeddings there, right? So we have two 768 dimensional vectors because we have two sentences. All right, now that's how we would use coheres embedding model, but before we move on to actually creating.</p>
<a href="https://www.youtube.com/watch?v=1aequYq5yTo&t=7s&t=499.259s"> Data prep and embedding</a>


<h1><b>Creating a vector index with Pinecone</b></h1>

<p>Out our index where we're going to store all of those embeddings we need to initialize an index so we're going to be using Vex database called pine cone for this. Now pine cone again we need API key which we can get from over here again it's free so app.pinecone.io. I'll just copy paste that. Okay cool so come over here I can already see I have a couple of indexes in here uh if this is your first time using Pinecone it will be empty and that's fine because we're going to create the index in the code but what you do need is your API key all right so your API key is here you copy that take over into your notebook and you would paste it here now again I've stored mine in a variable then you also have your Environ now your environment is next to the API key in the console right so here Us East one gcp your environment is not necessarily going to be the same as mine so you should check that okay great so that has initialized and then we come down here and what we're going to do here is initialize an index which is where we're going to store all of these embeddings now you give your index a name it doesn't matter what you call it okay you can call it whatever you want but there are a few things that are important here that we should not change so dimension Dimension is the dimensionality of your embedding so it's coming from Echo here right this is where I mentioned before there's the the price advantage of using cohere when dimensionality is lower like 768 it's going to be cheaper to store all of your vectors if you are needing to pay for that storage so we need that and our index needs to know this value okay so it needs to know the expected dimensionality the vectors we're putting into it then we have our metric which is dot products this is needed by coheirs multilingual model if you look on the I think the about page for the multilingual model it will say you need to use that product and then these here you can you can actually leave them empty the default values for these are also okay but I thought I'd put them in there so S1 is basically the storage optimized pod for Pinecone which means you can put in about five million vectors in here for free without paying anything and then there's also P1 which is like the speed optimized version which enables you to put in around 1 million uh vectors for free okay and then pods is the number of those pods you need so if you needed 10 million vectors we would say okay we need two parts here cool but we just need one we're not paying that mission now so we would you know we'd run that then we connect the index we use this grpc index which is we can also use index so we could also use this but grpc index is just more stable and it's also faster so we're doing that and then we're going to describe the index stats so we're going to see what's in there now I already created the index before so for you when you're running through this first time this will actually say zero for me I've already added things in there that's why it's at.</p>
<a href="https://www.youtube.com/watch?v=1aequYq5yTo&t=7s&t=678.54s"> Creating a vector index with Pinecone</a>


<h1><b>Embedding and indexing everything</b></h1>

<p>200,000 and 100. Now, with the embedding model and Vector index, we can move on to actually indexing everything. So basically, we're just going to loop through our dataset and we're going to do what we just did. So we're going to embed things with coherent and then what we're going to do is with those embeddings, we're going to add them into Pyco. Right, actually, I don't think I showed you how we do that, but it's really simple. It's actually just this line here, but let me explain what we have here. So batch size is the number of items that we're going to send to go here and then upset into Pinecone at any one time. The Lang limit, so this is a number of records from each language that we would like to include that would like to embed and add to Pine Cone. We have our data here, so I'm just formatting this so that it's a bit easier later on when we get to this bit here and errors. And this is just so we can store a few errors because every now and again, we might hit one, and I'll explain why it's not necessary, but there are ways to avoid it basically that I'm not that hard, but for simplicity's sake, I haven't included them in here. So here, I'm just saying, you know, don't go over the Lang limit, and then we're going through English and Italian one at a time. We get a relevant batch from our data, which we've created here. So it's actually just see the iterable of the data first English and Italian. We extract text from that. We create our embeddings using that text. Then we just create some IDs. This is just an ID variable that was in the data up at the top here, ID, and also including text in there as well, okay? And then what we do is we create this metadata list of dictionaries. Now, each dictionary is going to contain some text, a title from the record, the URL of the record, and also the language, so English or Italian. And then what we do is we add everything like this, okay? So it's pretty straightforward. There's nothing too complicated going on there. The one thing is that I have added in there is occasionally, so you know, we saw those the text earlier on. It was, you know, they were pretty short Trump's attacks, but for some reason, not all of them are like this. It's kind of like a messy dataset. So some of them are actually quite long, and they actually exceed the metadata limit in Pinecone, which is 10 kilobytes per Vector. So basically, we can add up to around 10 kilobytes of text with per Vector in Pinecone, but some of them go over that, and they will throw an error. So I'm actually, for now, I'm just skipping those, but in reality, what you do is you chunk those larger chunks of text into smaller chunks and then just add them individually or just saw your text somewhere else. It doesn't have to go into Pine again. Right now, I've already run this. I'm not going to run it again, and yeah, I can just come down to here. I can run this. We have our describe index.</p>
<a href="https://www.youtube.com/watch?v=1aequYq5yTo&t=7s&t=889.68s"> Embedding and indexing everything</a>


<h1><strong>Making multilingual queries</strong></h1>

<p>And look the same as it did before for me okay cool now what I want to do so this is a more I think more interesting part is searching so to search through uh what we do is we take a query we embed it and then we so embed is exactly the same as what we did before with cohere and then we query with that embedding xq here and we return the top three most similar items and then we want to include metadata which is going to contain our text title and a couple of other things the URL is pretty important and then we return it in this kind of format we include this this is pretty good idea from those we include the translate URL that will just allow us so when we're getting Italian results or any other language results we just click on this it will take us to Google Translate and we can see what it actually says so let's run this and we can try both of these that I'm not even sure if they work that well because we don't have that much data in here but we can try okay I I don't know any uh okay yeah so number three here so this is you know he's famous in Italy but I think less famous outside of Italy um so if we go to here we see translation and you can see okay it's on the most important and prestigious personalities and fight against the mafia he was killed by kosa Nostra together with his wife and so on and so on right so he he's super famous in Italy but if you look on Wikipedia for him in English I think there's it mentions a little bit about him but there isn't really that much information there so that's why we're getting you know we're just getting like Italian results here and then if we go for this one as well so this is another one I think in the English Wikipedia there's like a paragraph about about this but then if you go to the Italian Wikipedia there is a ton of these now in this I don't have yeah I don't I don't I don't have enough data in here so let's let's switch across to the larger data set and I'll show you uh what the results look like that which are much better okay I can ask about oh this one here so what is the mafia Capital case okay and we get Mafia Capital here and if you go to translate you can see yes that is you know that is the thing that I was talking about and then if we go to Wikipedia here I just want to point out okay so you get all of this text which is tons if we go to the English version okay so I'm searching in Google here Mafia capitali what what do we get right we get this literally three paragraphs so basically nothing so you can see why it would be bringing the Italian stuff here rather than the or why being able to search the Italian stuff is useful even if you're you're speaking English now another one we're going to ask what is Aaron cheat it's Aaron Chino but I'm going to spell it wrong just to point out the fact that it can actually handle that maybe Aaron G oh I slot it no no I I did get it right okay so this is wrong uh the one I did before was was actually correct I kind of half expected to go wrong anyway right so it's uh we can go on here see what it says so Aaron Gino is specialty of Sicilian cuisine it's very nice if you ever had the chance to try it you should have this with a pizza so arancino pizza and fury the zuka it's amazing it's like my favorite meal okay so let's try one more who is Emma Maroney is that right yes Okay so go to here and I I don't actually know who this is</p>
<a href="https://www.youtube.com/watch?v=1aequYq5yTo&t=7s&t=1094.4s"> Making multilingual queries</a>


<h1><strong>Final throughts on Cohere and OpenAI</strong></h1>

<p>So, I hope this is correct. It's apparently this person, okay. So, that's it for this introduction to Cohere. I feel like it was a bit longer than I had intended it to be, but that's fine. I'm hoping that it was at least useful, and we went through a lot of things there. So, yeah, I just wanted to share this. It's an alternative to OpenAI. I'm not saying it's necessarily better. I'm not saying it's necessarily cheaper. I think that is very much going to depend on your use case, what you're doing, and many other factors, right?</p>

<p>You can train these models. For example, if you're able to train them, then you know you're probably going to get some pretty good performance as well. And I suppose like one big factor here is actually the multilingual aspect of this model. At the moment, OpenAI doesn't have any multilingual models or not they're actually trained to do that. Some of them, I think, can handle multilingual queries relatively well, but then they haven't been trained for that, and you know this can be relatively problematic, you know, especially when you're dealing with multinational companies or just companies that are not American or English or Australian as well. I'm not gonna forget you, any in the rest of the world speaks different languages, so having this multilingual model is pretty good. </p>

<p>So, yeah, I mean, this is still very early days for Cohere. I'm pretty excited. I know they have a lot planned, and that'll be really interesting to see, but for now, I think we'll leave it there. I hope all this has been useful and interesting, so thank you very much for watching, and I will see you again in the next one. Bye.</p>
<a href="https://www.youtube.com/watch?v=1aequYq5yTo&t=7s&t=1366.62s"> Final throughts on Cohere and OpenAI</a>
