

<h1><strong>What are Cohere embeddings?</strong></h1>

<p>Today, we're going to take a look at Cohere's multilingual embedding model. For those of you that are not aware of Cohere, they are kind of similar to OpenAI in that they are essentially a service provider of large language models and all of the services that come with that. Now, right now, they are not as well known as OpenAI, which is understandable. OpenAI has been around for a bit longer, but Cohere is actually a really good company that offers a lot of really good tooling that is actually very much comparable to what OpenAI offers. And that's actually the first thing I want to look at here. I just want to show you a few comparisons.</p>
<a href="https://www.youtube.com/watch?v=1aequYq5yTo&t=7s&t=48.84s"> What are Cohere embeddings</a>


<h1><strong>Cohere v OpenAI on cost</strong></h1>

<p>Points between Cohere and OpenAI in terms of embedding models. Okay, so we're going to first take a look at the costs between these two opening eyes. So, Premiere embedding model right now is auto 002 comes out to this much per 1000 tokens. Cohere doesn't have a like per 1000 tokens for the cost, it actually goes with one dollar per 1000 embeddings. What does one embedding mean? Well, basically every call or every chunk of text that you ask out here to embed that is one embedding. So, upon embedding the maximum size of that is actually just over four thousand tokens. So, if you're maxing out every embedding is in you are sending 4 000 tokens to every embedding call then that means you would be getting this comparable price here which is actually half price which is pretty good. Now, if we kind of translate this into something that's a bit more understandable we have like 13 paragraphs is roughly about a thousand tokens these are the prices all right. So, with order with OpenAI it's one dollar per thirty two and a half thousand pairs so here is actually one dollar per 65 000 paragraphs which is really good but there is obviously a catch which is you know this thing up here or this one dollar per 1000 embeddings right the chances are you're probably not going to use four thousand embeddings with every call to Cohere so 2000 tokens well that's probably like 26 paragraphs if you're embedding 26 paragraphs at a time realistically you're probably gonna do much less right. So, if let's say you're going for more like a thousand tokens which I think is more realistic then obviously the price of go here is actually double the price of OpenAI in this instance so it kind of depends on what you're doing there as to whether you are throwing a load of text into your embeddings or not so I think the costs are pretty comparable Cohere can be cheaper but it can also be more expensive according to this logic anyway.</p>

<p>Okay so one thing I miss very quickly is the on-prem solution that Cohere offers so we have it here essentially you can run your own AWS instance and in the time that it would take you this is assuming you're running at 100 and the time date would tell you to encode 1 billion paragraphs if you use coheres on-prem solution you would end up paying two and a half thousand dollars it's also a lot quicker and you know there are all the other benefits as well but I thought when we're talking about cost we should definitely include that in there so you know it depends essentially embedding size actually you know this is a good indicator of how much it's going to cost you so it's actually under costs the higher your embedding size the more storage you need to store all of your embeddings after you've created them right so the embedding size smaller is cheaper soccer here is half the size of open AI in this case so you know long term you would probably actually be saving money with cohere with this embedding size if you're storing a lot of vectors so you know that's definitely something to consider if you consider this with the embedding cost initially you know maybe you're actually saving money with cohere even if you're just embedding like a thousand tokens or even 500 tokens at a time.</p>
<a href="https://www.youtube.com/watch?v=1aequYq5yTo&t=7s&t=293.699s"> Cohere v OpenAI on cost</a>


not.

<h1><b>Cohere v OpenAI on performance</b></h1>

<p>Long term, you're probably going to end up saving money now performance. So, this is kind of hard to judge because this is a single benchmark that knows where iron is put together and okay. I mean, cohere for sure is coming out on top here. It's kind of hard to say it again like whether this is representative across a board or not, but nonetheless, the two models that are comparable here. Okay, here's multilingual model and open ai's auto 002 model which is English and this is an English search task. So, it's pretty interesting that opening eyes best English language model is comparable to his multilingual model because his English model is better and then there's a coheri rankers. This is an embedding model. It's at like a imagine you retrieve all of your items or you get two chunks of text and you feed them into like a transform model and compare them directly. It takes basically a lot slower but generally speaking it will be more accurate. So, I think they are pretty interesting results. It seems like they're kind of on par like open Ai and cohere are very on par but it seems like cohere at least from you know what I've seen here is slightly ahead of open AI in terms of performance on that single benchmark which is not the best comparison in all fairness but also slightly cheaper in the long run because of the embedding size but again like the everything here is so close that it's going to depend a lot on on your particular use case so it's not that Co here is better than open AI it's just that in some cases they probably are better and in some cases they're not.</p>
<a href="https://www.youtube.com/watch?v=1aequYq5yTo&t=7s&t=416.58s"> Cohere v OpenAI on performance</a>


<h1><b>Implementing Cohere Multilingual Model</b></h1>


<p>Probably cheaper as well, so that's definitely something to consider. Now, how do we actually use Cohere for embeddings? So, we're going to be focusing on the Cohere multilingual model, and in this example, we're going to be running through. It's not really my example. I've taken this example from Nose Rhymers based on a webinar that we are doing together. He's basically put all this together, and I've just kind of reformatted it in a way so that I can show you how it works and also show you kind of focus on the multilingual search component of Cohere and show you how it works. Let's, you know, let's just jump straight into it, right?</p>

<p>So, the first thing we need to do is our pip installs. So, we have hook and face datasets here. We're getting data from there. Go here and Pinecone client. We're using the GRPC client so that we can upset things faster. We'll see how to use that soon. Now, I actually have a couple of notes here. So, what a couple of things to point out with Cohere's multilingual model is that it supports more than 100 languages. I think the benchmarks that they've tested it on covered 16 of those languages or something around there.</p>
<a href="https://www.youtube.com/watch?v=1aequYq5yTo&t=7s&t=499.259s"> Implementing Cohere multilingual model</a>
