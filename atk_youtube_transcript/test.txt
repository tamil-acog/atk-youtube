

Sparse and dense vector search 

 

In information retrieval, vector embeddings represent documents and queries in a numerical vector format. That means that we can take some texts, which could be web pages from the internet in the case of Google or maybe product descriptions in the case of Amazon, and we can encode it using some sort of embedding method or model. And we all get something that looks like this. So, we have now represented our text in a vector space. Now, there are different ways of doing this, and sparse and dense vectors are two different forms of this.

Comparing sparse vs. dense vectors 

representation each with their own pros and their own cons. Typically, when we think of sparse vectors, things like TF-IDF and BM25, they have very high dimensionality and they contain very few non-zero values. So, the information within those vectors is very sparsely located. And with these types of vectors, we have decades of research looking at how they can be used and how they can be represented using compact data structures. And there are naturally many very efficient retrieval systems designed specifically for these vectors. On the other hand, we have dense vectors. Dense vectors are lower dimensional, but they are very information-rich. And that's because all of the information is compressed into this much smaller dimensional space. So, we don't have these non-zero values that we would get in a sparse vector. And hence, all of the information is very densely packed. And hence why we call them dense vectors. These types of vectors are typically built using neural network type architectures like Transformers. And through this, they can represent more abstract information like the semantic meaning behind some text. 

When it comes to sparse embeddings, the pros are typically faster retrieval, a good baseline performance, we don't need to do any model fine-tuning, and we also get to do exact matching of terms. Whereas on the cons, we have a few things as well. So, the performance cannot really be improved significantly over the baseline performance of these algorithms. They also suffer from something called the vocabulary mismatch problem, and we'll talk about that in more detail later. And it also doesn't align with the human-like thought of abstract concepts that I described earlier. Naturally, we have a completely different set of pros and cons when it comes to dense factors. On the pros, we know that dense vectors can outperform sparse vectors with fine-tuning. We also know that using these, we can search with human-like abstract concepts. We have great support of multi-modality, so we can search across text, images, audio, etc. And we can even do cross-modal search, so we can go from text to image or image to text or whatever you can think of. But of course, there are also the cons. We know that in order to outperform sparse vector embeddings or even get close to sparse fracture embeddings in terms of performance, we very often require training. And training requires a lot of data, which is very difficult to find when we find ourselves in low resource scenarios. These models also do not generalize very well, particularly when we are moving from one domain with the very specific terminology to another domain with completely different terminology. These embeddings also require more compute and memory to build and store and search across than sparse methods. We do not get any exact match search, and it's kind of hard to understand why we're getting results.

Using sparse and dense together 

Some of the time, it's not very interpretive. Ideally, we want a way of getting the Best of Both Worlds. We want the pros of dents and the pros of sparse and just. We don't want any of these cons, but that's very hard to do. There have been some Band-Aid Solutions. One of those is to perform two-stage retrieval. In this scenario, we have two stages to retrieve and rank relevant documents for a given query. In the first stage, our system would use a sparse retrieval method to search through and return relevant documents from a very large set of candidate documents. These are then passed on to the second stage, which is a re-ranking stage. This uses a dense embedding model to re-rank from that smaller set of candidate documents which one it believes is the most relevant, using its more human-like semantic comprehension of language.

There are some benefits to this. First, we can apply the sparse method to the full set of documents, which makes it more efficient to actually search through those. After that, we can re-rank everything with our Dennis model, which is naturally much slower, but we're dealing with a smaller amount of data. Another benefit is that this re-ranking stage is detached from the retrieval system, so we can modify one of those stages without affecting the other. This is particularly useful if we have multiple models that take, for example, the output of the sparse retrieval stage. So that's another thing to consider. However, of course, this is not perfect. Two stages of retrieval and re-ranking can be slower than using a single-stage system that uses approximate nearest neighbor search algorithms. And of course, having two stages within the system is more complicated, and there are naturally going to be many engineering challenges that come with that. We're also very reliant on that first Sage retriever. If that first stage retriever doesn't perform very well, then there's nothing we can do with the second stage re-ranking model. Because if it is just being given a load of rubbish results, it's just going to re-rank rubbish results, and the final result will still be rubbish. So they're the main problems with this, and ideally, we want to solve that. We want to do that by improving single-stage systems. Now, a lot of work has been put into improving single-stage retrieval systems. A big part of research.

What is SPLADE? 

SPLADE has been in building more robust and learnable sparse embedding models, and one of the most promising models within this space is known as SPLADE. Now, the idea behind the sparse lexical and expansion models is that a pre-trained model like BERT can identify connections between words and sub-words, which we can call word pieces or terms, and use that knowledge to enhance our sparse fact embeddings. This works in two ways. It allows us to measure the relevance of different terms, so the word "the" will carry less significance in most cases than a less common word like "orangutan." The second thing it helps us with is it enables learnable term expansion, where term expansion is the inclusion of alternative but relevant terms beyond those that are found in the original sentence or sequence. 

Now, it's very important to take note of the fact that I said learnable term expansion. The big advantage of SPLADE is not a can-do term expansion that is something that has been done for a while, but they can learn term expansions. In the past, term expansion could be done with more traditional methods, but it required rule-based logic and rule-based logic. Someone would have to write that, and this is naturally time-consuming and fundamentally limited because you can't write rules for every single scenario in human language. 

Now, by using SPLADE, we can simply learn these using a Transformer model, which is, of course, much more robust and much less time-consuming for us. Another benefit of using a context om transform model like BERT is that it will modify these term expansions based on the context based on the sentence that's being input. So it won't just expand the word "rainforest" to three different words. It will expand the word "rainforest" to many different words that entirely depends on the context or the sentence that was fed in with, and this is one of the big benefits.

Vocabulary Mismatch Problem

 

Of attention models like Transformers, that is very context aware. Now, term expansion is crucial in minimizing a very key problem with sparse embedding methods, and that is the vocabulary mismatch problem. 

Now, the vocabulary mismatch problem is the very typical lack of overlap between a query and the documents that we are searching for. It's because we think of things in abstract ideas and concepts, and we have many different words in order to explain the same thing. It's very unlikely that the way that we describe something when we're searching for something contains the exact terms, the exact words that this relevant information contains.

How SPLADE works (transformers 101) 

This is just a side effect of the complexity of human language. Now, let's move on to SPLADE and how SPLADE actually builds these sparse embeddings. Now, it's actually relatively easy to grasp what is happening here. We first start with the transform model like BERT. Now, these transform models use something called Mass language modeling in order to perform their pre-training on a ton of text data. Not all transform models use this, but most do. Now, if you're familiar with BERT and mass language modeling, that's great. If not, we're going to just quickly break it down. 

So, starting with BERT, it's a very popular transform model and like all transform models, its core functionality is actually to create information-rich token embeddings. Now, what exactly does that mean? Well, we start with some text like "orangutans are native to the forests of Indonesia and Malaysia." With a transform model like BERT, we would begin by tokenizing that text into BERT-specific subword or word-level tokens, and we can see that here. So, we're using the Hugging Face Transformers Library. We have this tokenizer object here. This is what's going to handle the tokenization of our text. So, we have the same sentence I described before, "orangutans are native to the rainforest of Indonesia and Malaysia," and we convert it into these tokens, which is what you can see here. 

Now, these are just the token IDs, which are integer numbers, but each one of these represents something within our text. So, here, for example, this 2030 probably represents "orangutan," and the 5654 here maybe represents the "s" at the end of "orangutans." And they can be word-level or subword-level like that. Now, these are just the numbers. Let's have a look down here, and we can actually see how our words are broken up into these token IDs or tokens. So, we convert those IDs back into human-readable tokens, and we'll see, okay, we have this "CLS" token. That is a special token used by BERT. We'll see that at the start of every sequence tokenized by tokenizer. And then we have "orangutans," so it's actually split between four tokens, and we can see the rest of the sentence there as well. 

Now, why do we create these tokens and these token IDs? Well, that's because these token IDs are then mapped to what is called an embedding matrix. The embedding matrix is the first layer of our Transformer model. Now, in this embedding matrix, we will find design-learned vector representations that literally represent the tokens that we fed in within a vector space. So, the vector representation for the token "rainforest" will have a high proximity because it has a high semantic similarity to the vector representations for the token "jungle" or the token "forest," whereas it will be further away in that vector space from somewhat less related tokens like "native" or "V." 

From here, the token representations of our original text are going to go through several encoder blobs. These blobs encode more and more contextual information into each one of these token embeddings. So, as we progress through all of these encoder blocks, the embeddings are basically going to be moved within that vector space in order to consider the meaning within the context of the sentence it appears in, rather than just the meaning of the token by itself. And after all this progressive iteration of encoding more contextual information into our embeddings, we arrive at the Transformer's output layer. Here, we have our final information-rich vector embeddings. Each embedding represents the early token, but obviously with that context encoded into it. 

This process is the core of BERT and every other Transformer model. However, the power of Transformers comes from the considerable number of things for which these information-rich embeddings can be used. Typically, what will happen is we'll add a task-specific head onto the end of the transform model that will transform these information-rich embeddings.

correct.

Masked language modeling (MLM)

vector embeddings into something else like sentiment predictions or sparse vectors. The mass language modeling head is one of the most common of these task-specific helps because it is used for pre-training most Transformer models. This works by taking an input sentence. Again, let's use the "Orangutans are native to the forests of Indonesia and Malaysia" example. We will tokenize this text and then mask a few of those tokens at random. This masked token sequence is then passed as input to Bert. At the other end, we actually feed in the original unmasked sequence to the mass language modeling head. What will happen is Bert and the mass language modeling head will have to adjust their internal weight in order to produce accurate predictions for the tokens that have been masked. For this to work, the mass language modeling head contains 30,522 Apple tokens, which is the vocabulary size of the Bert base model. So, that means we have an output for every possible prediction for every possible token prediction, and the output as a whole acts as a probability distribution over this entire vocabulary. The highest activation across that probability distribution represents the token that Bert and the mass language modeling head have predicted as being correct.

How SPLADE builds embeddings with MLM 

The token behind that mass token position, now at the same time, we can think of this probability distribution as a representation of the words or tokens that are most relevant to a particular token within the context of The Wider sentence. With that, what we can do with SPLADE is take all of these distributions and aggregate them into a single distribution called The Importance estimation. The importance estimation is actually the sparse vector produced by SPLADE, and that is done using this equation here. This allows us to identify relevant tokens that do not exist in the original sequence. For example, if we mask the word "rainforest," we might return high predictions for the words "jungle," "land," and "forest." These words and their associated probabilities would then be represented in SPLADE-built sparse vector. And that doesn't mean we need to mask everything. The predictions will be made relevant to each token, whether it is must or not. So, in the end, all we have to input is the unmasked sequence, and what we'll get is all of these probabilities distributions for similar words to whatever has been input based on the sentence and the context. Now, many transform models are trained with mass language modeling, which means they are a huge number of models that have already got these Master language modeling weights, and we can actually use that to fine-tune those models as SPLADE models.

need to know about where SPLADE doesn't work so well. Something that we will cover in another video.

Now, let's have a quick look at where SPLADE works kind of less well. So, as we've seen, SPLADE is a really good tool for minimizing the vocabulary mismatch problem. However, there are, of course, some drawbacks that we should consider compared to other sparse methods retrieval. Which SPLADE is very slow. There are three primary reasons for this. First, the number of non-zero values in SPLADE query and document vectors is typically much greater than in traditional sparse vectors because of that term expansion. And sparse retrieval systems are rarely optimized for this. Second, the distribution of these non-zero values also deviates from the traditional distribution expected by most sparse retrieval systems, again causing slowdowns. And third, SPLADE vectors are not natively supported by most sparse retrieval systems, meaning that we have to perform multiple pre and post-processing steps, weight discretization, and other things in order to make it work, if it works at all. And it, again, it's not optimized for that.

Fortunately, there are some solutions to all of these problems. For one, the authors of SPLADE actually address this in a later paper that minimizes the number of non-zero values in the query vectors. And they do that with two steps. First, they improved the performance of SPLADE document encodings using Max pooling rather than the traditional pooling strategy. And second, they limited the term expansion to the document encodings only, so they didn't do the query expansions. And thanks to the improved document encoding performance, dropping nodes, query expansions still lesions with better performance than the original SPLADE model.

And then, if we look at the final two problems, that two and three, these can both be solved by using the pi convector database. Two is solved by pine cone's retrieval engine being designed to be agnostic to data distribution. And for number three, pine cone supports real valued sparse vectors, meaning SPLADE vectors are supported natively without needing to do any of those weird things in pre-processing, post-processing, or discretization.

Now, with all of that, I think we have covered everything we need to know about where SPLADE doesn't work so well, something that we will cover in another video.

Implementing SPLADE in Python 

 

Could possibly cover in all this on sunsplate. Now, let's have a look at how we would actually implement SPLADE in practice. Now, we have two options for implementing SPLADE. We can do it directly with hook and face transformers and PyTorch, or with a high-level abstraction using the official SPLADE library. We'll take a look at doing both, starting with a home face and PyTorch.

SPLADE with PyTorch and Hugging Face 

 implementation just so we can understand how it actually works. Okay, so first we start by just installing a few prerequisites. So we have displayed Transformers and PyTorch. And then what we need to do is install this. And then what we need to do is initialize the tokenizer, very similar to the bird tokenizer we initialized earlier, and the auto model for Mast and LM. So this is mass language modeling. So we're going to be using the Naver splade model here, and we just initialize all of that. Okay, and we have one pretty large chunk of text here. So this is very domain specific, so it has a lot of very specific words in there that a typical Dent embedding model would probably struggle with unless it has been fine-tuned on data containing these exact same terms. So we'll run that, and what we do is we tokenize everything so that will give us our token IDs that you saw earlier, and then we process those through our model to create our logic output, which is what we will see in a moment this here. Okay, so as we saw before, those logits will be each one of them contains our probability distribution over the 30.5 000 possible tokens from the vocabulary, and we have 91 of those. Now, the reason we have 91 of those is because from our tokens here, we have actually had 91 input tokens. So if we have a look at tokens input IDs dot shape, we'll see that there was a 91 input in there, so that will change depending on how many input tokens we have. Now, from here, what we're going to do is take these output Logics, and we want to transform them into a sparse Vector. Now, to do that, we're going to be using the formula that you saw earlier to create the importance estimation, and if we run that, we'll get a single probability distribution which represents the actual sparse Vector from split, and we can have a look at that vector, and we'll see it as mostly zeros in there. There are a few values, but very few. So what I'm going to do now is first I want to just ignore this bit. We're going to come down to here, and we're going to create a dictionary format of our sparse Vector. So we run this, and there's a few things I want to look at here. So a number of non-zero values that we actually have is 174, and all of them are now contained within this sparse dictionary. Okay, so these are the token IDs, and these are the weights or the relevance of each one of those particular tokens. Now, we can't read any of these token IDs, so similar to before, what we're going to do is convert those into actual human-readable tokens. So to do that, we'll need to run this, and then we come down here, and we're going to convert them into a more readable format. Okay, and we can see what it believes is important is all of these values. So we've sorted everything here, so that's why the numbers have changed here, and we can see that most importantly, it's seeing like programmed death cell.

Using the Naver SPLADE library 

 

Lattice, so a lot of very relevant words within that particular domain. Now, if we come a little bit further down, we can also see how to do that using the navisblade library. So, for that, we would have to pip install Spade. We did that at the top of the notebook, so we don't need to do it again. We're going to be using the max aggregation, so this is using the max pooling method. Run this again using the same model ID here because it's also downloading the model from hooking face Transformers, and what we do is we set torch and no grabs. So, this is saying we don't want to update any of the model weights because we're not doing fine-tuning. We're just performing inference, EG prediction here, and we just pass into the naval model out tokens which we built using the tokenizer earlier on. From there, we need to extract the d-rep tensor, and we'll squeeze that to remove one of the dimensions that is unnecessary, and we can then have a look. We have 30.5 000 Dimensions here, so this is our probability distribution or importance estimation, and that is obviously our sparse vector. What we can do is actually use what we've done so far in order to compare different documents. So, let's take a few of these. Our program cell death, no, no, no, this is the original text, and then the ones below here are just me attempting to write something that is either relevant or not relevant that uses a similar type of language. So, we can run that. We'll encode everything. We're going to use the pi torch and hooking face Transformers method, but either way, it will both of these will produce the same result, whether you use that or the actual displayed library, and what we'll get is three of these importance estimations, the splayed vectors, and then what we can do is calculate cosine similarity between them. So, here I'm just going to initialize a zeros array that is just to store the similarity scores that we're going to create using this here. So, we run that. Let's have a look at similarity, and we can see that obviously these in the diagonal here, this is where we're comparing each of the vectors to itself, so it scores pretty highly because obviously they're the same, but then the ones that we see as being the most similar other than the, you know, themselves is sentence zero and sentence one. So, this one here, if we come up to here, so basically these two here are being viewed as the most similar, and if we read those, we can see that they are in fact, uh, much more similar. They have a lot more overlap in terms of the terms, but it's not just about the terms that we see here, but also the terms I produce from the term expansion as well. So, that's how we would compare everything. That's how we would actually use splade to create embeddings and to.

What's next for vector search?

Actually, compare those sparse vectors as well using cosine similarity. Now, that's it for this introduction to learn sparse embeddings with splade.

Now, using Spade, we can represent text with more efficient sparse vector embeddings that help us, at the same time, deal with the vocabulary mismatch problem whilst enabling is like matching and drawing from some of the other benefits of using sparse vectors. But of course, there's still a lot to be done, and there's more research and more efforts looking at how to mix both dense and sparse vector embeddings using things like hybrid search, as well as things like splade. And using both of those together, we can actually get really cool results. So, I think this is just one step towards making vector search and information retrieval way more accessible because we no longer need to fine-tune all these really big models in order to get the best possible performance. But we can use things like hybrid search and things like splayed in order to really just improve our performance with very little effort, which is a really good thing to see. But that's it for this video. I hope everything we've been through is interesting and useful, but for now, that's it. So, thank you very much for watching, and I will see you again in the next one. Bye.