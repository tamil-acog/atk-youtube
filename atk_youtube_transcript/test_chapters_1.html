
<img src="0.jpg" alt="Not Applicable" />

<h1><strong>Introducing Eustis and his work on Protein MPNN</strong></h1>

<p>Hi everyone, it's my pleasure today to introduce Eustis. Eustis is a postdoc at the Baker Lab at the University of Washington. He obtained his bachelor's and Master's in mathematics and his PhD in biophysics, all from the University of Cambridge. Today, Eustis will talk about his work on protein MPNN. I'm sure all of us have seen the work recently come out in science, and we're all very excited to hear what you've got for us. So, I think we're going to keep questions for the end, but feel free to put them in the chat. In the meantime, take it away, Eustis. Thank you for the introduction.</p>

<p>So, I'll be presenting on party MPN, and please note down the questions and then ask them at the end. First of all, I'd like to thank all protein MPN team. So, there's lots of people who've been working on it, some both creating database training models and also doing experimental work. Usually, when I present the work, I go into the details how the project started and present all the sort of like nasty bits, but I'm just going to reverse and explain what is part of the MPN and present experimental data first, and then we will dive into some of the metrics.</p>

<p>The protein MPNN is the model to map protein backbone geometry into protein sequences. So, this structured a sequence and sequence of structure mapping, and we all know sort of alpha Falls and Rec centerfold predicting from a sequence structure. So, some of the experimental results one.</p>
<a href="https://www.youtube.com/watch?v=aVQQuoToTJA&t=4.38s">hi everyone it's my pleasure and today</a>

<img src="1.jpg" alt="Not Applicable" />

<h1><strong>Hallucinating Symmetric Protein Assemblies</strong></h1>

<p>Of the papers that came out at the same time, were partly mpnn is called hallucinating symmetric protein assemblies and in this case this is a figure from the paper the so Lucas puzzle and Alexi in our lab they've been playing with hallucinating or creating generating proteins using Alpha fold so in this case with the interest in it cyclic proteins so in this for example three copies of the protein of the exactly the same sequence. I introduced initially and then we can use R4 to predict the structure and impose some losses so for example ask the model to be very um very certain so high pldt and PTM but also introduced a cycling loss and then do Markov chain Monte Carlo mcmc to try to mutate some sequences and keep doing this Loop until we get really the metrics like we want the and then at the end I will show that these designs need to be redesigned with 14 npn and some sort of model that Maps structure the sequence because the original design did not work and if we look at the sort of the solubility so the how much soluble yield there was from different designs so this original Markov chain Monte Carlo hallucination for these homologous those who have the solubility results on guest median is about 10 milligrams per liter but then so a bunch of them are really low yields so they sort of say insoluble proteins they aggregated redesigned the same exact backbone supporting the impended and increase the soluble yield quite a lot this is a log scale so now we're talking about hundreds even close to thousands of um of milligrams per liter um also from these uh oligomeric designs they we got Crystal structures so this is showing seven Crystal structures some of the homo dimers and uh trimers and so on and the gray are the models and the salt structures are colored and there's some interesting designs the beta sheets and Alpha helices wrapping around so that's quite exciting and they all match quite well the design structures um I guess I should say that all of these were so I'll probably mention before later but the redesign sequence of Courtney and pin and again we predicted by Alpha fold to make sure that there's a match between what we try to design and what we redesigned reporting in pnn also we can hallucinate these like large structures so these different rings of different symmetries of c18 6 so there's a six units and there's internal symmetry of 18 and this showing at chromium a negative stain microscopy results so that's quite exciting there's lots of big structures and our show property and pnm you can redesign these symmetric proteins making sure that the sequence stays symmetric um in the in the in the I guess in the uh Institute for protein design we also have the Kings lab they're working on different nanoparticles so in this case uh Robert de Haas redesigned some of the tetrahedral nanoparticles so in this case the original design was then the Rosetta uh class design and the interface was not forming this particle was not forming together but then redesigned the party and K N quite similar backbones that fail initially from earlier papers now give really nice Crystal structures that match and these particles are really stable so that's quite exciting um</p>
<a href="https://www.youtube.com/watch?v=aVQQuoToTJA&t=106.439s">Hallucinating Symmetric Protein Assemblies</a>

<img src="2.jpg" alt="Not Applicable" />

<h1><b>Incorporating these Short Linear Motives into Genova Design Protein Scaffolds</b></h1>


One of the other examples is that who are they in our lab was trying to design incorporating these short linear motives into Genova design protein scaffolds, so the idea is that for example if this sa issue domain is to bind this native peptide, so we want to come up with the protein structure this de Nova small peptide that would support this green structure. So originally it was designed with Rosetta remodel their design and packing, but there was no binding signal. But then redesigning the protein opinion whole uh this orange structure keeping the green structures the given native so trying to support it provided binding. And then just to test whether some of these threads is actually supporting the structure, they were modifications made from spark Gene to aspartate and it indeed was supporting this small green structure. The more papers coming using coming out they're using that are using protein in pnm so this is one example from the uh from Nate Bennett it's called improving the neuroprotein design uh using deep learning. 

So some of the some of the failure modes for Designing uh proteins is that if we're trying to design a binder for a Target sometimes the binder itself doesn't form so this type of one failure but sometimes also it doesn't bind so it's a sidewalk to failure and there's some examples sort of showing metrics in this case the metric is Alpha fold success of the predicted align error using different methods like Rosetta design protein and pnn and also people try to run protein and pnn and run fast relax to move the backbone a little bit and then run protein and pin and again and you can see that the third method of running approaching independent and fast relax in Cycles is increasing this Alpha false success rate at least sort of tricking alcohol to think that those are good binders and also in this paper there's some benchmarking results showing that the previous results Alpha fold prediction line error can be used to discriminate better binders from the worst binders. 

Another paper that you supporting the opinion is our lab is related to the conclusion hallucinating this symmetric oligomers but in this case think about creating proteins with pockets so these pockets would be used for binding different small molecules so the same idea of Designing these oligomers but then redesigning the protein and pen and removing the symmetry so allowing these three units to be of different sequence so the sequence would become different so they wouldn't be perfect homologous but they would have different genes but the they call it pseudo symmetric so the backbone is symmetric but the sequence is not symmetric um so it's always I guess fun to do a little demonstration so I'm gonna show this hugging face created by Simon and try to predict this um this top seven so this is the Nova design party in one qos this figure was made for the ipd so instantly property and design celebrating 10 years so we can try to redesign this sequence and then predict the buffer fold um so this is this is this hugging face setup there's also a GitHub code where it has maybe slightly more options how to make but this is a very simple setup so in this case we can either upload a pdb or write the uh write the code so one qis was the code and if we go to the settings we can choose which sequencer designs so this design there's only one chain a I'm sorry we'll let's say design for sequences which is the sampling temperature is to go small sampling temperature we can choose what sort of model you use and I'll talk about these different models train a different amount of noise so let's use the default and then we can just run the model and design sequences so it is quite fast that's it we we this is what this was the original sequence and then now we redesigned four sequences and the model outputs the temperature that were designed at uh what is the score so this is a negative log probability so lower the better than the sequencer probably what is the match between the output sequence and the input sequence what was the model name so we have about 40 45 sequence similarity from the input.
<a href="https://www.youtube.com/watch?v=aVQQuoToTJA&t=341.58s">Incorporating these Short Linear Motives into Genova Design Protein Scaffolds</a>

<img src="3.jpg" alt="Not Applicable" />

<h1><b>Structural Addition</b></h1>


<p>Native crystal structure to the design sequences and now we can do structural addition. So, we're going to run Alpha fold three recycles on all of the sequences and it's going to take about 30 seconds. So, I'll go back to this later to see what are the what is the match between the input and the outputs. Um, so if we continue I'll share my slide one second. <span>[Music]</span></p>

<p>Okay, so while this prediction is running I'm going to continue the describing the model. So, we have a structure, the sequence model and some of the analogies that often people think in protein machine learning are these sort of I guess computer related image image generation or text. So, in this case I'm sort of showing the this task of mapping from protein backbone into the sequence and then from the sequencer to the backbone is an analogy between mapping an image describing it as in a caption. So, in this case this is a small cactus wearing a straw hat and a neon sunglasses in the Sahara Desert and then using the text to generate the image. So, in this case this image was generated by Imogen giving this text prompt and generating the image. So, this is sort of like a generator model that will produce many many solutions. In this case, this protein name pianon is more like mapping an image into the text but the distributions are quite different. In this case it's a 2d go to pixels in probably in a protein world it's 3D coordinate of the atoms and from this sort of like a translation perspective the number of residues are matching in the coordinates and also in the residue so we have a matching lens and there's only 20 layers available so somewhat simpler maybe task the task for prediction structure is quite difficult and it requires homology and other.</p>
<a href="https://www.youtube.com/watch?v=aVQQuoToTJA&t=605.519s">Structural Addition</a>

<img src="4.jpg" alt="Not Applicable" />

<h1><b>Problem Statement</b></h1>

<p>Information, uh, so the problem statement, what sort of was the idea behind protein MPN is to come up to the model that can sample sequences that are highly likely. We could model multiple trains, we could fix parts of the chain, and we provide uncertainty about the samples. So in this case, this could be an example of two chains, one chain here and one chain here, and we want to maybe fix everything and just redesign the interface, so just redesigned the red parts, giving all the rest. And the model we'd like to output the sequence and also the probabilities of the sequence experience that when you gave I the one at 9am was 42 and then you added more. So the examples would be one-sided binded design, uh, homologous design, enzyme design, and so on. Uh, the training data was collected by Yvonne and schenko, so it's a similar training data for, I guess, uh, Rosetta fold, but in this case, we're using the pdb bi-units. So these biomets, we collect a single chance of ppdb costed at 30 C plus identity. We're using three and a half anxon resolution cut off and taking the complexes that are smaller than 10,000 rescues. And so in the training data set, there would be a bunch of homo dimers, uh, homo ligamers, and the we are asking for the model to only predict one single chain that we clustered in the context of everything else. So it would be a very simple task if we ask for example given the blue chain that's exactly matching the right chain to predict the sequence that would be the model can just copy over. So what do we do? We check the sequence similarity to see if it's more than 70 similar to the from Two Chains and if they are then the model has to predict res sequence of all the chains instead of only one chain so preventing this data leakage from the homolog American similar sequences and the objective for the model is to minimize the caloric per centropy so the this P distribution is the original distribution so in this case for example it's a glutamate so it's a probability of one this is an initial distribution and the model is outputting a distribution q that would be some distribution that the model thinks it is and then we'll just compute the the log probability of the correct amino acids of glutamate and send them over so in this case the model is using Auto aggressive decomposition so for example given a small bit of the sequence that was already decoded and the backbone the model produces the probabilities of the amino acids and then we sample from this distribution to get to get a sample so in this case it's sampling e and then using this information we predict again a new probability distribution for the next amino acid and then again samples so in this case it's Proline so the model is multi-step or Auto regressive is decomposed that one amino acid at a time it is predicting distributions and then we sample from these distributions uh so just comparing this left to right decoding that is more common in a language translation and probably an opinion was using arbitrary decoding so doing training and doing training a random decoding order was given and the model had to learn all the different decoding orders so in this case for Left Right decoding we have some text that's on the left and we just predict the next as a function of everything on the left in the arbitable decoding we can for example still wanted to code these two amino acids the model can use both the context on the left and on the right whereas the left radical would not be able to use for example these three letters to the right the input features to the model so talking about these geometric features are first of all we sort of assume that the local context will be the most important so we're forming these local neighborhoods what we raised you in terms of DC Alpha sulfur distance and then the input features for to tell that residue I and J are for example in different chains so the green uh green nodes or these circles are chain a the red one is chain B so we would give the plus 32 plus minus 32 in the primary sequence so example this residue would know it's one residue to the left of this other residue or two residues to the left of the this residue if the residues are in a different chains then we just give a sort of binary indicator say that these residues are in different units so this is similar to the alpha fold uh encoding of the positional encoding but plus also taking care that there's no it doesn't matter what the chain a is called a or called B so that there's no egg a goes before B so it's just the indication what is the same chain of different chains and then on the edges we also have the distances so we have this radial basis functions I'm not sure what they are for distances between backbone atoms so ncl2c and oxygen and there's also a virtual C beta that is calculated as a function of the other residues to get to test whether this Assumption of the local neighborhood is correct we can train neural networks and to check sequence recovery as a function of the neighbors and the graph so this is for example using 16 areas neighbors 32 years neighbors 64 years neighbors and you can see that the performance of in terms of sequence recovery is uh converging it's not getting better if we make the graph uh more and more connected so it is mostly the local information that is important to predict amino acid identity so these distances or these input features are these 25 distance between nclfc and virtual C beta so in this case this is ncl4c oxygen and C beta so we have all the interdistances that are encoded it's like radial basis function so we can imagine this like little channels that are exponent exponential of the true distance minus this middle of the distances of these bins divided by some standard deviation so they go only from zero to one everything is very nicely normalized and if the distance is really big it will just get everything with zeros and if the distance is very small there'll be maybe a very small clip for the first bin we try to do all the different variations that have angles so using given the hero angles as an input relative frame angles and so on it really is does not work as well or doesn't add on top of the what the distances give so the distances are probably a better inductive bias maybe it's easier for the model to learn what's going on so just giving distances between all the params so things we have so far is we made this graph that is local graph and we have the
<a href="https://www.youtube.com/watch?v=aVQQuoToTJA&t=737.22s">Problem Statement</a>

<img src="5.jpg" alt="Not Applicable" />

<h1><b>Sampling Temperature</b></h1>


<p>Groups, so when using the model, there's this sampling temperature. So I'm going to just quickly explain what the sampling temperature is. So what it mainly does, it makes the amino acid distribution sharper. So probably a pen index, the background is an input and then produces these called logic. So lodges are the final outputs of the neural network before using any other things, and probability distribution is formed as the exponential exponentiation of The Lodges divided by this temperature. So it can adjust the relative scaling between the largest, of course, it's normalized so it adds up to one. So making the temperature small takes a distribution very sharp, and if we increase the temperature, we make it almost uniform. We can forget training time. The temperature is set to 1 according to the loss so we can produce a higher probability sequences probably by getting some bias by reducing this temperature, and I'll show how this temperature affects different things. So for example, sequence recovery as a function of the temperature is for these monomers and polymers is slowly decreasing, so we're sampling less and less matching sequences to the inputs. But then the sequence diversity, if we take match if we try to design multiple sequences and see how similar these sequences are, the diversity is increasing. It's interesting to notice that at zero temperature, so it means taking Arc Max, this sequence diversity is still not exactly zero, it's about ten percent, and this ten percent Sigma diversity is coming from the random decoding order that these models are slightly different from each other. And we, if we look at the sequence similarity per this burial look at the core versus the surface, so different temperatures, we see that the core is quite the same, but then the surface is quite different. The sequence similarity is about 75 for the surfaces at the low temperature and only about 47 at higher temperature, which means there's lots of uncertainty and the probability Solutions are quite wide on the surface. The model is not sure what.</p>
<a href="https://www.youtube.com/watch?v=aVQQuoToTJA&t=1570.98s">Sampling Temperature</a>
