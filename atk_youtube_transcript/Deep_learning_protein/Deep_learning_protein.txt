 Hi everyone, it's my pleasure today to introduce Eustace. Eustace is a postdoc at the Baker Lab at the University of Washington. He obtained his bachelor's and master's in mathematics and his PhD in biophysics, all from the University of Cambridge. Today Eustace will talk about his work on protein MPNN. I'm sure all of us have seen the work recently come out in science and we're all very excited to hear what you've got for us. So, I think we're going to keep questions for the end but feel free to put them in the chat in the meantime. Take it away Eustace. Thank you for the introduction. So I'll be presenting on protein MPNN and please note down the questions and then ask them at the end. So first of all, I'd like to thank all protein MPNN team. So there's lots of people have been working on it from both creating database training models and also doing experimental work. And usually, when I present the work, I go into the details of the project started and present all the sort of a math C bits, but I'm just going to reverse and explain what is part of the MPNN and present experimental data first and then we will dive into some of the metrics. So the protein MPNN is the model to map protein backbone geometry into protein sequences. So this structure to sequence and sequence of structure mapping and we all know, or the alpha fold and the reseta fold predicting from a sequence structure. So some of the experimental results, one of the papers that came out at the same time with protein MPNN is called hallucinating symmetric protein assemblies. And in this case, this is a figure from the paper. The so Lucas Basel and Alexi in our lab, they've been playing with hallucinating or creating generating proteins using alpha fold. So in this case, we're interested in the cyclic protein. So in this, for example, three copies of the protein of the exactly the same sequence I introduced initially, and then we can use awful to predict the structure and impose some losses of example, as the model to be very, very certain. So high PLDT and PTM, but also introduced a cycling loss and then do Mark Mark of chain Monte Carlo MCMC to try to mutate them sequences and keep doing this loop until we get really the metrics that we want the, and then at the end, I will show that these designs near to be redesigned for the MPNN some sort of model that maps structure the sequence, because the original designs did not work. And if we look at the sort of the solubility so the how much so we'll yield there was from different designs so this original Mark of chain Monte Carlo, so, hallucination for these homo oligomers, those were the solubility results on, just median is about 10 milligrams per liter, but then. So, a bunch of them are really low yields. So they sort of say in soluble proteins. So we have aggregated redesigned the same exact back bone to point MPN and increase the soluble yield by quite a lot this is a log scale so now we're talking about 100s, even close to 1000s of of milligrams per liter. So these all the American designs, they we got crystal structures so this showing seven crystal structures, some of the homodimers and trimmers and so on, and they great are the models and the solid structures are colored. And there's some interesting designs that the beta sheets and alpha helices wrapping around. So that's quite exciting. And they all match quite well the design structures. And we've got all of these where. So, I'll probably mentioned before, but later but the redesign sequence of putting in and then again we predict by alpha fold to make sure that there's a match between what we try to design and what we redesign for the MPN and also, we can design these like large structures so these different rings of different symmetries of C 18 six so there's a six units and there's internal symmetry of 18 and this showing a cry I'm a negative state staying microscopy results. That's quite exciting there's lots of big structures and also per the MPN and can redesign these symmetric proteins making sure that the sequence stay symmetric. In the, in the, in the, I guess in the institute for protein design we also have the King's lab they're working on different nanoparticles in this case, Robert the house redesign some of the tetrahedral nanoparticles so in this case the original design was then the fast design and the interface was not forming this particle was not forming together but then we designed the party and pain in quite similar baggones that fail initially from earlier papers. Now give really nice crystal structures that match and these particles are really stable. So that's quite exciting. One of the other examples is that who obey in our lab was trying to design incorporating this short linear motives into the novel design protein scaffolds. So the idea is that, for example, this is a should domain is to bind this native peptide so we want to come up with a protein structure this de novo small peptide that would support this green structure. So originally it was designed the Rosetta remodel their design and packing, but there was no binding signal, but then redesigning the party and panion whole. This orange structure keeping the green structures that given native so trying to support it provided binding and then just to test whether some of these resist actually supporting the structure, they were modifications made from as far gene to aspartate and it indeed was supporting this small green structure. The more papers coming using coming out to use that are using protein and PNN so this is one example from the from Nate Bennett is called improving the novel protein design using deep learning. So some of the some of the failure modes for designing proteins is that we're trying to design a binder for a target. Sometimes the binder itself doesn't form. So this type of one failure but sometimes also doesn't bind so it's out of two failure. And there's some examples are metrics in this case the metric is alpha fold success of the predicted online error using different methods like Rosetta design protein and PNN and also people try to run protein and PNN and run fast relaxed to move the bag a little bit and then run putting in PNN again. And you can see that the third method of running protein and PNN and fast relax in cycles is increasing this alpha fold successfully at least sort of tricking alpha fold to think that those are good binders and also in this paper there's some benchmarking results showing that the previous results. Alpha fold predict an error can be used to discriminate better binders from the worst partners. Another paper that's the sprouting and PNN is our lab is related to the close and hallucinating these symmetric or leave them as but in this case, think about creating a protein proteins with pockets so these pockets would be used for binding different small molecules. So the same idea of designing these oligomers but then redesigning the protein and PNN and removing the symmetry. So allowing these three units to be of different sequence so the sequence would become different. So they wouldn't be perfect homo oligomers, but it would have different chains but the color pseudo symmetric so the backbone is symmetric but the sequence is not symmetric. So, it's always, I guess fun to do a little demonstration so I'm going to show this hugging face created by Simon and try to predict this. This top seven. So this is the novel design protein one QS this figure was made for the IPD so it's a proper design celebrating 10 years. So we can try to redesign the sequence and then predict the baffle fold. So this is this is this hugging face setup. There's also a GitHub code where it has maybe slightly more options how to make but this is a very simple setup so in this case we can either upload a PDB right there. Right the code so one QIS was the code and for good to the settings, we can choose which sequence of design so this design there's only one chain a sorry, we'll let's say design for sequences. Is there something temperature list to go small something temperature we can choose what sort of model use and I'll talk about these different models train of different amount of noise. So let's use the default, and then we can just run the model and design sequences so it is quite fast. That's it we we this is what this was the original sequence and then now we redesign for sequences and the model outputs the temperature they were designed that what is the score so this is a negative log probability so lower the better than the sequence recovery what is the match and then the output sequence and input sequence. What was the model name. So we have about 4045% sequence similarity from the input native crystal structure to the design sequences, and now we can do structural addition so we're going to run off of all three sequences on all of the sequences, and it's going to take about 30 seconds. So I'll go back to this later to see what are the, what is the match between the input and the output. So if we continue I'll share my slide. So, while this prediction is running. I'm going to continue the describing the model. So we have a structure to sequence model, and some of the analogies that often people think in protein machine learning are these sort of, I guess computer related image, image distribution or text. So in this case, I'm sort of showing the, this task of mapping from protein backbone to the sequence and then from sequence to the backbone is an analogy between mapping an image, describing it as in the captions in this case, this is a small cactus pattern and neon sunglasses in the Sahara desert, and then using the text to generate the image so in this case, this image was generated by imaging, giving this text prompt and generating the image. So this is sort of like a generator model that will produce many, many solutions. In this case, this protein in pen is, is more like mapping a image into the text, but the distributions are quite different in this case is that to D grid in protein, in a protein world, it's really coordinated for the animals. And from this sort of like a translation perspective, the number of ways to do some matching in the coordinates and also in the residue so we have a matching length and there's only 20 letters available to somewhat simpler maybe tasks that I have a pretty good structure is quite difficult and requires homology and other information. So there's a problem statement, what sort of was the idea behind protein and PNN is to come up to the model that can sample sequences that are highly likely. We could model multiple trains, we could fix part of the chain and we provide uncertainty about the samples. So in this case, this could be an example of two chains one chain here and one chain here. We want to maybe fix everything and just redesign the interface of just redesign the red parts giving all the rest and the model would like to output the sequence and also the probabilities of the sequence. And that when you gave I know in a nine AM was 42 and then you added more. The examples would be one sided binder design homo ligament design enzyme design and so on. The training data was collected by one and Shenko so it's a similar training data for a guest. So in this case we're using the PDB by units. So these by units, we collect the single chains of the PDB classed at 30% sequence identity using three and a half extra resolution cut off and taking the complexes that are smaller than 10,000 rescue and so in the training data set there would be a bunch of homo dimers homo ligaments and the we are asking for the model to only predict one single chain that we've listed in the context of everything else. So it would be a very simple task if we asked for example given the blue chain that's exactly matching the red chain to predict the sequence that would be the model can just copy over. So what we do we check the sequence similarity to see if it's more than 70% similar to the from two chains and if they are then the model has to predict. Resid sequence of all the chains instead of only one chain. So preventing this data leakage from the homo ligament and similar sequences. And the objective for the model is to minimize the carol percentropy so the this P distribution is the original distribution so in this case, for example, it's a glutamate. So it's a probability of one this is a native distribution, and the model is outputting a distribution Q. That would be some distribution that the model things it is and then we just complete the, the log probability of the correct amino acids glutamate and send them over. So in this case, the model is using auto aggressive decomposition so for example given a small bit of the sequence that was already decoded and the backbone, the model produces the probabilities of the amino acids, and then we sample from this issue, distribution to get to get a sample so in this case is sampling E, and then using this information, we predict, again, a new probability solution for the next amino acid, and then again samples in this case it's probably so the model is multi step. Or auto aggressive is decomposed that one amino acid at a time, it is predicting distributions and then we sample from the distributions. So just comparing this left to right decoding that is more common in the language translation and protein opinion was using arbitrary decoding so doing training and doing training, a random decoding order was given and the model had to learn all different decoding orders. So, in this case, for left right decoding, we have some text thousand left and we just predict the next as a function of everything on the left. In the arbitrary decoding, we can, for example, still wanted to code these two amino acids, the ball can use both the context on the left and on the right. Whereas the left to right decoding would not be able to use, for example, these three letters to the right. So the input features to the model so talking about these geometric features are, first of all, we sort of assume that the local context will be the most important so we're forming these local neighborhoods, why we raise you in terms of the self and self distance. And then the input features for to tell that raise the I and J are for example in different chains so the green green nodes, or these circles are chain a, the red one is chain B. So we would give the plus 32 plus minus 32 in the primary sequence so example this residue would know it's one residue to the left of this other residue or two residues to the left of the this residue. If the residue is in a different chains then we just give a sort of binary indicator say that these residues are in different chains. So it's just similar to the alpha fold encoding of the positional encoding, but plus also taking care that there's no. So we have to know whether chain is called a or called be so that there's no a goes before be so it's just indication where they say same chain or different chains. And then on the edges we also have the distances so we have this radial basis functions unless you're what they are for distances between backbone items so Nc alpha C and oxygen, and there's also a virtual C beta there's a function of the other residues. So to test whether these assumption of the local neighborhoods correct, we can train neural networks and check sequence recovery as a function of the neighbors in the graph so this is, for example, using 16 years neighbors 32 years neighbors 64 years neighbors and you can see that the performance of the terms of sequence recovery is converging it's not getting better if we make the graph more and more connected. So it is mostly the local information that is important to predict the amino acid data. So these distances so these input features these 25 distance between Nc alpha C and virtual C beta. So in this case this is Nc alpha C oxygen and C beta. So we give all the inter distances that are encoded this like radial basis functions so we can imagine this like little channels that are exponent exponential of the true distance minus these middle of the distances of these bins, divided by some side deviation so they go only from zero to everything is very nicely normalized and if the distance is really big, it will just get everything with zeros and if the distance is very small they'll be maybe a very small play for the first bin. We try to do all different variations with angles so using given the heat real angles and input relative frame angles and so on. So the thing is, does not work as well or doesn't add on top of the what the distances give so distances up probably a better inductive bias maybe it's easier for the model to learn what's going on so just giving distances between all the items. So the first part is, we made this graph that is a local graph and we have features on the edges that are this personal encoding and also the geometric features. And then we want to predict these probability distributions per node, given the rest of the amino acid so amino acids will be encoded as nodes that some of them that are decoded. And then we have the true we can calculate the carol cross entropy loss. Since we have to run this model as many times as there are amino acids, it is useful to somehow discretize the model so basically you have a backbone encoder and the sequence decoder and that's very often done in the language of the transition to having an encoder and decoder. So the encoder job will be to encode the backbone and then the decoder job will be to run it relatively multiple times to decode one amino acid at a time. So let's dive in slightly deeper, a little bit deeper into the backbone encoder. So the backbone encoder as we're saying takes these distances between different atoms as node features, and then there are no, no as edge features and they're no node features. So the model is having these edges and nodes, those are just initially zero so we can just be omitted. It's a message passing neural network that updates the nodes and then updates the edges to come up with better nodes and edges that are three layers of those that can be run. And then the decoder takes the final encoded edges and nodes, and also has the sequence into the nodes, the one that was already decoded to produce probabilities from which can be sampled during training we're not sampling. We're using teacher forcing to just run in one pass or the decoder, but doing doing the inference, we can sample one at a time and run decoder multiple times. So the model IOT is really lightweight it's about 1.7 million parameters so just three decoder layers, three encoder layers, three decoder layers they didn't mention this 128, comparing with alpha fold which has about 100 million parameters. So this is a really cheap to run even on CPUs and can run on really big proteins like 10,000 of residue so more. One of the important aspects in training these models is adding a backbone noise. So since the crystal structures, we have some artifacts that very, especially the higher resolution ones. And when people are designing things we really not sure about the backbone how does that one exactly is positioned. So we want to make the model that is not very sensitive to the input input coordinates there's some slack. So we adding Gaussian noise in training to all the coordinates, but then doing the inference, there's no need to add any noise because the model is already for example not paying attention to the scales of 0.2 angstroms. So this is one of the benchmarks of the whole idea for the paper was born from the john ingrams paper to model for graph based party in design. And we try to build on top to see what could be improved. So this is a table showing that if we take exactly the model from john ingrams method, and then we just change one thing at a time so this model is not going to exactly match the model that I just described here. So in this case there was some the heat rule feed the heat rule input features and so on but if we take the baseline model that is exactly in this paper and then we just change one thing at a time. So experiment one is adding these distances as an input features. We can see that the sequence recovery straight away goes from 41 to 49%. So there's definitely adding the inter distance feature was a really helpful inductive bias. And updating encoder edges in the normal sort of vanilla and PNN is only note updates there's no edge updates is giving a small increase compared to the baseline. And then if we combine one and two we get a slightly higher boost. And then if we these all the baseline experiment 123 are all left to right decoding progressive models. If we switch from the left to right to run in decoding order so the model has to learn all the possible decoding orders. There's no decrease in performance it's almost the same. And one interesting thing to notice is that the this noise level doing training is that the original models where run without any noise and we also test what would happen if we add a very small 0.02 angstrom side and deviation Gaussian noise. And we can see that the performance is on the PDB crystal structure is getting lower so even at the highest it's about 3% different so it means about 3% of the performance is coming from this really really tiny amount of distance that the model is either picking up on the crystals that the distance between c alpha and c alpha is indicating that it's going to be specific amino acid. But if we take alpha fold models that are idealized backbone. The difference is much smaller, or sometimes it's even better is better to train the model that is robust it's not overfitted to the crystal structure. So that's an interesting observation. All of the other results that I'll show will be for the model that is actually protein and panning on the GitHub so this is the only one that is more like a toy example. So comparing the broussetta that's been mainly used in in a vehicle lab. And the sequence recovery as a function of the average C beta distance so I wanted some measure that is independent of the section parking that would show how buried the residue is so we have a core residues that are in the core of the protein and then the surface residues, and I'm showing the fraction of the residues that fall within these distances for the nearest C beta and this is this virtual C beta, even if that doesn't have C beta we calculate virtual C beta that on average to, for example, 30% of the residue is at this 6.2 angstrom eight neighbor closer distance. We see that bringing opinion is doing better than was that in both in the core and on the surface, and they're both kind of matching that the core is a really high certainty, almost 90% accuracy sequence recovery, the model can recover what was actually there, and then the surface it's much lower, but they're both almost monotonic functions of this nearest C beta distance which is an interesting suggests that the ability to recover the sequence is, it's a function of the geometry, it's saying how much of the constraints I have from other residues around. This is not incorporating any function or any other things. So if you look at the protein by protein basis so one dot is one protein I think this is about 400 monomeric structures, probably the impedance sequence recovery is better than Rosetta in most of the cases except this one point, and there's definitely a correlation probably coming from intrinsically how how well packed the backbone is. We could also look at the sequence recovery so the model was seen on complexes has been trained on the homomers heteromers and interfaces so we can put out the how it performs a sequence recovery for these different groups. So, when using the model there's this sampling temperature so I'm going to just quickly explain what the sampling temperature is. So what it mainly does it makes the amino acid distribution sharper. So protein opinion takes the bagman's input and then produces these called logic so largest of the final outputs of the neural network before using any other things, and probability distribution is form this the this exponential log is divided by this temperature so it can adjust the relative scaling between the largest of course it's normalized so it adds up to one. So making the temperature small, it's a distribution very sharp, and if we increase the temperature we make it almost uniform, we can figure training time the temperature is set to one, according to the loss. So we can produce a higher probability sequences, probably by getting some bias by reducing this temperature and I'll show how this temperature affects different things so for example, sequence recovery as a function of the temperature is for these monomers and homomers is slowly decreasing so we're sampling less and less matching sequences to the input, but then the sequence diversity if we take match, if we try to design multiple sequences and see how similar sequences are the diversity is increasing. It's interesting to notice that zero temperatures would mean staking our max, this sequence diversity is still not exactly zero it's about 10% and this 10% sequence diversity is coming from the this run and decoding order that these models are slightly different from each and if we look at the sequence similarity per this burial look at the core versus surface, the different temperatures, we see at the core is quite the same but then the surface is quite different that sequence similarity is about 75% for the surface area is at the low temperature, and only about 47% at higher temperature which means there's lots of uncertainty and the probability situations are quite wide on the surface the model is not sure what amino acid should go on the surface but the cores very well result. If we look at the amino acid biases in this case, I'm plotting the models bias minus the PDB bias so we can see the things that are above, it means they are over represented and the things below would be under represented. So if we look at the sub function of the temperature, the lower the temperature the higher there is the bias so 0.1, there's more negative recharge glutamates. If we look at the temperature one, which the model was trained, it's matching quite well. So there's only a really really small bias with respect to the true distribution. But this bias is quite interesting that the model is trying to put more glutamates and lysines on the surface, positively negative charge the amino acids, and it's reducing polar amino acid so the model sort of find out that the best guess is to put on the surface charge amino acids and then don't think about the polars. If we compare this bias with Rosetta design bias, Rosetta has some issues that people knowing so for example, for all residues, there's over position of alanine in the core sometimes the Rosetta is not able to pack things in the core so it just puts alanine things are too close. There are too many tryptophan on the surface and so on so there are different sort of biases that Rosetta is bringing in. Also these models are sensitive to the backbone resolution so whenever someone is reporting sequence recovery you have to check what is the backbone resolution. So sequence recovery is a function of the PDB resolution, and there's a clear trend for for example three angstrom resolution protein versus one or 1.5 angstrom resolution protein. Exactly the same story goes to alpha fall model so the more confident alpha fall is about the structure. Maybe it's closer to the crystal structure because it was trained with crystal structures, the higher sequence recovery protein and can give. So we want to produce predicting predict uncertainty. So this is what I'm plotting is the sequence recovery at these two different temperatures your point one is your point five. It's a function of the negative log probability so probability of a sequence giving a structure. And when we're designing something, we don't know the sequence recovery because we don't know what we're trying to get but we, we get from the model, the negative log probability of what the model designed. So, get an idea how confident the model is about this specific backbone and sequence match. So there's quite nice line trends, lines between the sequence recovery and the negative log probability. And then, for example, in this case, I tried to design 32 sequences for these backbones, and then rank them by the log probability so the most probable sequences this one, and then least probably sequences on the right. So at the higher temperatures, there's only a very small sort of distance between the least likely sequence and the most likely sequence, whereas at the least higher temperatures where there's maybe more randomness. There's a bigger gap between for example 48 and 50% sequence recovery. So one could sample many sequences with 50% was open five temperature and then rank them by the score and pick the ones that satisfy specific features. That they want. In this case, they will get more diverse feet. So now going back to the sequence sort of this thing about the backbone to sequence and sequence backbone matching. What is the other way to sort of try to think how can we evaluate see how good the method is and what sort of people want they design a sequence. So one of the things that people do is this like forward folding okay when I design a sequence, how would the sequence look. So in this case, after fold using afford in the single sequence regime because often the sequences, these design sequences are quite different from the native sequences. So we're not even collecting multiple sequence alignments is just using a single sequence of alpha fold to predict the structure and then check how well this infrastructure matches to the infrastructure match the infrastructure. So this is a structure, a model that we try to design and we come up with sequence and then we just want to double check how good the matches. So if we look at this native sequences so 400 backbones protein backbones and take the native sequences without any homology data, the alpha fold map matching between the input and output is really low because it's really hard to predict native sequences. So if you have just a single sequence without any evolutionary information those sequences are probably not very ideal, but then taking the same backbones and then redesigning the protein and PNN, they become way more idealized, more structurally ideal for the alpha fold to predict and we get a different distribution that a bunch of sequences can be repredicted to map the input. And similar idea for example, please for this, do not design in TF2 so there's like a result of designing TF2s, and then we design the same backbones of protein and PNN, protein and PNN gives sequences that are more preferred compared to the rest of the sequences according to alpha fold. So that's also good to see the magic in the input and output. So there's also we trained with noise so we can see how does the noise affect sequence recovery and also how does it affect alpha fold success rate. So that's quite interesting that adding this very small amount of noise in this in this PDB by unit regime reduces the sequence recovery from 55% to almost 51%. So this is this what I'm talking about the crystal artifacts of the PDB, but then as we reduce the increase the noise more and more the sequence recovery slowly goes down. But what's interesting is that this alpha fold success rate so for example, having the input and output match being 90 LDDT C alpha or 95, it sort of increases as we adding more noise so it's also the models probably becoming more robust and giving more idealized sequences that could be easier predicted by alpha fold. And it's also interesting that if we look at the similarities if we take the same PDB sequences and generate a bunch of sequences and see how all the sequences match to the this PSSM so evolutionary generated similar sequences that the model with more noise generates sequences that are more similar to the PSSM sequences so we can think of this or the consensus like sequence. And I don't know whether it's just a coincidence or alpha fold is actually better at predicting consensus like sequences. Also, so when we think about this consistency check the input backbone and we have one model protein PNN, and then have a sequence and another model of the question is, what it doesn't match why doesn't match is protein PNN came up with a bad sequence or is it an alpha fold that it couldn't predict the structure. So this is showing that this alpha fold success rate to match these 90 95% cutoffs is the number of recycles. And you can see that having one recycle and three recycle being a baseline so this 3.6% and 16% success rate, increasing to six you get about 20% boost and then increasing another six times recycle to get another almost 20% boost so it's sort of showing that the structure prediction is the hard part in this encoder decoder regime so often even sequences that maybe couldn't be pretty a lot of all could be still folding into the right bag bones. So as I guess the structure prediction methods get better. There's more chance to sort of check yourself how well input and output is matching. One of the last things also to talk about is that once we have this model that takes the backbone and produces distributions, we can think about combining these distributions so in this case, I'm showing is that what if we want to design this multi multi multi multi state backbone. So we want to sample distribution that would, if we have either two chains or two states that would focus in this case I'm showing this for homo oligomer design. If I have chain a and chain B and I want just one sequence that would be a and be exactly the same sequence. So we can predict the largest for both of the sequences, and then average them out to get this average blue distribution from which I can then sample in more general, we can predict multiple backbones, either they're similar backbones or different states, there could be, and then sum them up in some linear way, where the beta is some sort of a linear coefficient that is mixing these lodges, and they could be positive or negative for positive design or negative design multi state design, coming up with a final sort of energy we can think, and then making one distribution from which you sample and you populate all the states, and you do that again again so that that's an interesting extensions of this. So some final points with party and pen is available on the GitHub, it's open source. It's fast and lightweight. There's almost no expert customization compared to Rosetta, except choosing which regions to redesign and picking some temperatures. So if you use a bunch of designs in the Baker lab, there is bias towards the surface chart them in our system maybe that's contributing to the high expression yields and so ability can do symmetric negative multi state design and so on. I'd like to thank all the people in the Baker lab that work on this and then we can maybe check the results from the alpha foot forward folding and then I can take some questions. So with the day we have four sequences, we can rank them by arm is the so it's the best 0.77 angstrom arm is DC alpha. Those are the models course so how confident MPNN is about these sequences sequence recovery PLDT's, and we can look that the structure is matching quite well to the we wanted to have. So it's quite an easy task to do to redesign this protein. I'm happy to take any questions. Great, thank you so much, Eustace. I think we can go over the questions in the order they appear in the chat so we can just stop. Sounds good. Okay, so the first question is, can MPNN be used to improve a protein like mute compute does. I'm not entirely sure what mute compute does, but I think it is possible to use MPNN to make proteins more stable or like rescue specific things, unless I don't know, Jack, I'm off you want to say more what mute compute does. Okay, otherwise I'll go to the next question. Kevin Yang, does it still work if you don't use the RBF encoding for distances. Yes, it does. I haven't maybe used exactly just raw distances but there are other encodings that can be used or even discrete bins I also try to use discrete bins just to get at this idea of not allowing the network to look at the two smaller distances, just discretizing for example distance at 0.5 angstroms or 0.2 angstroms. It does work in a very simple way. Can MPNN take multiple structures, conformations and inputs. Yes, the MPNN can take so it takes only one PDB file as an input but you can input multiple backbones in the same PDB and just separate them in space. Since the model is a local graph sort of model, it will only pick on the notes that are nearby so wouldn't pick on the other PDB that is far away. And that's how people for example in our lab are doing this multi state design just taking two states and inputting them in the same PDB and separating them apart. So we can put atoms other than amino acids like chemical ligands and metals. So in this version, it is not like I'm working on other versions that take in different metals ligands DNA RNA and so on. What's the best theoretical, what's the theoretical or the best guess maximum accuracy presumably you don't expect to recover sequence exactly given that many sequences are structured homologists. I have no idea about the sequence recovery, maximum accuracy. Most of the time I usually look at the sequence recovery in the core because I would expect that this may be more deterministic in the core, whereas the surface is quite, it's harder maybe to recover and so also function of the crystallization and so on. So what is the maximum accuracy? I guess I haven't emphasized but in general it's quite difficult to benchmark general to models. Same goes for the example, the sequence design model. Like how do you know when you come up with a better model, like sequence better sequence recovery doesn't mean maybe that you have a better model. So having alpha fold predictability really helps but also probably setting up some large scale experiments would be a good idea at some point. Another question is in natural language random automotives usually do worse than forward decoding do you have any intuition for why that's not the case here. One maybe answer is that it's, there's not much data so it's quite easy for the model to overfit and maybe learning multiple decoding orders is preventing this overfitting. That's one answer. Another is maybe how something to do to this local sort of neighborhoods that in this whole sequence is not function of the left to right is these local neighborhoods that are determining and they're not ordered I guess left to right there's lots of these non locality that also could do something with why that's also working. Another question from Simon is for the perennial pen and persuasionally spot a call do you think there is a biological reason why all of all likes these structures better is there a difference between the different model noise levels and alpha or PLE of the perennial and was faster like protocol. I'm not sure about what are the PLDT so having certainty about the structure, what I was saying is that they were better predicted the line errors between the chains for the binder. Imagine the PLDT maybe also better. I don't know exactly what are these small changes that fast or lax is doing to the structure that is helping for the MPN and design a better structure and powerful to more conflict predict maybe it's idealizing some small bits, but yeah, I don't know. So it was the point in training only on high resolution structures not sure if that's mentioned the beginning of the talk so it was not trained on only high resolution structures I did try to train on structures that are, for example, better than two and some resolution two and a half three and a half and some resolution and the performances are quite similar so I thought it's a better idea to train on more data, which means three and a half angstrom resolution maybe it could get even higher lower resolution, but I guess at some point I'm not hurting. Also wondering if MPN was trained on both crystal and cryo structures or just crystal so it was only trained on crystal structures we haven't trained on cryo em structures. It might be interesting to try my feelings that they might be slightly too low, low resolution. Another idea would be to actually use for example, off of old structures as an additional training set. A question from the VHR with the model work for the multi state design if so could you give an example of them and how robust the model for this case. It does work for multi state design, we know an example, I don't have data yet I guess it'll be published at some point in the future. I believe the previous slide said three and a half angstrom or lower okay so I'm just finding is the code that integrates and PNM to fast relax available. I think it should be reliable available. I can ask Nate in a lab that set up the model. So pretty opinions being incorporated into Rosetta as a C++ code that would be able to run from Rosetta straight away so it should be made available compatible. Have you thought about solution conditions and how to include that. I guess that might influence the surface site chain chain biases. I'm not sure about the solution conditions but I did train a neural network without including any membrane proteins to try to have a just a model to design soluble protein so maybe that's one of the ideas to but I guess it also I will train a model to input, maybe the outside conditions as whether it's a protein or soluble protein as an input label, I guess that could be done really quite easily. How does the model handle different topology stuff protein site chain connectivity is like GFP for for. I don't have a specific answer for this question I don't know I have not tried but we haven't seen any specific overfitting on different topologies or specific proteins. So the model is not a sampling confirmations is predicting a sequence maybe misunderstood the question. But it would be possible, for example, to protect the backbone have some way to move the bag with a sample bunch of sequences and maybe we predict them to get an idea about the movement. Someone else is asking what is the difference between options V underscore 48 002 and the underscore 002 for modeling. So the models that I showed maybe just check in in the GitHub repo and also here in this notebook. The models. So, the is the version 48 is 48 edges so 48 nearest neighbors for the graph and zero point zero two it means there's your point zero two angstrom noise the model was trained with this model was turned with zero point two angstrom this model was zero point three angstrom noise. So those are the models that are more robust to the larger noise levels. Okay, the next question is regarding our fall hallucinations the protein assemblies followed by putting in sequence design, the protein and then improve the alpha false sequences mainly by making the surface more soluble, always the evidence of important mutations in the course binding interfaces. I would guess that it's mainly the surface redesign, there might be some current or like in between changes to but I'm not sure I don't have statistics could ask Lucas and Basel and Alexi. So I was asking if not not mistaken for the opinion is deterministic if so heavy experiment of the coding orders to significant parameter to affect predictions for instance, an interface designed as decoding from the rest is proximal to the fixed sequence improves course and vice versa. So I did try to experiment off initially set up using the random decoding order tried to design from the core to the surface on the surface of the core and other ideas. There's no clear evidence that it does help to go from some, I guess, dramatically geometrically inspired decoding order. It seemed like the performance is about the same. How long did it. How long did training take and what soft of hardware so I train on the one GPU. And it, I train on a 100 for probably two days so the training is fairly fast. Do you think that be factor would be helpful input perhaps for protein flexibility disorder. It might be helpful but at the same time the real question is what would be this input when someone is designing protein would they have an idea what sort of be factor to put in because most of the time the model is used on some backbones that are either some sort of like helices that are parametrically designed or alpha for backbones or some other bag months so it's. Yeah, I don't know whether it would be someone lesson user would know what sort of be factor to use. How's in PNM performing with membrane proteins. I haven't tested that exactly. Haven't looked exactly at the memory proteins, but I do know that it does put create membrane like proteins if someone is trying to design something that is long and extended, having put an MPN and things as a membrane protein so it's, but I draw fabrics on the surface. So the question is pardoning PNM score dependent on the decoding order here are the log likely from the model depending on sequence context from the decoding previous or as you. It does depend the dependence is pretty small but the score or log likelihood is a function of the decoding order. So if one wants to get a very precise score and run MPN multiple times and get an average and starting deviation to see what is the score usually the sound deviation is quite small. So the question is what applications you envisioned MPN will be used for. So it's already used for different sort of redesigning sequences trying to make them more soluble. I think there's some efforts and trying to use MPN to make to make enzymes more stable so for example taking native enzymes and trying to redesign parts of them, trying to keep the function but maybe make them. They're more dynamically resistant, higher yields and so on so so somehow trying to idealize making the proteins better by the designers using MPN. Multi-state design and all the other sort of sequence related things. I guess an interesting result from this MPN work was that a bunch of research design things ideal, healers and so on. Sometimes they wouldn't work and probably mainly because the sequence was sort of incorrect or not ideal that it was very hard to express or would oligomerize. So now this tool is helping to solve this question if you have a decent backbone, can you come up with a sequence that work in experiments. But now I guess the hard task is now how do you come up with backbones that you want to have how do you come up with functional proteins and so on. Another question is, what would using amino acid encodings from the pre-trained protein language model as part of the would be helpful for training the model. It could be helpful. I don't know exactly. I guess, yeah, one could try to do that. I haven't personally tried and looked into that sort of area yet. Any other questions. Another question is, if a background has more diversity at lower temperatures. Could that mean it is more designable. So I would say if the model if the bag one probably has less diversity at lower temperatures the model is very certain about this thing it would be maybe more designable the model. It's probably makes sense to more look at the at the log probability to think about which bag ones are designable. So how confident the model is about the backbones. So I'm not asking, can you speak towards suggesting typical bias values or the bias by residue Jason flag. So there is a possibility to add biases so for example for one more polar residues or other sort of, or maybe less alanine in the design. So these values are more just like experimental I think there's an example in the GitHub repo that is suggesting some values but otherwise probably some error and trial trial and errors needed to figure out what are those biases. I have not tried to design any CDR for the using impianet, but I think it would be an interesting application I imagine it's quite a difficult task but it would be interesting to see. And I don't know whether this training on specifically CDR loops would be helpful. All the chains are included that are bigger than 30 residues I think. So if, if, if it's in PDB it's lower than three and a half extra resolution and and bigger than 30 amino acids it should be included. So. Thanks everyone.