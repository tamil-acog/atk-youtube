

Hi everyone, it's my pleasure today to introduce Eustis. Eustis is a postdoc at the Baker Lab at the University of Washington. He obtained his bachelor's and Master's in mathematics and his PhD in biophysics, all from the University of Cambridge. Today, Eustis will talk about his work on protein MPNN. I'm sure all of us have seen the work recently come out in science, and we're all very excited to hear what you've got for us. So, I think we're going to keep questions for the end, but feel free to put them in the chat. In the meantime, take it away, Eustis.

Thank you for the introduction. So, I'll be presenting on protein MPNN. Please note down the questions and then ask them at the end. First of all, I'd like to thank all protein MPNN team. So, there are lots of people who've been working on it, some both creating database training models and also doing experimental work. Usually, when I present the work, I go into the details of how the project started and present all the sort of like nasty bits, but I'm just going to reverse and explain what is part of the MPNN and present experimental data first. And then we will dive into some of the metrics. So, the protein MPNN is the model to map protein backbone geometry into protein sequences. So, this structured a sequence and sequence of structure mapping, and we all know sort of alpha Falls and Rec centerfold predicting from a sequence structure. So, some of the experimental results. One...

Hallucinating Symmetric Protein Assemblies 

 

Of the papers that came out at the same time, one was partly MPNN, called "Hallucinating Symmetric Protein Assemblies." In this case, this is a figure from the paper. So, Lucas Puzzle and Alexi in our lab, they've been playing with hallucinating or creating, generating proteins using AlphaFold. So, in this case, with the interest in cyclic proteins. So, for example, three copies of the protein of the exactly the same sequence I introduced initially, and then we can use R4 to predict the structure and impose some losses. So, for example, ask the model to be very, um, very certain, so high PLDT and PTM, but also introduced a cycling loss and then do Markov chain Monte Carlo (MCMC) to try to mutate some sequences and keep doing this loop until we get really the metrics like we want the and then at the end, I will show that these designs need to be redesigned with 14 NPN and some sort of model that maps structure the sequence because the original design did not work. 

If we look at the sort of the solubility, so the how much soluble yield there was from different designs, so this original Markov chain Monte Carlo hallucination for these homologous, those who have the solubility results on guest median is about 10 milligrams per liter, but then so a bunch of them are really low yields, so they sort of say insoluble proteins. They aggregated redesigned the same exact backbone supporting the impended and increase the soluble yield quite a lot. This is a log scale, so now we're talking about hundreds even close to thousands of um of milligrams per liter. Also, from these uh oligomeric designs, they we got Crystal structures. So, this is showing seven Crystal structures, some of the homo dimers and uh trimers and so on, and the gray are the models and the salt structures are colored, and there's some interesting designs, the beta sheets and Alpha helices wrapping around, so that's quite exciting, and they all match quite well the design structures. 

I guess I should say that all of these were so I'll probably mention before later, but the redesign sequence of Courtney and pin and again we predicted by AlphaFold to make sure that there's a match between what we try to design and what we redesigned reporting in PNN. Also, we can hallucinate these like large structures, so these different rings of different symmetries of c18 6, so there's a six units and there's internal symmetry of 18, and this showing at chromium a negative stain microscopy results, so that's quite exciting. There's lots of big structures and our show property and PNM. You can redesign these symmetric proteins making sure that the sequence stays symmetric in the in the in the I guess in the uh Institute for protein design. We also have the Kings lab. They're working on different nanoparticles, so in this case, uh Robert de Haas redesigned some of the tetrahedral nanoparticles. So, in this case, the original design was then the Rosetta uh class design, and the interface was not forming. This particle was not forming together, but then redesigned the party and K N quite similar backbones that fail initially from earlier papers now give really nice Crystal structures that match, and these particles are really stable, so that's quite exciting.

Incorporating these Short Linear Motives into Genova Design Protein Scaffolds 

One of the other examples is that who are they in our lab was trying to design incorporating these short linear motives into Genova design protein scaffolds. So, the idea is that for example, if this sa issue domain is to bind this native peptide, so we want to come up with the protein structure this de Nova small peptide that would support this green structure. So, originally it was designed with Rosetta remodel their design and packing, but there was no binding signal. But then redesigning the protein opinion whole uh this orange structure keeping the green structures the given native so trying to support it provided binding. And then just to test whether some of these threads is actually supporting the structure, they were modifications made from spark Gene to aspartate and it indeed was supporting this small green structure. The more papers coming using coming out they're using that are using protein in pnm, so this is one example from the uh from Nate Bennett. It's called improving the neuroprotein design uh using deep learning. So, some of the some of the failure modes for Designing uh proteins is that if we're trying to design a binder for a Target, sometimes the binder itself doesn't form, so this type of one failure, but sometimes also it doesn't bind, so it's a sidewalk to failure, and there's some examples sort of showing metrics in this case the metric is Alpha fold success of the predicted align error using different methods like Rosetta design protein and pnn, and also people try to run protein and pnn and run fast relax to move the backbone a little bit and then run protein and pin and again, and you can see that the third method of running approaching independent and fast relax in Cycles is increasing this Alpha false success rate at least sort of tricking alcohol to think that those are good binders, and also in this paper there's some benchmarking results showing that the previous results Alpha fold prediction line error can be used to discriminate better binders from the worst binders. Another paper that you supporting the opinion is our lab is related to the conclusion hallucinating this symmetric oligomers, but in this case think about creating proteins with pockets so these pockets would be used for binding different small molecules. So, the same idea of Designing these oligomers but then redesigning the protein and pen and removing the symmetry so allowing these three units to be of different sequence so the sequence would become different so they wouldn't be perfect homologous but they would have different genes but the they call it pseudo symmetric so the backbone is symmetric but the sequence is not symmetric um so it's always I guess fun to do a little demonstration so I'm gonna show this hugging face created by Simon and try to predict this um this top seven so this is the Nova design party in one qos this figure was made for the ipd so instantly property and design celebrating 10 years so we can try to redesign this sequence and then predict the buffer fold um so this is this is this hugging face setup there's also a GitHub code where it has maybe slightly more options how to make but this is a very simple setup so in this case we can either upload a pdb or write the uh write the code so one qis was the code and if we go to the settings we can choose which sequencer designs so this design there's only one chain a I'm sorry we'll let's say design for sequences which is the sampling temperature is to go small sampling temperature we can choose what sort of model you use and I'll talk about these different models train a different amount of noise so let's use the default and then we can just run the model and design sequences so it is quite fast that's it we we this is what this was the original sequence and then now we redesigned four sequences and the model outputs the temperature that were designed at uh what is the score so this is a negative log probability so lower the better than the sequencer probably what is the match between the output sequence and the input sequence what was the model name so we have about 40 45 sequence similarity from the input.

Structural Addition 

native crystal structure to the design sequences and now we can do structural addition. So, we're going to run Alpha fold three recycles on all of the sequences and it's going to take about 30 seconds. So, I'll go back to this later to see what are the what is the match between the input and the outputs. Um, so if we continue, I'll share my slide one second. [Music] 

Okay, so while this prediction is running, I'm going to continue describing the model. So, we have a structure, the sequence model, and some of the analogies that often people think in protein machine learning are these sort of, I guess, computer-related image generation or text. So, in this case, I'm sort of showing the task of mapping from protein backbone into the sequence, and then from the sequencer to the backbone is an analogy between mapping an image, describing it as in a caption. 

So, in this case, this is a small cactus wearing a straw hat and neon sunglasses in the Sahara Desert, and then using the text to generate the image. So, in this case, this image was generated by Imogen giving this text prompt and generating the image. So, this is sort of like a generator model that will produce many, many solutions. In this case, this protein name pianon is more like mapping an image into the text, but the distributions are quite different. 

In this case, it's a 2D go to pixels, and probably in a protein world, it's 3D coordinate of the atoms, and from this sort of like a translation perspective, the number of residues are matching in the coordinates and also in the residue. So, we have a matching lens, and there's only 20 layers available. So, somewhat simpler maybe task, the task for prediction structure is quite difficult and it requires homology and other.

Problem Statement:

Information, uh, so the problem statement, what sort of was the idea behind protein MPN, is to come up with a model that can sample sequences that are highly likely. We could model multiple chains, we could fix parts of the chain, and we provide uncertainty about the samples. So in this case, this could be an example of two chains, one chain here and one chain here, and we want to maybe fix everything and just redesign the interface. So just redesign the red parts, giving all the rest. And the model we'd like to output the sequence and also the probabilities of the sequence. Experience that when you gave I the one at 9am was 42, and then you added more. So the examples would be one-sided binded design, uh, homologous design, enzyme design, and so on. Uh, the training data was collected by Yvonne and Schenko, so it's a similar training data for, I guess, uh, Rosetta fold, but in this case, we're using the pdb bi-units. So these biomets, we collect a single chance of ppdb costed at 30 C plus identity. We're using three and a half anxon resolution cut off and taking the complexes that are smaller than 10,000 rescues. And so in the training data set, there would be a bunch of homo dimers, uh, homo ligamers, and the we are asking for the model to only predict one single chain that we clustered in the context of everything else. So it would be a very simple task if we ask for example, given the blue chain that's exactly matching the right chain to predict the sequence, that would be the model can just copy over. So what do we do? We check the sequence similarity to see if it's more than 70% similar to the from Two Chains, and if they are, then the model has to predict res sequence of all the chains instead of only one chain. So preventing this data leakage from the homolog American similar sequences, and the objective for the model is to minimize the caloric per centropy. So the P distribution is the original distribution. So in this case, for example, it's a glutamate, so it's a probability of one. This is an initial distribution, and the model is outputting a distribution q that would be some distribution that the model thinks it is, and then we'll just compute the log probability of the correct amino acids of glutamate and send them over. So in this case, the model is using auto-aggressive decomposition. So for example, given a small bit of the sequence that was already decoded and the backbone, the model produces the probabilities of the amino acids, and then we sample from this distribution to get to get a sample. So in this case, it's sampling e, and then using this information, we predict again a new probability distribution for the next amino acid, and then again samples. So in this case, it's proline. So the model is multi-step or auto-regressive is decomposed that one amino acid at a time. It is predicting distributions, and then we sample from these distributions. Uh, so just comparing this left to right decoding that is more common in a language translation, and probably an opinion was using arbitrary decoding. So doing training and doing training a random decoding order was given, and the model had to learn all the different decoding orders. So in this case, for left-right decoding, we have some text that's on the left, and we just predict the next as a function of everything on the left. In the arbitable decoding, we can, for example, still wanted to code these two amino acids. The model can use both the context on the left and on the right, whereas the left radical would not be able to use, for example, these three letters to the right. The input features to the model. So talking about these geometric features are first of all, we sort of assume that the local context will be the most important. So we're forming these local neighborhoods, what we raised you in terms of DC Alpha sulfur distance, and then the input features for to tell that residue I and J are, for example, in different chains. So the green, uh, green nodes or these circles are chain A, the red one is chain B. So we would give the plus 32 plus minus 32 in the primary sequence. So example, this residue would know it's one residue to the left of this other residue or two residues to the left of the this residue. If the residues are in a different chains, then we just give a sort of binary indicator, say that these residues are in different units. So this is similar to the alpha fold uh, encoding of the positional encoding, but plus also taking care that there's no, it doesn't matter what the chain A is called A or called B. So that there's no egg A goes before B. So it's just the indication what is the same chain of different chains. And then on the edges, we also have the distances. So we have this radial basis functions. I'm not sure what they are for distances between backbone atoms. So ncl2c and oxygen, and there's also a virtual C beta that is calculated as a function of the other residues to get to test whether this assumption of the local neighborhood is correct. We can train neural networks and to check sequence recovery as a function of the neighbors and the graph. So this is, for example, using 16 areas neighbors, 32 years neighbors, 64 years neighbors, and you can see that the performance of in terms of sequence recovery is uh, converging. It's not getting better if we make the graph uh, more and more connected. So it is mostly the local information that is important to predict amino acid identity. So these distances or these input features are these 25 distance between nclfc and virtual C beta. So in this case, this is ncl4c oxygen and C beta. So we have all the interdistances that are encoded. It's like radial basis function. So we can imagine this like little channels that are exponent exponential of the true distance minus this middle of the distances of these bins divided by some standard deviation. So they go only from zero to one. Everything is very nicely normalized. And if the distance is really big, it will just get everything with zeros. And if the distance is very small, there'll be maybe a very small clip for the first bin. We try to do all the different variations that have angles. So using given the hero

Sampling Temperature

Groups, when using the model, there's this sampling temperature. So, I'm going to just quickly explain what the sampling temperature is. What it mainly does, it makes the amino acid distribution sharper. So, probably a pen index, the background is an input and then produces these called logic. So, lodges are the final outputs of the neural network before using any other things, and probability distribution is formed as the exponential exponentiation of The Lodges divided by this temperature. So, it can adjust the relative scaling between the largest, of course, it's normalized so it adds up to one. Making the temperature small takes a distribution very sharp, and if we increase the temperature, we make it almost uniform. We can forget training time, the temperature is set to 1 according to the loss. So, we can produce higher probability sequences, probably by getting some bias by reducing this temperature, and I'll show how this temperature affects different things. 

So, for example, sequence recovery as a function of the temperature is for these monomers and polymers is slowly decreasing. So, we're sampling less and less matching sequences to the inputs, but then the sequence diversity, if we take match, if we try to design multiple sequences and see how similar these sequences are, the diversity is increasing. It's interesting to notice that at zero temperature, so it means taking Arc Max, this sequence diversity is still not exactly zero, it's about ten percent, and this ten percent Sigma diversity is coming from the random decoding order that these models are slightly different from each other. And we, if we look at the sequence similarity per this burial look at the core versus the surface, so different temperatures, we see that the core is quite the same, but then the surface is quite different. The sequence similarity is about 75 for the surfaces at the low temperature and only about 47 at higher temperature, which means there's lots of uncertainty and the probability solutions are quite wide on the surface. The model is not sure what.

Amino Acid Biases 

Amino acid should go on the surface but the core is very well resolved. If we look at the amino acid biases, so in this case, I'm plotting the model's bias minus the pdb bias so we can see the things that are above it means they are overrepresented and the things below would be underrepresented. So if we look at the summation of the temperature, the lower the temperature, the higher there is the bias. So 0.1 there's more negative which are glutamates. If we look at the temperature one which the model was trained, it's matching quite well so there's only a really small bias with respect to the true distribution. But this bias is quite interesting that the model is trying to put more glutamates and lysines on the surface, positively negatively charged amino acids, and it's reducing polar amino acids. So the model sort of finds out that the best guess is to put on a surface charge amino acids and then don't think about the polaris. If we compare this bias with Rosetta design bias, Rosetta has some issues that people know. So example of all residues, there's a reposition of alanines in the core sometimes. The more the Rosetta is now able to pack things in the course so it just puts alanine things are too close. There are too many tryptophans on the surface and so on. So the different sort of biases that Rosetta is bringing in. Also, these models are sensitive to the backbone resolution. So whenever someone is reporting sequence recovery, you have to be to check what is the backbone resolution. So this is showing the sequence recovery is a function of the pdb resolution and there is a clear trend. For example, three angstrom resolution protein versus one or 1.5 angstrom resolution protein. Exactly the same story goes to Alpha fold models. So the more confidential is about the structure, maybe it's closer to the crystal structure because it was trained with Crystal structures. The highest sequence recovery protein impedance can give it was also important to produce predicting predict uncertainty. So this is what I'm plotting is the sequence recovery at these two different temperatures, 0.1 and 0.5, as a function of the negative log probability. So probability of a sequence giving a structure. And when we're designing something, we don't know the signature carrier because we don't know what we're trying to get, but we get from the model the negative log probability of what the model designed. So we can still get an idea how confident the model is about this specific backbone and sequence match. There's quite nice line trends lines between these sequence recovery and the negative log probability. And then for example, in this case, I try to design 32 sequences for these backbones and then rank them by the log probability. So the most probable sequence is this one and then this probability sequence is on the right. You see at the higher temperatures there's only a very small sort of distance between the least likely sequence and the most likely sequence, whereas at these higher temperatures where there's maybe more randomness, there's a bigger gap between, for example, 48 and 50 sequence recovery. So one could sample many sequences with 50 was open five temperature and then rank them or by the score and pick the ones that satisfy specific features that they want. In this case, they will get more diversity. 

Now going back to this sequence sort of this thing about the backbone to sequence and sequence the backbone matching. What is the other way to sort of try to think how can we evaluate see how good the method is and what sort of people want? They design a sequence. So one of the things that people do is it's like forward folding. Okay, when I design the sequence, how would the sequence look? So in this case, Alpha fold using off fold in this single sequence regime because often the sequences these design sequences are quite different from the Native sequences. So we're not even collecting multiple sequence alignments. It's just using a single sequence with Alpha fault to predict the structure and then check how well this infrastructure matches to the output structure match the infrastructure. So this is a structure a model that we try to design. We come up with sequence and then we just want to double-check how good the match is. So if we look at this native sequences, so 400 backbones protein diagnose and take the native sequences without any homology data that fold map matching between the input and output is really low because it's really hard to predict native sequences with just a single sequence without any evolutionary information. Those sequences are probably not very ideal, but they're taking the same backgrounds and then redesigning the protein and pnn they become way more idealized more structurally ideal for the alpha fall to predict and we get a different distribution that a bunch of sequences can be repredected to map the input and similar idea for example this for this do not have a designed in TF2 so there's like a rosetta design in tf2s and then redesigned the same bag most of protein pnn protein opinion gives sequences that are more preferred compared to over seven sequences according to After Fall so that's also good to see the match between the input and the output. 

We trained with noise so we can see how does the noise affect sequence recovery and also how does it affect other fault success rate so that's quite interesting that adding this very small amount of noise in this in this pdb bi-unit regime reduces the sequence recovery from 55 to almost 51. So this is this one I'm talking about the crystal artifacts of the pdb but then as we reduce the increase the noise more and more the sequence recovery slowly goes down but what's interesting is that this Alpha fault success rate so for example having the input and output match being 90 ldtc alpha or 95. It sort of increases as we're adding more noise so it's also the model is probably becoming more robust and giving more idealized sequences that could be easier predicted by Alpha fold and it's also interesting that if we look at the similarities if we take the same pdbs and sequences and generate a bunch of sequences and see how well those sequences match to the pssm so evolutionary generated similar sequences that the model with more noise generates sequences that are more similar to the pssm sequences so we can think of this more of the consensus-like sequence and I don't know whether it's just a coincidence or Alpha fault is actually better at predicting consensus-like sequences also. 

So when we think about this consistency check the input backbone and we have one model property in pnn and then have a sequence and another model off of all the question is well it doesn't match why it doesn't match it's protein npn came up with that sequence or is it off of all that it couldn't predict the structure so this is showing that this Alpha fault success rate to match these 1995 cutoffs is the number of recycles and you can see that having one recycle and three recycle being a baseline so this 3.6 percent and 16 percent success rate increasing to six recycles you get about 20 boost and then increasing another six times recycles you get another almost 20 boost so it's sort of showing that the structure prediction is the hard part in this encoder decoder regime so often even sequences that maybe couldn't be particularly by off of all it could be still folding into the correct backgrounds so as I guess the structure prediction methods get better there's more chance to sort of check yourself how well the input and output is matching. 

One of the last things also to talk about is that once we have this model that takes the backbone and produces distributions we can think about combining these distribution so in this case I'm showing is that what if we want to design this multi multi multi-state backbone so we want to sample distribution that would if we have either two chains or two states that would so I guess in this case I'm showing this for homo oligomer design if I have chain a and chain B and I want just one sequence that would be a and b exactly the same sequence so I can predict the largest both of the sequences and then average them out to get this average blue distribution from which I can

Sequence Recovery Maximum Accuracy 

Homologous, I have no idea about the sequence recovery maximum accuracy. Most of the time, I usually look at the sequence recovery in the core because I would expect that this may be more deterministic in the core, whereas the surface is quite harder maybe to recover, and it's also a function of the crystallization and so on. I have no idea what the maximum accuracy is. I guess I haven't emphasized, but in general, it's quite difficult to benchmark generative models. The same goes for the example, the sequence design model. Like how do you know when you come up with a better model? It's like sequence better sequence recovery doesn't mean maybe that you have a better model. So having Alpha fault predictability really helps, but also probably setting up some large-scale experiments would be a good idea at some point.

Another question is in natural language random order models usually do worse than forward decoding. Do you have any intuition for why that's not the case here? [Music] Um, one maybe answer is that it's there's not much data, so it's quite easy for the model to overfit, and maybe learning multiple decoding orders is preventing this overfitting. That's one answer. Another is maybe has something to do to this local sort of neighborhoods that in this whole sequence is not function of the left to right. It's these local neighborhoods that are determining, and they're not ordered. I guess left to right, there's lots of this non-locality that also could use something that why that's also working.

Another question from Simon is for the pregnant opinion of specialized particle. Do you think there is a biological reason why alcohol likes these structures better? Is there a difference between the different model and noise levels and the alphability of the pregnancy protocol? Um, I'm not sure about what are the PLDT, so having certainty about the structure. What I was saying is that they were better predicted the line errors between the chains for the binder. Imagine the PL LEDs may be also better. I don't know exactly what are these small changes that fascolax is doing to the structure that is helping for the MP and design a better structure, and it's possible to more confidently predict. Maybe it's idealizing some small bits, but yeah, I don't know what was the party dependent train only on high-resolution structure. It's not sure if that's mentioned at the beginning of the talk. So it was not traded on only high-resolution structures. I did try to train on structures that as for example better than doing some resolution two and a half three and a half resolution, and the performances are quite similar. So I thought it's a better idea to train on more data, which means three and a half angstrom resolution. Maybe it could get even higher lower resolution, but uh, I guess at some point, it would start hurting.

Also wondering if NPNN was trained on both Crystal and cryo structures or just crystals. So it was only trained on crystal structures. We haven't trained on prior EM structures. It might be interesting to try. My feeling is that they might be slightly too low resolution. Another idea would be to actually use, for example, Alpha fold structures as an additional training set.

Question from DJ Chang, would the model work for the multi-state design? If so, could you give an example come in how robust the model for this case? Um, it does work for multi-state design. We know an example. I don't have data yet. I guess it'll be published at some point in the future. Um, I believe the previous slides are three and a half angstrom or lower.

Okay, someone's responding, is the code that integrates in PNN with astral X available? Um, I think it should be relatable available. I can ask Nate in a lab that set up the model so protein opinion is being incorporated into Rosetta as a C plus plus code that would be able to run from Rosetta straight away. So it should be.

Solution Conditions 

Data available compatible. Have you thought about solution conditions and how to include that? I guess that might influence the server-side chain biases. Um, I haven't thought about the solution conditions, but I did train a neural network without including any uh, membrane proteins to try to have it just the model to design soluble proteins. So maybe that's one of the ideas too, but I guess it also, I will train a model to input maybe the outside conditions as whether it's a membrane or a soluble protein input label. I guess that could be done really quite easily. 

How does the model handle different topologies of protein side chain connectivities like GFP 404? Um, I don't have a specific answer for this question. I don't know. I have not tried, but we haven't seen any specific overfitting on different topologies or specific proteins. 

Can you sample different confirmations of the interacting partner, say peptide, we're starting with large noise level of temperature? Upper sizes that backbone have to be so the model is not, I guess, sampling confirmations is predicting a sequence. Maybe I misunderstood the question, but it would be possible, for example, to search up the backbone, have some way to move the background, then sample a bunch of sequences, and maybe we predict them to get an idea about the movement. 

Someone else is asking what is the difference between options V underscore 48 0 0 2 and the underscore zero zero two for modeling? So the models that I showed, maybe just show again in the GitHub repo and also in here in this notebook, the models. So V is the version, 48 is 48 edges, so 48 nearest neighbor service for the graph, and 0.02 it means there's 0.02 angstrom noise. The model was trained with this model was streamed with 0.2 angstrom. This model wasn't 0.3 angstrom noise, so those are the models that are more robust to the larger noise levels. 

Okay, the next question is regarding our four hallucination of the protein assemblies followed by chronium sequence design. It probably has been improved Alpha sequences mainly by making the surface more soluble, always the evidence of important mutations, and of course, binding interfaces. Um, I would guess that it's mainly the surface redesigned. There might be some corn or like in-between changes too, but I'm not, I don't have statistics. You could ask Lucas and Basil and Alexi. 

Someone was asking if no, I'm not mistaken, for the opinion is deterministic. If so, have you experimented decoding orders, a significant parameter to affect predictions? For instance, in interface design, does decoding from the resist proximal to the fixed sequence improves course and vice versa? So I did try to experiment of initial instead of using the random decoding order, try to design from the core to the surface, from the surface to the core, and other ideas. There's no clear evidence that it does help to go from some, I guess, geometrically, geometrically inspired decoding order. It seems like the performance is about the same. 

How long did it, how long did training take and what sort of hardware? So I train only one GPU, and it, I train on A100 for probably two days, so the training is fairly fast. 

Do you think that b Factor would be helpful input, perhaps, or protein flexibility or disorder? It might be helpful, but at the same time, the real question is what would be this input when someone is designing protein? Would they have an idea what sort of b Factor to put in? Because most of the time, the model is used on some backbones that are either some sort of like helices that are parametrically just designed or Alpha full backbones or some other backgrounds. So it's, yeah, I don't know whether it would be someone less than user would.

The Membrane Proteins 

Know what sort of B factor to use when housing and performing the membrane proteins.

I haven't tested that exactly, haven't looked exactly at the membrane proteins, but I do know that it does create membrane-like proteins. If someone is trying to design something that is long and extended, having problems is the...

Is Body Impedance Score Dependent on the Decoding Order? 

 

Membrane protein puts Hydra subjects on the surface. Another question is, "Is body impedance score dependent on the decoding order?" Here are the log likelihoods from the model depending on sequence context. When you're decoding previous residue, it does depend. The dependence is pretty small, but the score or log likelihood is a function of the decoding order. So, if one wants to get a very precise score, we can run npn in multiple times and get an average and standard deviation to see what is the score usually. The sound.

What Applications Do You Envision MPN Will Be Used for?

Deviation is quite small, and the final question is: what applications do you envision MPN will be used for? So, it's already used for different sorts of redesigning, uh, sequences, trying to make them more soluble. I think there are some efforts in trying to use MPN to make enzymes more stable. So, for example, taking native enzymes and trying to redesign parts of them, trying to keep the function but maybe make them more thermodynamically resistant, higher yields, and so on. So, somehow trying to idealize making the proteins better. Binder design is using MPN, um, multi-design, and all the other sort of sequence-related things. I guess an interesting, uh, result from this MPNN work was that a bunch of Rosetta design things, ideal helices, and so on, sometimes they wouldn't work, and probably mainly because the sequence was sort of incorrect or not ideal, that it was very hard to express, I would oligomerize. So now this tool is helping to solve this question of if you have a decent backbone, can you come up with a sequence that works in experiments? But now, I guess the hard task is now how do you come up with backbones that you want to have, how do you come up with functional proteins, and so on. Another question is, would using amino acid encodings from the pre-trained protein language model as part of the input be helpful for training the model? Um, it could be helpful. Um, I don't know exactly, uh, I guess yeah, one could try to do that. Um, I haven't personally tried and looked into that sort of area yet.

If a Background Has More Diversity at Lower Temperatures, Could that Mean It Is More Designable?

Any other questions?

Another question is, if a background has more diversity at lower temperatures, could that mean it is more designable? Um, so I would say if the model, if the backbone probably has uh less diversity at lower temperatures, the model is very certain about this thing, it would be maybe more designable. The model, it's probably makes sense to more look at the at the log probability to think about which backgrounds are designable. So how confident the model is about the backbones. Um, someone else asking, can you speak towards suggesting typical bias values or the bias by residue Json flag? So there is a possibility to add biases, so for example, for one more polar residue, so other sort of or maybe less alanines in the design. So these values are more just like experimental. I think there's an example in the GitHub repo that is suggesting some values, but otherwise probably some error in trial uh trial and errors needed to figure out what are those bad seeds. I have not tried to design any cdrs for using npn, but I think it would be an interesting application. I imagine it's quite a difficult task, but it would be interesting to see. And I don't know whether yes training on specifically CDR Loops would be helpful. So all the chains are included that are bigger than 30 residues. I think so if if it's in pdb it's lower than three and a half angle resolution and um and bigger than 30 amino acids, it should be included. All right, thanks everyone again.