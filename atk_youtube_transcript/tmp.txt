 What are Cohere embeddings 

 today we're going to take a look at coheres multilingual embedding model for those of you that are not aware of cohere they are kind of similar to open AI in that they are essentially a service provider of large language models and all of the services that come with that now right now they are not as well known as open AI which is understandable opening hours has been around for a bit longer but cohere is actually a really good company that offers a lot of really good tooling that is actually very much comparable to what openai offers and that's actually the first thing I want to look at here I just want to show you a few comparison 


  Cohere v OpenAI on cost 

 points between cohere and open AI in terms of embedding models okay so we're going to first take a look at the costs between these two opening eyes so Premiere embedding model right now is auto 002 comes out to this much per 1000 tokens coher doesn't have a like per 1000 tokens for the cost it actually goes with one dollar per 1000 embeddings what does one embedding mean well basically every call or every chunk of text that you ask out here to embed that is one embedding so upon embedding the maximum size of that is actually just over four thousand tokens so if you're maxing out every embedding is in you are sending 4 000 tokens to every embedding call then that means you would be getting this comparable price here which is actually half price which is is pretty good now if we kind of translate this into something that's a bit more understandable we have like 13 paragraphs is roughly about a thousand tokens these are the prices all right so so with order with open AI it's one dollar per thirty two and a half thousand pairs so here is actually one dollar per 65 000 paragraphs which is really good but there is obviously a catch which is you know this thing up here or this one dollar per 1000 embeddings right the chances are you're probably not going to use four thousand embeddings with every call to cohere so 2000 tokens well that's probably like 26 paragraphs if you're embedding 26 paragraphs at a time realistically you're probably gonna do much less right so if let's say you're going for more like a thousand tokens which I think is more realistic then obviously the the price of go here is actually double the price of open AI in this instance so it kind of depends on what you're doing there as to whether you are throwing a load of text into your embeddings or not so I think the costs are pretty comparable cohere can be cheaper but it can also be more expensive according to this logic anyway okay so one thing I miss very quickly is the on-prem solution that cohere offers so we have it here essentially you can run your own AWS instance and in the time that it would take you this is assuming you're running at 100 and the time date would tell you to encode 1 billion paragraphs if you use coheres on-prem solution you would end up paying two and a half thousand dollars it's also a lot quicker and you know there are all the other benefits as well but I thought when we're talking about cost we should definitely include that in there so you know it depends essentially embedding size actually you know this is a good indicator of how much it's going to cost you so it's actually under costs the higher your embedding size the more storage you need to store all of your embeddings after you've created them right so the embedding size smaller is cheaper soccer here is half the size of open AI in this case so you know long term you would probably actually be saving money with cohere with this embedding size if you're storing a lot of vectors so you know that's definitely something to consider if you consider this with the embedding cost initially you know maybe you're actually saving money with cohere even if you're just embedding like a thousand tokens or even 500 tokens at a time 


  Cohere v OpenAI on performance 

 long term you're probably going to end up saving money now performance so this is kind of hard to judge because this is a single Benchmark that knows where iron is put together and okay I mean cohere for sure is coming out on top here it's kind of hard to say it again like whether this is representative across a board or not but nonetheless the the two models that are comparable here okay here's multilingual model and open ai's auto 002 model which is English and this is a English search task so it's pretty interesting that opening eyes best English language model is comparable to his multilingual model because his English model is better and then there's a coheri rankers this is an embedding model it's at like a imagine you retrieve all of your items or you get two chunks of text and you you feed them into like a transform model and compare them directly it takes basically a lot slower but generally speaking it will be more accurate so I think they are pretty interesting results it seems like they're kind of on par like open Ai and cohere are very on par but it seems like cohere at least from you know what I've seen here is slightly ahead of open AI in terms of performance on that single Benchmark which is not the best comparison in all fairness but also slightly cheaper in the long run because of the embedding size but again like the everything here is so close that it's going to depend a lot on on your particular use case so it's not that Co here is better than open AI it's just that in some cases they probably are better and in some cases they're 


  Implementing Cohere multilingual model 

 probably cheaper as well so that's definitely something to consider now how do we actually use coherent for embeddings so we're going to be focusing on the coherent multilingual model and in this example we're going to be running through is it's not really my example I've taken this example from nose rhymers based on a webinar that we are doing together he's basically put all this together and I've just kind of reformatted it in a way so that I can show you how it works and also show you kind of focus on the multilingual search component of cohere and show you how it works so let's you know let's just jump straight into it right so the first thing we need to do is our pip installs so we have hook and face data sets here we're getting data from there go here and Pinecone client we're using the grpc client so that we can upset things faster we'll see how to use that soon now I actually have a couple of notes here so what a couple of things to point out with coher's multilingual model is that it supports more than 100 languages I think the the benchmarks that they've tested it on covered 16 of those languages or something around there and 


  Data prep and embedding 

 of course you can create embeddings for longer terms of text okay and this is the dates that we're going to be using um it's some straight data from Wikipedia that Nils put together I believe and just hosted under coher on I can face datasets so let's have a look at these for now we're just going to look at the English and Italian and we're going to see how we would put those and create a search with them and then what I'm going to do is switch across to a example where we have way more data in the database and that covers I think nine languages but it's pretty interesting so this is what day looks like we just have like some text in the middle that's what we're going to be encoding right so if we're embedding these chunks one at a time maybe it would be more expensive using cohere but I think in reality we could we could put a lot more of these together so we could put together like five of these chunks or or more and it should work pretty well so okay let's go down here you need a go here API key so to get that you would go to here so you type in dashboard dot coher.ai okay and you'll probably have to log in if you haven't already logged in to go here and then you go over to the left here and you will find some API keys from there you take your API key and you just put it in here okay I have my API key stored already in a variable called cohere API key cool then this is how you would embed something right so we have a list of text that we would like to embed and we just pass them to this Co dot embed So Co is just a client that we've initialized up here so co.embed text and then you have your model the this is the only multilingual model that cohere offers at the moment but I mean like if you compare that to open AI right now they just offer English models so I think they've taken the lead with that which is pretty cool pull embeddings from response so okay we create our embedding six gives us like response and it has a lot of information in there but all we need are the embeddings right so we're just starting those out and obviously dimensionality of those embeddings which is going to be 768 so that's the dimensionality and then we have two of those Vector embeddings there right so we have two 768 dimensional vectors because we have two sentences all right now that's how we would use coheres embedding model but before we move on to actually creating 


  Creating a vector index with Pinecone 

 out our index where we're going to store all of those embeddings we need to initialize an index so we're going to be using Vex database called pine cone for this now pine cone again we need API key which we can get from over here again it's free so app.pinecone.io I'll just copy paste that Okay cool so come over here I can already see I have a couple of indexes in here uh if this is your first time using Pinecone it will be empty and that's fine because we're going to create the index in the code but what you do need is your API key all right so your API key is here you copy that take over into your notebook and you would paste it here now again I've stored mine in a variable then you also have your Environ now your environment is next to the API key in the console right so here Us East one gcp your environment is not necessarily going to be the same as mine so you should check that okay great so that has initialized and then we come down here and what we're going to do here is initialize an index which is where we're going to store all of these embeddings now you give your index a name it doesn't matter what you call it okay you can call it whatever you want but there are a few things that are important here that we should not change so dimension Dimension is the dimensionality of your embedding so it's coming from Echo here right this is where I mentioned before there's the the price advantage of using cohere when dimensionality is lower like 768 it's going to be cheaper to store all of your vectors if you are needing to pay for that storage so we need that and our index needs to know this value okay so it needs to know the expected dimensionality the vectors we're putting into it then we have our metric which is dot products this is needed by coheirs multilingual model if you look on the I think the about page for the multilingual model it will say you need to use that product and then these here you can you can actually leave them empty the default values for these are also okay but I thought I'd put them in there so S1 is basically the storage optimized pod for Pinecone which means you can put in about five million vectors in here for free without paying anything and then there's also P1 which is like the speed optimized version which enables you to put in around 1 million uh vectors for free okay and then pods is the number of those pods you need so if you needed 10 million vectors we would say okay we need two parts here cool but we just need one we're not paying that mission now so we would you know we'd run that then we connect the index we use this grpc index which is we can also use index so we could also use this but grpc index is just more stable and it's also faster so we're doing that and then we're going to describe the index stats so we're going to see what's in there now I already created the index before so for you when you're running through this first time this will actually say zero for me I've already added things in there that's why it's at 


  Embedding and indexing everything 

 200 000 and 100. now with the embedding model and Vector index so we can move on to actually indexing everything so basically we're just going to Loop through our data set and we're going to do what we just did so we're going to embed things with coherent and then what we're going to do is with those embeddings we're going to add them into pyco right actually I don't think I showed you how we do that but it's really simple it's actually just this line here but let me explain what we have here so batch size is the number of items that we're going to send to go here and then upset into Pinecone at any one time the Lang limit so this is a number of records from each language that we would like to include that would like to embed and add to Pine Cone we have our data here so I'm just formatting this so that it's a bit easier later on when we get to this bit here and errors and this is just so we can store a few errors because every now and again we might we might hit one and I'll explain why it's not necessary but there are ways to avoid it basically that I'm not that hard but for simplicity's sake I haven't included them in here so here I'm just saying you know don't go over the Lang limit and then we're going through English and Italian one at a time we get a relevant batch from our data which we've created here so it's actually just see the iterable of the data first English and Italian we extract text from that we create our embeddings using that text then we just create some IDs this is just an ID variable that was in the in the data up at the top here ID and also including text in there as well okay and then what we do is we create this metadata list of dictionaries now each dictionary is going to contain some text a title from the record the URL of the record and also the language so English or Italian and then what we do is we add everything like this okay so it's pretty straightforward there's nothing too complicated going on there the one thing is that I have added in there is occasionally so you know we saw those the text earlier on it was you know they were pretty short Trump's attacks but for some reason not all of them are like this it's kind of like a messy data set so some of them are actually quite long and they actually exceed the metadata limit in Pinecone which is is 10 kilobytes per Vector so basically we can add up to around 10 kilobytes of text with per Vector in in Pinecone but some of them go over that and they will throw an error so I'm actually for now I'm just skipping those but in reality what you do is you chunk those larger chunks of text into smaller chunks and then just add them individually or just saw your text somewhere else it doesn't have to go into Pine again right now I've already run this I'm not going to run it again and yeah I can just come down to here I can run this we have our describe index 


  Making multilingual queries 

 and look the same as it did before for me okay cool now what I want to do so this is a more I think more interesting part is searching so to search through uh what we do is we take a query we embed it and then we so embed is exactly the same as what we did before with cohere and then we query with that embedding xq here and we return the top three most similar items and then we want to include metadata which is going to contain our text title and a couple of other things the URL is pretty important and then we return it in this kind of format we include this this is pretty good idea from those we include the translate URL that will just allow us so when we're getting Italian results or any other language results we just click on this it will take us to Google Translate and we can see what it actually says so let's run this and we can try both of these that I'm not even sure if they work that well because we don't have that much data in here but we can try okay I I don't know any uh okay yeah so number three here so this is you know he's famous in Italy but I think less famous outside of Italy um so if we go to here we see translation and you can see okay it's on the most important and prestigious personalities and fight against the mafia he was killed by kosa Nostra together with his wife and so on and so on right so he he's super famous in Italy but if you look on Wikipedia for him in English I think there's it mentions a little bit about him but there isn't really that much information there so that's why we're getting you know we're just getting like Italian results here and then if we go for this one as well so this is another one I think in the English Wikipedia there's like a paragraph about about this but then if you go to the Italian Wikipedia there is a ton of these now in this I don't have yeah I don't I don't I don't have enough data in here so let's let's switch across to the larger data set and I'll show you uh what the results look like that which are much better okay I can ask about oh this one here so what is the mafia Capital case okay and we get Mafia Capital here and if you go to translate you can see yes that is you know that is the thing that I was talking about and then if we go to Wikipedia here I just want to point out okay so you get all of this text which is tons if we go to the English version okay so I'm searching in Google here Mafia capitali what what do we get right we get this literally three paragraphs so basically nothing so you can see why it would be bringing the Italian stuff here rather than the or why being able to search the Italian stuff is useful even if you're you're speaking English now another one we're going to ask what is Aaron cheat it's Aaron Chino but I'm going to spell it wrong just to point out the fact that it can actually handle that maybe Aaron G oh I slot it no no I I did get it right okay so this is wrong uh the one I did before was was actually correct I kind of half expected to go wrong anyway right so it's uh we can go on here see what it says so Aaron Gino is specialty of Sicilian cuisine it's very nice if you ever had the chance to try it you should have this with a pizza so arancino pizza and fury the zuka it's amazing it's like my favorite meal okay so let's try one more who is Emma Maroney is that right yes Okay so go to here and I I don't actually know who this is 


  Final throughts on Cohere and OpenAI 

 so I hope this is correct it's apparently this person okay so that's it for this uh introduction to coher I feel like it was a bit longer than I had intended it to be uh but that's fine I'm hoping that it was at least useful and we we kind of went through a lot of things there so yeah I I just wanted to share this it's a alternative to open AI I'm not saying it's necessarily better I'm not saying it's necessarily cheaper I think that is very much going to depend on your use case what you're doing and many other factors right you can train these models for example if you're able to train them then you know you're probably going to get some some pretty good performance as well and I suppose like one big factor here is actually the multilingual aspect of this model at the moment openai doesn't have any more tilingual models or not they're actually trained to do that some of them I think can handle multilingual queries relatively well but then they haven't been trained for that and you know this can be relatively problematic you know especially when you're dealing with multinational companies or just companies that are not American or English or Australian as well I'm not gonna forget you any in the rest of the world speaks different languages so having this multilingual model is it it's pretty good so yeah I mean this is still very early days for cohere I'm pretty excited I know they have a lot planned and that'll be really interesting to to see but for now I think we'll leave it there I hope all this been useful and interesting so thank you very much for watching and I will see you again in the next one bye 


 thank you