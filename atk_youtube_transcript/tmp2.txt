hi everyone it's my pleasure and today to introduce Eustis Eustis is a postdoc at the baker Lab at the University of Washington he obtained his bachelor's and Master's in mathematics and his PhD in biophysics all from the University of Cambridge today Eustis will talk about his work on protein mpnn I'm sure all of us have seen the work recently come out and in science and we're all very excited to uh to hear what you've got for us so I think we're going to keep questions uh for the end but feel free to to put them in the chat in the meantime take it away you sis uh thank you for the introduction um so I'll be presenting on party MPN and please note down the questions and then ask them at the end so first of all I'd like to thank all protein MPN team so there's lots of people who've been working on it some both creating database training models and also doing experimental work and usually when I present the work I go into the details how the project started and present all the sort of like nasty bits but I'm just going to reverse and explain what is part of the MPN and present experimental data first and then we will dive into some of the metrics so the protein mpnn is the model to map protein backbone geometry into protein sequences uh so this structured a sequence and sequence of structure mapping and we all know sort of alpha Falls and Rec centerfold predicting from a sequence structure so some of the experimental results one 


 Hallucinating Symmetric Protein Assemblies 

 of the papers that came out at the same time were partly mpnn is called hallucinating symmetric protein assemblies and in this case this is a figure from the paper the so Lucas puzzle and Alexi in our lab they've been playing with hallucinating or creating generating proteins using Alpha fold so in this case with the interest in it cyclic proteins so in this for example three copies of the protein of the exactly the same sequence I introduced initially and then we can use R4 to predict the structure and impose some losses so for example ask the model to be very um very certain so high pldt and PTM but also introduced a cycling loss and then do Markov chain Monte Carlo mcmc to try to mutate some sequences and keep doing this Loop until we get really the metrics like we want the and then at the end I will show that these designs need to be redesigned with 14 npn and some sort of model that Maps structure the sequence because the original design did not work and if we look at the sort of the solubility so the how much soluble yield there was from different designs so this original Markov chain Monte Carlo hallucination for these homologous those who have the solubility results on guest median is about 10 milligrams per liter but then so a bunch of them are really low yields so they sort of say insoluble proteins they aggregated redesigned the same exact backbone supporting the impended and increase the soluble yield quite a lot this is a log scale so now we're talking about hundreds even close to thousands of um of milligrams per liter um also from these uh oligomeric designs they we got Crystal structures so this is showing seven Crystal structures some of the homo dimers and uh trimers and so on and the gray are the models and the salt structures are colored and there's some interesting designs the beta sheets and Alpha helices wrapping around so that's quite exciting and they all match quite well the design structures um I guess I should say that all of these were so I'll probably mention before later but the redesign sequence of Courtney and pin and again we predicted by Alpha fold to make sure that there's a match between what we try to design and what we redesigned reporting in pnn also we can hallucinate these like large structures so these different rings of different symmetries of c18 6 so there's a six units and there's internal symmetry of 18 and this showing at chromium a negative stain microscopy results so that's quite exciting there's lots of big structures and our show property and pnm you can redesign these symmetric proteins making sure that the sequence stays symmetric um in the in the in the I guess in the uh Institute for protein design we also have the Kings lab they're working on different nanoparticles so in this case uh Robert de Haas redesigned some of the tetrahedral nanoparticles so in this case the original design was then the Rosetta uh class design and the interface was not forming this particle was not forming together but then redesigned the party and K N quite similar backbones that fail initially from earlier papers now give really nice Crystal structures that match and these particles are really stable so that's quite exciting um 


 Incorporating these Short Linear Motives into Genova Design Protein Scaffolds 

 one of the other examples is that who are they in our lab was trying to design incorporating these short linear motives into Genova design protein scaffolds so the idea is that for example if this sa issue domain is to bind this native peptide so we want to come up with the protein structure this de Nova small peptide that would support this green structure so originally it was designed with Rosetta remodel their design and packing but there was no binding signal but then redesigning the protein opinion whole uh this orange structure keeping the green structures the given native so trying to support it provided binding and then just to test whether some of these threads is actually supporting the structure they were modifications made from spark Gene to aspartate and it indeed was supporting this small green structure uh the more papers coming using coming out they're using that are using protein in pnm so this is one example from the uh from Nate Bennett it's called improving the neuroprotein design uh using deep learning so some of the some of the failure modes for Designing uh proteins is that if we're trying to design a binder for a Target sometimes the binder itself doesn't form so this type of one failure but sometimes also it doesn't bind so it's a sidewalk to failure and there's some examples sort of showing metrics in this case the metric is Alpha fold success of the predicted align error using different methods like Rosetta design protein and pnn and also people try to run protein and pnn and run fast relax to move the backbone a little bit and then run protein and pin and again and you can see that the third method of running approaching independent and fast relax in Cycles is increasing this Alpha false success rate at least sort of tricking alcohol to think that those are good binders and also in this paper there's some benchmarking results showing that the previous results Alpha fold prediction line error can be used to discriminate better binders from the worst binders another paper that you supporting the opinion is our lab is related to the conclusion hallucinating this symmetric oligomers but in this case think about creating proteins with pockets so these pockets would be used for binding different small molecules so the same idea of Designing these oligomers but then redesigning the protein and pen and removing the symmetry so allowing these three units to be of different sequence so the sequence would become different so they wouldn't be perfect homologous but they would have different genes but the they call it pseudo symmetric so the backbone is symmetric but the sequence is not symmetric um so it's always I guess fun to do a little demonstration so I'm gonna show this hugging face created by Simon and try to predict this um this top seven so this is the Nova design party in one qos this figure was made for the ipd so instantly property and design celebrating 10 years so we can try to redesign this sequence and then predict the buffer fold um so this is this is this hugging face setup there's also a GitHub code where it has maybe slightly more options how to make but this is a very simple setup so in this case we can either upload a pdb or write the uh write the code so one qis was the code and if we go to the settings we can choose which sequencer designs so this design there's only one chain a I'm sorry we'll let's say design for sequences which is the sampling temperature is to go small sampling temperature we can choose what sort of model you use and I'll talk about these different models train a different amount of noise so let's use the default and then we can just run the model and design sequences so it is quite fast that's it we we this is what this was the original sequence and then now we redesigned four sequences and the model outputs the temperature that were designed at uh what is the score so this is a negative log probability so lower the better than the sequencer probably what is the match between the output sequence and the input sequence what was the model name so we have about 40 45 sequence similarity from the input 


 Structural Addition 

 native crystal structure to the design sequences and now we can do structural addition so we're going to run Alpha fold three recycles on all of the sequences and it's going to take about 30 seconds so I'll go back to this later to see what are the what is the match between the input and the outputs um so if we continue I'll share my slide one second [Music] okay so while this prediction is running I'm going to continue the describing the model so we have a structure the sequence model and some of the analogies that often people think in protein machine learning are these sort of I guess computer related image image generation or text so in this case I'm sort of showing the this task of mapping from protein backbone into the sequence and then from the sequencer to the backbone is an analogy between mapping an image describing it as in a caption so in this case this is a small cactus wearing a straw hat and a neon sunglasses in the Sahara Desert and then using the text to generate the image so in this case this image was generated by Imogen giving this text prompt and generating the image so this is sort of like a generator model that will produce many many solutions in this case this protein name pianon is more like mapping a image into the text but the distributions are quite different in this case it's a 2d go to pixels in probably in a protein world it's 3D coordinate of the atoms and from this sort of like a translation perspective the number of residues are matching in the coordinates and also in the residue so we have a matching lens and there's only 20 layers available so somewhat simpler maybe task the task for prediction structure is quite difficult and it requires homology and other 


 Problem Statement 

 information uh so the problem statement what sort of was the idea behind protein MPN is to come up to the model that can sample sequences that are highly likely we could model multiple trains we could fix parts of the chain and we provide uncertainty about the samples so in this case this could be an example of two chains one chain here and one chain here and we want to maybe fix everything and just redesign the interface so just redesigned the red Parts giving all the rest and the model we'd like to Output the sequence and also the probabilities of the sequence experience that when you gave I the one at 9am was 42 and then you added more so the examples would be one-sided binded design uh homologous design enzyme design and so on uh the training data was collected by Yvonne and schenko so it's a similar training data for I guess uh Rosetta fold but in this case we're using the pdb bi-units so these biomets we collect a single chance of ppdb costed at 30 C plus identity we're using three and a half anxon resolution cut off and taking the complexes that are smaller than 10 000 rescues and so in the training data set there would be a bunch of homo dimers uh homo ligamers and the we are asking for the model to only predict one single chain that we clustered in the context of everything else so it would be a very simple task if we ask for example given the blue chain that's exactly matching the right chain to predict the sequence that would be the model can just copy over so what do we do we check the sequence similarity to see if it's more than 70 similar to the from Two Chains and if they are then the model has to predict res sequence of all the chains instead of only one chain so preventing this data leakage from the homolog American similar sequences and the objective for the model is to minimize the caloric per centropy so the this P distribution is the original distribution so in this case for example it's a glutamate so it's a probability of one this is an initial distribution and the model is outputting a distribution q that would be some distribution that the model thinks it is and then we'll just compute the the log probability of the correct amino acids of glutamate and send them over so in this case the model is using Auto aggressive decomposition so for example given a small bit of the sequence that was already decoded and the backbone the model produces the probabilities of the amino acids and then we sample from this distribution to get to get a sample so in this case it's sampling e and then using this information we predict again a new probability distribution for the next amino acid and then again samples so in this case it's Proline so the model is multi-step or Auto regressive is decomposed that one amino acid at a time it is predicting distributions and then we sample from these distributions uh so just comparing this left to right decoding that is more common in a language translation and probably an opinion was using arbitrary decoding so doing training and doing training a random decoding order was given and the model had to learn all the different decoding orders so in this case for Left Right decoding we have some text that's on the left and we just predict the next as a function of everything on the left in the arbitable decoding we can for example still wanted to code these two amino acids the model can use both the context on the left and on the right whereas the left radical would not be able to use for example these three letters to the right the input features to the model so talking about these geometric features are first of all we sort of assume that the local context will be the most important so we're forming these local neighborhoods what we raised you in terms of DC Alpha sulfur distance and then the input features for to tell that residue I and J are for example in different chains so the green uh green nodes or these circles are chain a the red one is chain B so we would give the plus 32 plus minus 32 in the primary sequence so example this residue would know it's one residue to the left of this other residue or two residues to the left of the this residue if the residues are in a different chains then we just give a sort of binary indicator say that these residues are in different units so this is similar to the alpha fold uh encoding of the positional encoding but plus also taking care that there's no it doesn't matter what the chain a is called a or called B so that there's no egg a goes before B so it's just the indication what is the same chain of different chains and then on the edges we also have the distances so we have this radial basis functions I'm not sure what they are for distances between backbone atoms so ncl2c and oxygen and there's also a virtual C beta that is calculated as a function of the other residues to get to test whether this Assumption of the local neighborhood is correct we can train neural networks and to check sequence recovery as a function of the neighbors and the graph so this is for example using 16 areas neighbors 32 years neighbors 64 years neighbors and you can see that the performance of in terms of sequence recovery is uh converging it's not getting better if we make the graph uh more and more connected so it is mostly the local information that is important to predict amino acid identity so these distances or these input features are these 25 distance between nclfc and virtual C beta so in this case this is ncl4c oxygen and C beta so we have all the interdistances that are encoded it's like radial basis function so we can imagine this like little channels that are exponent exponential of the true distance minus this middle of the distances of these bins divided by some standard deviation so they go only from zero to one everything is very nicely normalized and if the distance is really big it will just get everything with zeros and if the distance is very small there'll be maybe a very small clip for the first bin we try to do all the different variations that have angles so using given the hero angles as an input relative frame angles and so on it really is does not work as well or doesn't add on top of the what the distances give so the distances are probably a better inductive bias maybe it's easier for the model to learn what's going on so just giving distances between all the params so things we have so far is we made this graph that is local graph and we have the features on the edges that are this positional encoding and also these geometric features and then we want to predict these probability distributions per node given the rest of the amino acid so amino acids will be encoded as nodes that some of them are decoded and then we have the true we can calculate the credible cross entropy laws since we have to run this model as many times as their amino acids it is useful to somehow discretize the model so basically you have a backbone encoder and a sequence decoder and that's very often done in the language transition to having an encoder and decoder so the encoder job will be to encode the backbone and that the decoder job will be to run iteratively multiple times to decode one amino acid at this time so let's dive in slightly deep a little bit deeper into the backbone encoder we're seeing takes this uh distances between different atoms as node features and then there are no no sh features and the no node features so the model is having these edges and nodes those are just initially zero so we can just unbe omitted and it's a message passing neural network that updates the nodes and then updates the edges to come up with better nodes and better edges that are the three layers of those that can be run and then the decoder takes the final encoded edges and nodes and also has the sequence into the nodes uh the one that was already decoded to produce probabilities from which can be sampled during training we're not sampling we're using teacher forcing to just run in one pass all the decoder but doing doing the inference we can sample one at a time and run decoder multiple times so the model architecture is really lightweight it's about 1.7 million parameters so just three decoder layers three encoders 3D code layers takening Dimension is 128 comparing with Alpha fold which has about 100 million parameters so this is a really cheap to run even on CPUs and can run on really big proteins like 10 000 of residue so more uh one of the important aspects in training these models is adding a backboard noise so since the crystal structures probably have some artifacts they're very uh especially the higher resolution ones and in when people are designing things we're really not sure about the backbone how does backbone exactly uh is positioned so we want to make the model that is not very sensitive to the input uh input coordinates there's some slack so we're adding gaussian noise in training to all the coordinates but then during the inference there's no need to add any noise because the model is already for example not paying attention to the scales of 0.2 angstroms so this is one of the benchmarks the whole idea for the paper was born from the John Ingram's paper journal to models for graph based partying design and we tried to build on top to see what could be improved so this is a table showing that if we take exactly the model from John Ingram's method and then we just change one thing at a time so this model is not going to exactly match the model that I just described here because in this case there were some dihedral feed the hedral input features and so on but if we take the Baseline model that is exactly in this paper and then we just change one thing at a time so experiment one is adding these distances as an input features we can see that the segments recovery straight away goes from 41 to 49 so there's definitely adding the inter distance feature was a really helpful inductive bias uh and then updating encoder edges in the normal sort of vanilla and pnn there's only no WS there's no Edge updates it's giving a small increase compared to the Baseline and then if we combine one and two we get a slightly higher boost and then if we this all the Baseline experiment one two three are all left to right decoding both aggressive models if we switch from the left to right to a random decoding order so the model has to learn all the possible detail in others there's no decrease in performance it is almost the same and one interesting thing to notice is that the this noise level of doing training is that the original models were run without any noise and we also test what would happen if we add a very small 0.02 angstrom side and deviation gaussian noise and you can see that the performance is on the pdb crystal structure is getting lower so even at the highest it's about three percent different so it means about three percent of the performance is coming from this really really tiny amount of distance that the model is either picking up on the crystals that maybe the distance between C Alpha and C Alpha is indicating that it's going to be specific amino acids but if we take off of all models that are idealized backbone the difference is much smaller or sometimes it's even better it sort of is better to train the model that is robust it's not overfitted to the crystal structure so that's an interesting uh observation all of the other results that I'll show will be for the model that is actually uh protein and pnn on the GitHub so this is the only one that is more like a toy example so comparing the Rosetta that's been mainly used in uh in a bigger lab I'm plotting the sequence recovery as a function of the average C beta distance so I wanted some measure that is independent of the side chain parking that would show how buried the residue is so we have a core residues that are in the core of the protein and then the surface residues and I'm showing the fraction of the residues that fall Within These distances for the nearest sibera and this is this virtual severe uh even if natom doesn't have C beta we'll calculate virtual C beta that on average there is for example 30 of residues at this 6.2 angstrom eight neighbor close the distance we see that pregnant is doing better than rosetta in both in the core and on the surface and they're both uh kind of matching at the core there's a really high certainty with almost 90 percent uh accuracy or sequence to copy the model can recover what was actually there and then on the surface it's much lower but they're both almost monotonic functions of this neuroscibility distance which is an interesting suggest that the the ability to recover the sequence is it's a function of the geometry it's saying how much of the constraints I have from other residues around this is not incorporating any function or any other things if we look at the protein by protein basis so one dot is one protein I think this is about 400 monomeric structures recovery is better than rosetta in most of the cases except this one point and there's definitely a correlation so this correlation is probably coming from intrinsically how how well packed the backbone is we could also look at the sequence recovery so the model was shown on complexes that has been trained on the homeowners heteromers and interfaces so we can plot out how it performs a sequence recovery for these different 


 Sampling Temperature 

 groups so when using the model there's this sampling temperature so I'm going to just quickly explain what the sampling temperature is so what it mainly does it makes the amino acid distribution sharper so probably a pen index the background is an input and then produces these called logic so lodges are the final outputs of the neural network before using any other things and probability distribution is formed as the this exponential exponentiation of The Lodges divided by this temperature so it can adjust the relative scaling between the largest of course it's normalized so it adds up to one so making the temperature small takes a distribution very sharp and if we increase the temperature we make it almost uniform we can forget training time the temperature is set to 1 according to the loss so we can produce a higher probability sequences probably by getting some bias by reducing this temperature and I'll show how this temperature affects different things so for example sequence recovery as a function of the temperature is for these monomers and polymers is slowly decreasing so we're sampling less and less matching sequences to the inputs but then the sequence diversity if we take match if we try to design multiple sequences and see how similar these sequences are the diversity is increasing it's interesting to notice that at zero temperature so it means taking Arc Max this sequence diversity is still not exactly zero it's about ten percent and this ten percent Sigma diversity is coming from the this random decoding order that these models are slightly different from each other and we if we look at the sequence similarity uh per uh this burial look at the core versus the surface so different temperatures we see that the core is quite the same but then the surface is quite different the sequence similarity is about 75 for the surfaces at the low temperature and only about 47 at higher temperature which means there's lots of uncertainty and the probability Solutions are quite wide on the surface the model is not sure what 


 Amino Acid Biases 

 amino acid should go on the surface but the core is very well resolved if we look at the amino acid biases so in this case I'm plotting the the model's bias minus the pdb bias so we can see the things that are above it means they are they're overrepresented and the things below would be underrepresented so if we look at the summation of the temperature the lower the temperature the higher there is the bias so 0.1 there's more negative which are glutamates if we look at the temperature one which the model was trained it's matching quite well so there's only a really small bias with respect to the true distribution but this bias is is quite interesting that the model is trying to put more glutamates and lysines on the surface positively negatively charged amino acids and it's reducing polar amino acids so the model sort of find out that the best guess is to put on a Surface charge amino acids and then in don't think about the Polaris if we compare the this bias with Rosetta design bias Rosetta has some issues that people knowing so example of all residues there's a reposition of alanines in the core sometimes the more the Rosetta is now able to pack things in the course so it just puts alanine things are too close uh the too many uh tryptophans on the surface and so on so the different sort of biases that Rosetta is bringing in also these models are sensitive to the backbone resolution so whenever someone is reporting sequence recovery you have to be to check what is the backbone resolution so this is showing the sequence recovery is a function of the pdb resolution and there is a clear Trend uh for for example three three angstrom resolution protein versus one or 1.5 x composition protein exactly the same story goes to Alpha fall models so the more confidential is about the structure maybe it's closer to the crystal structure because it was trained with Crystal structures the highest sequence recovery protein impedance can give it was also important to produce produce predicting predict uncertainty so this is what I'm plotting is the sequence recovery at these two different temperatures 0.10.5 as a function of the negative log probability so probability of a sequence giving a structure and when we're designing something we don't know the signature carrier because we don't know what we're trying to get but we we get from the model the negative log probability of what the model designed so we can still get an idea how confident the model is about this specific backbone and sequence match uh so there's quite nice line Trends lines between these sequence recovery and the negative log probability uh and then for example in this case I try to design 32 sequences for these backbones and then rank them by the log probability so the most probable sequence is this one and then this probability sequence is on the right you see at the higher temperatures there's only a very small uh sort of distance between the least likely sequence and the most likely sequence whereas at the these higher temperatures where there's maybe more Randomness there's a bigger gap between for example 48 and 50 sequence recovery so one could sample many sequences with 50 was open five temperature and then rank them or by the score and pick the ones that satisfy specific uh features that they want in this case they will get more diversity so now going back to this sequence sort of this thing about the backbone to sequence and sequence the backbone matching what is the other way to sort of try to think how can we evaluate see how good the method is and what sort of people want they design a sequence so one of the things that people do is it's like forward folding okay when I design the sequence how would the sequence look uh so in this case Alpha fold using off fold in this single sequence regime because often the sequences these design sequences are quite different from the Native sequences so we're not even collecting multiple sequence alignments it's just using a single sequence with Alpha fault to predict the structure and then check how well this infrastructure matches to the uh output structure match the infrastructure so this is a structure a model that we try to design we come up with sequence and then we just want to double check how good the match is so if we look at this native sequences so 400 backbones protein diagnose and take the native sequences without any homology data that fold map matching between the input and output is really low because it's really hard to predict native sequences with just a single sequence without any evolutionary information those sequences are probably not very ideal but they're taking the same backgrounds and then redesigning the protein and pnn they become way more idealized more structurally ideal for the alpha fall to predict and we get a different distribution that a bunch of sequences can be repredected to map the input and similar idea for example this for this do not have a designed in TF2 so there's like a rosetta design in tf2s and then redesigned the same bag most of protein pnn protein opinion gives sequences that are more preferred compared to over seven sequences according to After Fall so that's also good to see the match between the input and the output uh so there's also we trained with noise so we can see how does the noise affect sequence recovery and also how does it affect other fault success rate so that's quite interesting that adding this very small amount of noise in this in this pdb bi-unit regime reduces the sequence recovery from 55 to almost 51 so this is this one I'm talking about the crystal artifacts of the pdb but then as we reduce the increase the noise more and more the sequence recovery slowly goes down but what's interesting is that this Alpha fault success rate so for example having the input and output match being 90 ldtc alpha or 95. it sort of increases as we're adding more noise so it's also the model is probably becoming more robust and giving more idealized sequences that could be easier predicted by Alpha fold and it's also interesting that if we look at the similarities if we take the same pdbs and sequences and generate a bunch of sequences and see how well those sequences match to the this pssm so evolutionary generated similar sequences that the model with more noise generates sequences that are more similar to the pssm sequences so we can think of this more of the consensus-like sequence and I don't know whether it's just a coincidence or Alpha fault is actually better at predicting consensus-like sequences also so when we think about this consistency check the input backbone and we have one model property in pnn and then have a sequence and another model off of all the question is well it doesn't match why it doesn't match it's protein npn came up with that sequence or is it off of all that it couldn't predict the structure so this is showing that this Alpha fault success rate to match these 1995 cutoffs is the number of recycles and you can see that having one recycle and three recycle being a baseline so this 3.6 percent and 16 percent uh success rate increasing to six recycles you get about 20 boost and then increasing another six times recycles you get another almost 20 boost so it's sort of showing that the structure prediction is the hard part in this encoder decoder regime so often even sequences that maybe couldn't be particularly by off of all it could be still folding into the correct backgrounds so as I guess the structure prediction methods get better there's more chance to sort of check yourself how well the input and output is matching uh one of the last things also to talk about is that once we have this model that takes the backbone and produces distributions we can think about combining these distribution so in this case I'm showing is that what if we want to design this multi multi multi-state backbone so we want to sample distribution that would if we have either two chains or two states that would so I guess in this case I'm showing this for homo oligomer design if I have chain a and chain B and I want just one sequence that would be a and b exactly the same sequence so I can predict the largest both of the sequences and then average them out to get this average blue distribution from which I can then sample in more General we can predict multiple backbones either they're similar backbones or different backbones in different states that could be and then sum them up in some linear way where the beta is also a linear coefficient that is mixing these Logics and they could be positive or negative for positive design or negative design multi-state design coming up with a final sort of energy we can think and then making one distribution from which we sample and you populate all the states and you do that again and again so that that's an there's some interesting extensions of this so some final points with product implant is available on the GitHub it's open source uh it's fast and lightweight there's almost no expert customization compared with resetter except choosing which regions they do redesign and picking some temperatures it rescues a bunch of designs in the baker lab there's bias towards the surface charge amino acid and maybe that's contributing to the high expression yields and solubility you can do symmetric negative multi-state design and so on uh I'd like to thank all the people in the bigger lab that worked on this and then we can maybe check the the results from the alpha foot forward folding and then I can take some questions so if we look at the here we have four sequences we can rank them by rmsd so it's the best 0.77 angstrom rmsdc Alpha those are the models course so how confident mpnn is about these sequences sequencer copy plts and we can look that the structure is matching quite well to the input structure that we wanted to have so it was quite an easy task I guess to redesign this protein I'm happy to take any questions great thank you so much she's just I think uh we can go over the questions in the order they appear in the chat so we can stop sure sounds good [Music] okay so the first question is can MPN be used to improve a protein like meat compute does um I'm not entirely sure what mute compute does but I think it is possible to use npnn to make proteins more stable or like rescue specific things unless I don't know Giacomo if you want to see more what mute compute does otherwise I'll go to the next question from Kevin Yang does it still work if you don't use the RPF encoding for distances yes it does um I haven't maybe used exactly just raw distances but there are other encodings that can be used or even discrete bins I also try to use discrete bins just to get this idea of not allowing the network to look at this too small of the distances just discretizing for example distance at 0.5 angstroms or 0.2 angstroms it does work in a very simple way um Can MPN take multiple structures confirmations and inputs yes the opinion can take so it takes only one pdb file as an input but you can input multiple backbones in the same pdb and just separate them in space since the model is a local graph sort of model it will only pick on the nodes that are nearby so wouldn't pick on the other pdb that is far away and that's how people for example in our lab are doing this multi-state design just taking two states and inputting them in the same pdb and separating them apart um do you input atoms other than amino acids like chemical ligands and metals so in this version it is not like I'm working on other versions that take in different Metals ligands DNA RNA and so on um what's the best theoretical what's the threat call or the best guess maximum accuracy presumably you don't expect to recover sequence exactly given that many sequences are structural 


 Sequence Recovery Maximum Accuracy 

 homologous I have no idea about the sequence recovery maximum accuracy most of the time I usually look at the sequence recovery in the core because I would expect that this may be more deterministic in the core whereas the surface is is quite it's harder maybe to recover and it's also function of the crystallization and so on I have no idea what what is the what is the maximum accuracy uh it's I I guess I haven't emphasized but in general it's quite difficult to uh to Benchmark generative models the same goes for the example the sequence design model like how do you know when you come up with a better model it's like sequence better sequence recovery doesn't mean maybe that you have a better model so having Alpha fault predictability really helps but also probably setting up some large scale experiments would be a good idea at some point another question is in natural language random order models usually do worse than forward decoding do you have any intuition for why that's not the case here [Music] um one maybe answer is that it's there's not much data so it's quite easy for the model to overfit and maybe learning multiple decoding orders is preventing this overfitting that's one answer another is maybe has something to do to this local sort of neighborhoods that in this whole sequence is not function of the left to right it's these local neighborhoods that are determining and they're they're not ordered I guess left to right there's lots of this non-locality that uh that also could use something that why that's also working um another question from Simon is for the pregnant opinion of specialized particle do you think there is a biological reason why alcohol likes these structures better is there a difference between the different model and noise levels and the alphability of the pregnancy protocol um I'm not sure about what are the pldt so having certainty about the structure what I was saying is that they were better predicted the line errors between the chains for the binder imagine the pl LEDs may be also better I don't know exactly what are these small changes that fascolax is doing to the structure that is helping for the MP and design a better structure and it's possible to more confidently predict maybe it's idealizing some small bits but uh yeah I don't know what was the party dependent train only on high resolution structure it's not sure if that's mentioned at the beginning of the talk so it was not traded on only high resolution structures I did try to train on structures that as for example better than doing some resolution two and a half three and a half resolution and the performances are quite similar so I thought it's a better idea to train on more data which means three and a half angstrom resolution maybe it could get even higher lower resolution but uh I guess at some point it would start hurting um also wondering if npnn was trained on both Crystal and cryo structures or just crystals so it was only trained on crystal structures we haven't trained on prior em structures it might be interesting to try my feeling is that they might be slightly too low resolution another idea would be to actually use for example Alpha fold structures as an additional training set um question from DJ Chang would the model work for the multi-state design if so could you give an example come in how robust the model for this case um it does work for multi-state design we know um an example I don't have data yet I guess it'll be published at some point in the future um I believe the previous slides a three and a half angstrom or lower okay someone's responding is the code that integrates in pnn with astral X available um I think it should be relatable available I can ask Nate in a lab that set up the model so protein opinion is being incorporated into Rosetta as a C plus plus code that would be able to run from Rosetta straight away so it should be 


 Solution Conditions 

 data available compatible um have you thought about solution conditions and how to include that I guess that might influence the server side chain biases um I haven't thought about the solution conditions but I did train a neural network without including any uh membrane proteins to try to have it just the model to design soluble proteins so maybe that's one of the ideas too but I guess it also I will train a model to input maybe the outside conditions as whether it's a membrane or a soluble protein input label I guess that could be done really quite easily um how does the model handle different topologies of protein side chain connectivities like gfp 404 um I don't have a specific answer for this question I don't know I have not tried but we haven't seen any specific overfitting on different topologies or specific proteins um can you sample different confirmations of the interacting partner say peptide we're starting with large noise level of temperature Upper sizes that backbone have to be so the model is not I guess sampling confirmations is predicting a sequence maybe I misunderstood the question um but it would be possible for example to search up the backbone have some way to move the background then sample a bunch of sequences and maybe we predict them to get an idea about the movement someone else is asking what is the difference between options V underscore 48 0 0 2 and the underscore zero zero two for modeling so the models that I showed maybe just show again in the GitHub repo and also in here in this notebook the models so V is the version 48 is 48 edges so 48 nearest neighbor service for the graph and 0.02 it means there's 0.02 angstrom noise the model was trained with this model was streamed with 0.2 angstrom this model wasn't 0.3 angstrom noise so those are the models that are more robust to the larger noise levels um okay the next question is regarding our four hallucination of the protein assemblies followed by chronium sequence design it probably have been improved Alpha sequences mainly by making the surface more soluble always the evidence of important mutations and of course binding interfaces um I would guess that it's mainly the surface redesigned there might be some corn or like in between changes too but I'm not I don't have statistics you could ask Lucas and basil and Alexi um someone was asking if no I'm not mistaken for the opinion is deterministic if so have you experimented decoding orders a significant parameter to affect predictions for instance in interface design does decoding from the resist proximal to the fixed sequence improves course and vice versa so I did try to experiment of initial instead of using the random decoding order try to design from the core to the surface from the surface to the core and other ideas there's no clear evidence that it does help to go from some I guess geometrically geometrically inspired decoding order it seems like the performance is about the same uh how long did it uh how long did training take and what soft of Hardware so I train only one GPU uh and it I train on a100 for probably two days so the training is fairly fast um do you think that b Factor would be helpful input perhaps or protein flexibility or disorder it might be helpful but at the same time the the real question is what would be this input when someone is designing protein would they have a idea what sort of beef factor to put in because most of the time the model is used on some backbones that are either some sort of like helices that are parametrically just designed or Alpha full backbones or some other backgrounds so it's yeah I don't know whether it would be someone less than user would 


 The Membrane Proteins 

 know what sort of B factor to use um housing performing the membrane proteins I haven't tested that exactly uh haven't looked exactly at the memory proteins but I do know that it does put create membrane-like proteins if someone is trying to design something that is long and extended having problems is the 


 Is Body Impedance Score Dependent on the Decoding Order 

 membrane protein so it puts Hydra subjects on the surface another question is body impedance score dependent on the decoding order here are the log likelihoods from the model depending on sequence context when you're decoding previous residue it does depend the dependence is pretty small but the score or log likelihood is a function of the decoding order so if one wants to get a very precise score we can run npn in multiple times and get an average and standard deviation to see what is the score usually the sound 


 What Applications Do You Envision Mpn Will Be Used for 

 deviation is quite small and final question is what applications do you envision MPN will be used for so it's already used for different sort of redesigning uh sequences trying to make them more soluble I think there's some efforts in trying to use MPN to make to make enzymes more stable so for example taking native enzymes and trying to redesign parts of them trying to keep the function but maybe make them more thermodynamically resistant higher yields and so on so somehow trying to idealize making the proteins better binder design is using MPN um multi design and all the other sort of sequence related things I guess an interesting uh result from this mpnn work was that a bunch of Rosetta design things ideal helices and so on sometimes they wouldn't work and probably mainly because the sequence was sort of incorrect or not ideal that it was very hard to express I would oligomerize so now this tool is helping to solve this question of if you have a decent backbone can you come up with sequence that work in experiments but now I guess the hard task is now how do you come up with backbones that you want to have how do you come up with functional proteins and so on another question is what we would using amino acid encodings from the pre-trained protein language model as part of the input be helpful for training the model um it could be helpful um I don't know exactly uh I guess yeah one could try try to do that um I haven't personally tried and looked into into that sort of area yet 


 If a Background Has More Diversity at Lower Temperatures Could that Mean It Is More Designable 

 any other questions another question is if a background has more diversity at lower temperatures could that mean it is more designable um so I would say if the model if the backbone probably has uh less diversity at lower temperatures the model is very certain about this thing it would be maybe more designable the model it's probably makes sense to more look at the at the log probability to think about which backgrounds are designable so how confident the model is about the backbones um someone else asking can you speak towards suggesting typical bias values or the bias by residue Json flag so there is a possibility to add biases so for example for one more polar residue so other sort of or maybe less alanines in the design so these values are more just like experimental I think there's an example in the GitHub repo that is suggesting some values but otherwise probably some error in trial uh trial and errors needed to figure out what are those bad seeds I have not tried to design any cdrs for using npn but I think it would be an interesting application I imagine it's quite a difficult task but it would be interesting to see and I don't know whether yes training on specifically CDR Loops would be helpful so all the chains are included that are bigger than 30 residues I think so if if it's in pdb it's lower than three and a half angle resolution and um and bigger than 30 amino acids it should be included all right thanks everyone again 


 stop the recording