

What are Cohere embeddings?

Today, we're going to take a look at Cohere's multilingual embedding model. For those of you that are not aware of Cohere, they are kind of similar to OpenAI in that they are essentially a service provider of large language models and all of the services that come with that. Now, right now they are not as well known as OpenAI, which is understandable. OpenAI has been around for a bit longer, but Cohere is actually a really good company that offers a lot of really good tooling that is actually very much comparable to what OpenAI offers. And that's actually the first thing I want to look at here. I just want to show you a few comparisons.

COHERE V OPENAI ON COST

Points between COHERE and OpenAI in terms of embedding models. Okay, so we're going to first take a look at the costs between these two opening eyes. So, Premiere embedding model right now is auto 002, comes out to this much per 1000 tokens. COHERE doesn't have a like per 1000 tokens for the cost, it actually goes with one dollar per 1000 embeddings. What does one embedding mean? Well, basically every call or every chunk of text that you ask out here to embed that is one embedding. So, upon embedding, the maximum size of that is actually just over four thousand tokens. So, if you're maxing out every embedding is in, you are sending 4,000 tokens to every embedding call, then that means you would be getting this comparable price here, which is actually half price, which is pretty good. 

Now, if we kind of translate this into something that's a bit more understandable, we have like 13 paragraphs is roughly about a thousand tokens. These are the prices, all right? So, with order with OpenAI, it's one dollar per thirty-two and a half thousand pairs. So, here is actually one dollar per 65,000 paragraphs, which is really good, but there is obviously a catch, which is, you know, this thing up here or this one dollar per 1000 embeddings. Right, the chances are you're probably not going to use four thousand embeddings with every call to COHERE. So, 2000 tokens, well, that's probably like 26 paragraphs. If you're embedding 26 paragraphs at a time, realistically, you're probably gonna do much less, right? So, if let's say you're going for more like a thousand tokens, which I think is more realistic, then obviously the price of COHERE is actually double the price of OpenAI in this instance. So, it kind of depends on what you're doing there as to whether you are throwing a load of text into your embeddings or not. So, I think the costs are pretty comparable. COHERE can be cheaper, but it can also be more expensive according to this logic anyway. 

Okay, so one thing I miss very quickly is the on-prem solution that COHERE offers. So, we have it here. Essentially, you can run your own AWS instance, and in the time that it would take you, this is assuming you're running at 100, and the time date would tell you to encode 1 billion paragraphs. If you use COHERE's on-prem solution, you would end up paying two and a half thousand dollars. It's also a lot quicker, and you know there are all the other benefits as well, but I thought when we're talking about cost, we should definitely include that in there. So, you know, it depends essentially embedding size. Actually, you know, this is a good indicator of how much it's going to cost you. So, it's actually under costs. The higher your embedding size, the more storage you need to store all of your embeddings after you've created them, right? So, the embedding size smaller is cheaper. So, COHERE is half the size of OpenAI in this case. So, you know, long term, you would probably actually be saving money with COHERE with this embedding size if you're storing a lot of vectors. So, you know, that's definitely something to consider if you consider this with the embedding cost initially. You know, maybe you're actually saving money with COHERE even if you're just embedding like a thousand tokens or even 500 tokens at a time.

not. 

COHERE V OPENAI ON PERFORMANCE

Long term, you're probably going to end up saving money now performance. So, this is kind of hard to judge because this is a single benchmark that knows where iron is put together and okay. I mean, Co here for sure is coming out on top here. It's kind of hard to say it again like whether this is representative across a board or not, but nonetheless, the two models that are comparable here. Okay, here's multilingual model and OpenAI's auto 002 model, which is English, and this is an English search task. So, it's pretty interesting that OpenAI's best English language model is comparable to his multilingual model because his English model is better. And then there's a Co here rankers. This is an embedding model. It's at like a imagine you retrieve all of your items or you get two chunks of text, and you feed them into like a transform model and compare them directly. It takes basically a lot slower, but generally speaking, it will be more accurate. So, I think they are pretty interesting results. It seems like they're kind of on par like OpenAI and Co here are very on par, but it seems like Co here at least from you know what I've seen here is slightly ahead of OpenAI in terms of performance on that single benchmark, which is not the best comparison in all fairness, but also slightly cheaper in the long run because of the embedding size. But again, like everything here is so close that it's going to depend a lot on your particular use case. So, it's not that Co here is better than OpenAI, it's just that in some cases they probably are better and in some cases they're not.

**Implementing Cohere Multilingual Model**

Probably cheaper as well, so that's definitely something to consider. Now, how do we actually use Cohere for embeddings? So, we're going to be focusing on the Cohere multilingual model, and in this example, we're going to be running through (it's not really my example, I've taken this example from Nose Rhymers based on a webinar that we are doing together). He's basically put all this together, and I've just kind of reformatted it in a way so that I can show you how it works and also show you kind of focus on the multilingual search component of Cohere and show you how it works. 

Let's jump straight into it, right? So, the first thing we need to do is our pip installs. So, we have Hugging Face datasets here. We're getting data from there. Go here and Pinecone client. We're using the GRPC client so that we can upset things faster. We'll see how to use that soon. 

Now, I actually have a couple of notes here. So, what a couple of things to point out with Cohere's multilingual model is that it supports more than 100 languages. I think the benchmarks that they've tested it on covered 16 of those languages or something around there.

**Data Prep and Embedding**

Of course, you can create embeddings for longer terms of text, okay? And this is the data that we're going to be using. It's some straight data from Wikipedia that Nils put together, I believe, and just hosted under coher on I can face datasets. So let's have a look at these. For now, we're just going to look at the English and Italian, and we're going to see how we would put those and create a search with them. And then what I'm going to do is switch across to an example where we have way more data in the database, and that covers, I think, nine languages, but it's pretty interesting.

So this is what day looks like. We just have some text in the middle that's what we're going to be encoding, right? So if we're embedding these chunks one at a time, maybe it would be more expensive using cohere, but I think in reality, we could put a lot more of these together. So we could put together like five of these chunks or more, and it should work pretty well. So okay, let's go down here. You need a go here API key. So to get that, you would go to here. So you type in dashboard dot coher.ai, okay? And you'll probably have to log in if you haven't already logged in to go here, and then you go over to the left here, and you will find some API keys. From there, you take your API key, and you just put it in here. Okay, I have my API key stored already in a variable called cohere API key. Cool.

Then this is how you would embed something, right? So we have a list of text that we would like to embed, and we just pass them to this Co dot embed. So Co is just a client that we've initialized up here. So co.embed text, and then you have your model. This is the only multilingual model that cohere offers at the moment, but I mean, like if you compare that to open AI right now, they just offer English models. So I think they've taken the lead with that, which is pretty cool. Pull embeddings from response. So okay, we create our embedding six gives us like response, and it has a lot of information in there, but all we need are the embeddings, right? So we're just starting those out, and obviously, dimensionality of those embeddings, which is going to be 768. So that's the dimensionality, and then we have two of those vector embeddings there, right? So we have two 768 dimensional vectors because we have two sentences. All right, now that's how we would use coheres embedding model, but before we move on to actually creating...

**Creating a vector index with Pinecone**

Out our index where we're going to store all of those embeddings. We need to initialize an index, so we're going to be using Vex database called Pinecone for this. Now, Pinecone again, we need API key which we can get from over here. Again, it's free, so app.pinecone.io. I'll just copy paste that. Okay, cool. So come over here, I can already see I have a couple of indexes in here. If this is your first time using Pinecone, it will be empty, and that's fine because we're going to create the index in the code. But what you do need is your API key, all right? So your API key is here, you copy that, take over into your notebook, and you would paste it here. Now, again, I've stored mine in a variable. Then you also have your Environ. Now, your environment is next to the API key in the console, right? So here, Us East one GCP, your environment is not necessarily going to be the same as mine, so you should check that. Okay, great. So that has initialized, and then we come down here, and what we're going to do here is initialize an index, which is where we're going to store all of these embeddings. Now, you give your index a name. It doesn't matter what you call it, okay? You can call it whatever you want, but there are a few things that are important here that we should not change. So dimension. Dimension is the dimensionality of your embedding, so it's coming from Echo here, right? This is where I mentioned before there's the price advantage of using cohere when dimensionality is lower like 768. It's going to be cheaper to store all of your vectors if you are needing to pay for that storage, so we need that, and our index needs to know this value, okay? So it needs to know the expected dimensionality the vectors we're putting into it. Then we have our metric, which is dot products. This is needed by coheirs multilingual model. If you look on the I think the about page for the multilingual model, it will say you need to use that product. And then these here, you can actually leave them empty. The default values for these are also okay, but I thought I'd put them in there. So S1 is basically the storage optimized pod for Pinecone, which means you can put in about five million vectors in here for free without paying anything. And then there's also P1, which is like the speed optimized version, which enables you to put in around 1 million vectors for free. Okay, and then pods is the number of those pods you need. So if you needed 10 million vectors, we would say okay, we need two parts here. Cool, but we just need one. We're not paying that mission now, so we would, you know, we'd run that. Then we connect the index. We use this grpc index, which is we can also use index. So we could also use this, but grpc index is just more stable, and it's also faster. So we're doing that, and then we're going to describe the index stats so we're going to see what's in there. Now, I already created the index before, so for you when you're running through this first time, this will actually say zero. For me, I've already added things in there, that's why it's at.

**Embedding and indexing everything**

---

200,000 and 100. Now, with the embedding model and Vector index, we can move on to actually indexing everything. So basically, we're just going to loop through our dataset, and we're going to do what we just did. So we're going to embed things with Coherent, and then what we're going to do is, with those embeddings, we're going to add them into Pyco. Right, actually, I don't think I showed you how we do that, but it's really simple. It's actually just this line here. But let me explain what we have here. 

So, batch size is the number of items that we're going to send to go here and then upset into Pinecone at any one time. The Lang limit, so this is a number of records from each language that we would like to include that would like to embed and add to Pine Cone. We have our data here, so I'm just formatting this so that it's a bit easier later on when we get to this bit here and errors. And this is just so we can store a few errors because every now and again, we might hit one, and I'll explain why it's not necessary, but there are ways to avoid it basically that I'm not that hard, but for simplicity's sake, I haven't included them in here. 

So here, I'm just saying, you know, don't go over the Lang limit, and then we're going through English and Italian one at a time. We get a relevant batch from our data, which we've created here. So it's actually just see the iterable of the data first English and Italian. We extract text from that. We create our embeddings using that text. Then we just create some IDs. This is just an ID variable that was in the data up at the top here, ID, and also including text in there as well. Okay, and then what we do is we create this metadata list of dictionaries. 

Now, each dictionary is going to contain some text, a title from the record, the URL of the record, and also the language, so English or Italian. And then what we do is we add everything like this. Okay, so it's pretty straightforward. There's nothing too complicated going on there. The one thing is that I have added in there is occasionally, so you know, we saw those the text earlier on. It was, you know, they were pretty short Trump's attacks, but for some reason, not all of them are like this. It's kind of like a messy dataset, so some of them are actually quite long, and they actually exceed the metadata limit in Pinecone, which is 10 kilobytes per Vector. 

So basically, we can add up to around 10 kilobytes of text with per Vector in Pinecone, but some of them go over that, and they will throw an error. So I'm actually, for now, I'm just skipping those, but in reality, what you do is you chunk those larger chunks of text into smaller chunks and then just add them individually or just saw your text somewhere else. It doesn't have to go into Pine again. Right now, I've already run this. I'm not going to run it again, and yeah, I can just come down to here. I can run this. We have our describe index.

MAKING MULTILINGUAL QUERIES

And look the same as it did before for me. Okay, cool. Now what I want to do, so this is a more, I think, interesting part, is searching. So to search through, uh, what we do is we take a query, we embed it, and then we, so embed is exactly the same as what we did before with cohere, and then we query with that embedding xq here, and we return the top three most similar items. And then we want to include metadata, which is going to contain our text title and a couple of other things. The URL is pretty important, and then we return it in this kind of format. We include this, this is a pretty good idea from those. We include the translate URL that will just allow us, so when we're getting Italian results or any other language results, we just click on this, it will take us to Google Translate, and we can see what it actually says. So let's run this, and we can try both of these that I'm not even sure if they work that well because we don't have that much data in here, but we can try. Okay, I, I don't know any, uh, okay, yeah, so number three here. So this is, you know, he's famous in Italy, but I think less famous outside of Italy, um, so if we go to here, we see translation, and you can see, okay, it's on the most important and prestigious personalities and fight against the mafia. He was killed by kosa Nostra together with his wife and so on and so on, right? So he, he's super famous in Italy, but if you look on Wikipedia for him in English, I think there's, it mentions a little bit about him, but there isn't really that much information there. So that's why we're getting, you know, we're just getting like Italian results here, and then if we go for this one as well, so this is another one, I think in the English Wikipedia, there's like a paragraph about about this, but then if you go to the Italian Wikipedia, there is a ton of these. Now in this, I don't have, yeah, I don't, I don't, I don't have enough data in here, so let's, let's switch across to the larger data set, and I'll show you, uh, what the results look like that which are much better. Okay, I can ask about, oh, this one here. So what is the mafia Capital case? Okay, and we get Mafia Capital here, and if you go to translate, you can see, yes, that is, you know, that is the thing that I was talking about, and then if we go to Wikipedia here, I just want to point out, okay, so you get all of this text, which is tons. If we go to the English version, okay, so I'm searching in Google here, Mafia capitali, what do we get, right? We get this, literally three paragraphs, so basically nothing. So you can see why it would be bringing the Italian stuff here rather than the or why being able to search the Italian stuff is useful even if you're speaking English. Now another one, we're going to ask, what is Aaron cheat? It's Aaron Chino, but I'm going to spell it wrong just to point out the fact that it can actually handle that. Maybe Aaron G, oh, I slot it, no, no, I, I did get it right. Okay, so this is wrong, the one I did before was actually correct. I kind of half expected to go wrong anyway, right? So it's, we can go on here, see what it says. So Aaron Gino is specialty of Sicilian cuisine. It's very nice if you ever had the chance to try it. You should have this with a pizza. So arancino pizza and fury the zuka, it's amazing. It's like my favorite meal. Okay, so let's try one more. Who is Emma Maroney? Is that right? Yes. Okay, so go to here, and I, I don't actually know who this is.

**Final Thoughts on Cohere and OpenAI**

So, I hope this is correct. It's apparently this person. Okay, that's it for this introduction to Cohere. I feel like it was a bit longer than I had intended it to be, but that's fine. I'm hoping that it was at least useful, and we went through a lot of things there. Yeah, I just wanted to share this. It's an alternative to OpenAI. I'm not saying it's necessarily better. I'm not saying it's necessarily cheaper. I think that is very much going to depend on your use case, what you're doing, and many other factors, right? You can train these models. For example, if you're able to train them, then you know you're probably going to get some pretty good performance as well. And I suppose like one big factor here is actually the multilingual aspect of this model. At the moment, OpenAI doesn't have any multilingual models or not they're actually trained to do that. Some of them, I think, can handle multilingual queries relatively well, but then they haven't been trained for that, and you know this can be relatively problematic, you know, especially when you're dealing with multinational companies or just companies that are not American or English or Australian as well. I'm not gonna forget you. Any in the rest of the world speaks different languages, so having this multilingual model is pretty good. So yeah, I mean, this is still very early days for Cohere. I'm pretty excited. I know they have a lot planned, and that'll be really interesting to see, but for now, I think we'll leave it there. I hope all this has been useful and interesting, so thank you very much for watching, and I will see you again in the next one. Bye.