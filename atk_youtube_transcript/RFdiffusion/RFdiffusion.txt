 It's my pleasure to introduce Joe Watson and David Urgans on their presentation for Arvifusion. Joe is an EMBO post-op fellow in the Baker Lab at the Institute for Protein Design, University of Washington. His current work focuses on developing generative models for protein design. He previously obtained his PhD in synthetic cell biology at the laboratory of molecular biology at the University of Cambridge. David is a graduate student in the Baker Group at the University of Washington, focused on developing deep learning-based solutions for protein sequence and structure design. More broadly, his interest is to advance their ability to computationally design molecules with desired chemical properties using molecular simulation and generative AI. He received his Bachelors of Science in Chemical Engineering from the University of Washington in 2019. Joe and David. Great, thank you so much for that introduction. We're really excited to be here today. I'm Joe and this is David. And yeah, we're really excited to tell you about our work developing RF Diffusion, this diffusion model for protein design. So I just want to start really by acknowledging everyone who was involved in this really fantastic collaboration. This was an extremely productive collaboration, which included many, many people, many of whom were here at the Institute for Protein Design, but also some elsewhere. So Brian Tripp in Columbia and Jason Yeham at MIT, both contributed really significantly to this work. So none of this would have been possible without the people on this slide. So firstly, why do we care about de novo protein design? And this is probably, many of you will be familiar with this, but I think it's useful to start here and to say that, well, nature has really explored only a tiny subset of the possible proteins that could exist. And on top of that, evolution doesn't necessarily select for the attributes of proteins that we desire or care about from sort of a pharmaceutical or biotechnological perspective. Evolution doesn't necessarily select for things like solubility or stability, ease of production, these things that we want in a sort of biotech or clinical setting. And therefore de novo protein design sort of seeks to derive new proteins with new functions, but also have these sort of more desirable attributes for the particular application in question. And I think it's sort of useful to start by thinking about sort of what is the workflow of protein design? And here in the Baker Lab, I think we start often with a sort of structure first approach to protein design, where largely we follow these kind of four steps. And these are the four steps that you kind of need to fulfill to successfully design a protein. And we start with sort of the generation of a backbone. So a structure that we believe is capable of carrying out some particular function, whether that be binding to another protein or perhaps holding an active site or something of an enzyme. And then once we have these backbones, we then derive a sequence or try and find a sequence that can fold up to that backbone structure. So we actually find the sequence after we've designed the backbone. And typically we can generate many more sequences and structures computationally than we're able to test experimentally. So therefore we need sort of good computational filters and metrics that we trust to be able to narrow down our set of designs to a set that we can then experimentally characterize and validate as working experimentally. And what I just want to point out is that with the advent of ML and its application to protein design, there's really been sort of enormous advance in these latter three stages or steps of protein design. Protein MPNN is this phenomenally good neural network for fixed backbone sequence design. So given a protein backbone, find a sequence that folds to that. And then with the development of AlphaFold2 and RosetteFold and other structure prediction networks, we now have these just amazing tools for really rapidly validating that our design sequences do indeed fold up to our design structure. So once we have a sequence, we can just put it into one of these structure prediction networks and ask, does it fold up to the same structure that we designed? And we have really extensive evidence now showing that if you can get really good recapitulation by for instance, AlphaFold, then your chance of experimental success on a whole range of different problems is really high. And then finally, I just want to point out there's also been significant development in experimental characterization of designs such that we can now go from sort of like a design problem or designs on a computer to validated proteins in around a week on the scale of sort of 96 to 384, well, 384 proteins. So, we can basically, if we can crack this problem of backbone generation, then the workflow downstream of that for a whole range of different problems in protein design is really well characterized now. And therefore, given the backbone generation for a whole host of problems is the kind of the bottleneck in protein design, we set out to try and develop new deep learning based methods for designing protein backgrounds. And for this, we turned to this sort of now reasonably well established area of ML, this area of probabilistic diffusion models. And these have been sort of shown to achieve state of the art performance in image generation. And the kind of basic principle of diffusion models is sort of something like what is depicted here. So typically you start with images or known data, so true data, true images, in this case, a dog. And then over a set number of time steps, you add sort of increasing amounts of noise. So, through repeated application of noise, your image or starting data becomes noisier and noisier. So it's sort of intermediate time steps, you have these sort of very grainy images that still clearly bear some resemblance to your starting image, but eventually your sort of final time step, you reach some distribution which bears no dependence on your starting image. So, you've basically erased all of the information in that starting image. And importantly, the structure of the noise that you add each time step is known. This is typically Gaussian noise or a known variance. And through repeated application of Gaussian noise, you basically converge to a known distribution. So, through application of many Gaussians, you reach an actual Gaussian distribution, typically indistinguishable from like just a unit Gaussian. And then given that you have this sort of noising schedule, you then train a neural network to basically undo this noise. So, given some slightly noisy image, say, XT plus one here, you train a neural network to undo one step of noise. So, to get you back to XT in this case. And the beauty of this is that if you can successfully train these models, then you can now sample from your known distribution, so your Gaussian distribution in this case, and you can then just feed pure noise into your model. And through sort of repeated steps of denoising, you eventually reach a distribution which closely matches the distribution that you trained on. So, in other words, you can start, you can feed in pure noise into your model and get out images in this case, which look really similar to the images that you trained your model on. And the reason we sort of wanted to train a diffusion model on proteins was that, you know, they're just a really natural fit for protein design. And that's for sort of three sort of main reasons, I think. So, you know, firstly, they can generate sort of basically endlessly diverse outputs. Because in that generation process, you're feeding in pure noise, so completely random noise. And actually then at each step of denoising, you're also adding more random noise. That, you know, basically every trajectory or every generation is novel and different, and therefore you can sample really diverse space of solutions. And this is good for protein design, where we want to sort of, you know, extensively sample the space of possible solutions. Secondly, at least in the way that we formulated this diffusion model, we can operate directly on amino acid coordinates. So, if we're building protein backbone, we're actually physically operating on the backbone coordinates. So, this is really beneficial for some of the applications that we'll discuss later in the talk. And then finally, in the image literature, they've been shown to be able to condition on a really wide range of inputs. And they can also be guided with sort of external auxiliary potentials. So, you know, you can basically have a model that can condition on really high resolution features, such as the structure of a functional motif or something, but also one sort of more sort of coarse grained, you know, make me an approximate fold kind of information. And then you can also guide this generation. So, this is useful in a design setting. So, you know, these kind of three things led us to really sort of, motivated us to train a diffusion model on protein backbone. So, you know, it kind of looks similar to that image I showed you of diffusion of images, but now we're starting from real protein structures, and then we iteratively add noise or corrupt these structures until we reach some known distribution. So, in our case, a 3D Gaussian distribution. And then we train a neural network to essentially undo that noise, such that we can now sample from that known 3D Gaussian distribution, feed that into our model and generate new protein structures. But, you know, training on protein backbone is challenging. It presents new challenges as compared to training on images. You know, protein backbones have these really strong constraints. Backbones comprise these four heavy atoms, the NCl for the C in the oxygen. They're connected by three covalent bonds, and obviously you have to be able to form a continuous chain to make a protein backbone. Also, then your backbone has to have a sequence that can encode it. So, you know, not all backbones are encodable by an amino acid sequence. So, you know, there are real challenges in training a diffusion model on protein structures. And therefore we kind of thought about, what is the best way to represent protein backbones in the simplest way to try and train a generative model on them? You know, you could think maybe you could just represent everything as atoms, but this is kind of complicated. So, instead we sort of turned to the way in which backbone residues are represented in both alpha fold and rosetta fold, which is this frame-based representation. So, this sort of takes advantage of the fact that the geometry of the NCl and C-alpha-C bonds in protein backbones is highly constrained, highly fixed, and the oxygen can then actually be derived, the position of the oxygen could be derived from the position of these sort of NCl-C frames. So, in other words, we can basically treat this NCl-C as a triangle, as a frame, which is then described purely by a translation and a rotation. So, we simplify these protein structures, these complex protein structures now, just into a set of like L translations and rotations where L is the length of the protein here. So, this is like a really efficient and simple way of representing protein backbones. And then the other challenge in sort of training a diffusion model on backbones is that now you're operating in three dimensions and you're, as I said, you know, these frames are represented by a translation and a rotation, and therefore we need to noise both of those modalities. So, when we add noise to our protein structures, we have to be able to noise both the position, so the translation of these frames, but also the orientation of these frames. So, specifically what we do is that we apply just 3D Gaussian noise to the C-alpha coordinates, the translation position, and then the rotations, we basically diffuse these, we apply Brownian motion on the manifold of rotation matrices, so this is known as SO3, and this is basically, yes, like Brownian motion on a sphere, you can think of it as. And that sort of looks like this movie shown on the right, where you start off with these sort of neat little triangles and then gradually these become sort of noisier and noisier, they're noise both in their translational and rotational component. And then the goal is, once we have this forward-noising process, we train a neural network to try and remove this noise, so to reverse this process. And just one final thing I need to sort of describe before we get into the guts of RF diffusion is this really nice property of diffusion models which really motivated the approach that we took here. So when we think about diffusion models, we often think about this reverse generative process as being sort of removing one step of noise at a time. But actually, this is really nice property that at least up to the scaling factor, there's an equivalent between whether you predict just one step of noise, so predict how to remove one step of noise, or whether you actually just directly predict the ground-proof image or protein structure. So it's equivalent up to the scaling factor to just predict the ground-truth protein structure and then back-calculate the next time step, or to predict just one step of noise. And this equivalence is really nice because this then motivated the approach that we took. So our specific approach was to take a structure prediction network, so Rosettefold in our case, and then fine-tune it to be the denoising network in a diffusion model. And we sort of theorize that this would be a good idea because the architecture of Rosettefold already sort of in some ways resembles that which would be required for a denoising network. It has SE3 equivalence. It then also takes these input sequences and input coordinates and predicts ground-truth protein structures. It's trying to predict the true protein structure. And therefore, we thought, well, this architecture and the inductive bias of a structure prediction network should really help us or act as a fantastic basis to be fine-tuned into a generative model, which we call RF diffusion. So in RF diffusion, we take this Rosettefold architecture and the pre-trained Rosettefold weights, and now we fine-tune it on this generative task. So what we do is that rather than providing input sequences, we now just mask that. So there's no or sometimes a small amount, but essentially just masked input sequence and then those noise input coordinates. So that forward-noising that we can apply, we can then just feed these in as coordinate inputs into Rosettefold. And then we just ask Rosettefold, given those two inputs, to predict a ground-truth structure, which we call X-naught hat in this case. So you can see that this architecture closely matches that of Rosettefold, and we therefore theorise that this would be an efficient way of training a generative model of protein backbones. So I won't go into all of the training details, but I'll just discuss some sort of high-level features of the training strategy of RF diffusion. So we train RF diffusion on the protein data bank. We train on basically most of the protein data bank. We limit ourselves to proteins less than 384 amino acids. We don't crop proteins at all. And these are just clustered by sequence similarity. Then I mentioned that in diffusion models, you're applying noise over a set number of time steps. In RF diffusion, this is 200 time steps, and this basically gets you to a purely random 3D Gaussian distribution of C alpha coordinates and a uniform distribution of your rotations. This then trains for a reasonably modest amount of compute, really. This trains on 8A100s for about four days. So given that this model is about 80 million parameters, this is pretty efficient for training such a big model. And then the two key things that I'll just go into a little bit more detail on are this concept of self-conditioning and then this concept of fine-tuning from a structured prediction network. So on this first point, this self-conditioning, this was really motivated by two things. So firstly, this was motivated by work in other diffusion literature, which shows that basically giving a model access to its prediction at the previous time step can help it make its next prediction. And this is conceptually similar to the concept of recycling in both alpha-fold and rosette-fold, where you give the structured prediction network multiple goes at making its structured prediction each time conditioned on its last prediction. So we theorize that we might be able to leverage this in RF diffusion. So specifically, what we do is that we take the X-naught hat prediction from a time step, and then we feed that in to the model during its next time step. So XT plus 1, we generate an X-naught hat, which we then feed into the model along with the coordinate input at time step T. And then the model can leverage both the position of the coordinates at the current time step and what it thought the ground truth structure was at the previous time step to make its next prediction. And this really markedly improves RF diffusion. So we're going to show you just a few plots that look like this throughout this talk. And basically, this is just benchmarking the performance of RF diffusion on a sort of mixed benchmark set. So this is split both into unconditional generations. So can you just make any protein? And then these functional motif scaffolding problems. So can you build the scaffold to support a particular bit of protein? And we'll go into more detail about this later in the talk. But basically, what we do is that we generate backbones. We find sequences with protein MPNN, and then we alpha fold them. And we ask, how closely does alpha fold recapitulate our design? So in these plots, the RMSD between alpha fold and design, the lower the better. That's the closer the recapitulation. And we also look at alpha fold confidence. So how confident alpha fold is in its prediction. This is the predicted alignment error. Lower is better in both cases. So what you can see here is that when we allow RF diffusion to self-condition, so to see what it predicted at previous time steps, it really markedly improves the performance. So you can see on unconditional generation, we're getting much better recapitulation by alpha fold. And also generally in functional motif scaffolding, we also get better recapitulation. And alpha fold is also more confident in this prediction. Then I mentioned that our whole approach was to fine tune a structured prediction network to try and directly leverage the sort of inductive bias of a structured prediction network to be the basis of a diffusion model. And this was indeed a good idea, I suppose. So we did as a control, try training RF diffusion from untrained weights. So we just randomly initialized weights in a reset fold RF diffusion and train it for an equivalent amount of compute. And you can see that in an equivalent amount of compute, you really are not very good at all. So that's shown in dark gray here across these tasks. You're really much, much worse than if you fine tune from pre-trained reset fold weights. And just really hammer home the point. If you just run protein MPNN on Gaussian noise, you basically achieve sort of a not much worse performance. So we are directly able to leverage this inductive bias in reset fold to fine tune a generative model. And then this is just a depiction of what proteins coming out of RF diffusion look like under these different conditions. So this is like the median sample by RMSD of the AlphaVolt model. And you can see that without pre-training, the model really struggles to learn in this amount of compute at least, the task at all. So the outputs just don't really look like proteins at all. When you don't, when you have pre-training, but you don't do self conditioning, you can kind of see that these proteins resemble proteins. You know, they have secondary structure, but they're just not well packed and they're not particularly diverse. But when we do self conditioning and we start from pre-trained reset fold weights, we get these just really nice structures. So this is the median 300 amino acid sample. You can see that it's well packed. It's like mixed apology and looks very nice. So I'll now hand over to David who will tell you about the experiments that we did characterizing RF diffusion. And yeah. Excellent. Yeah, so now that Joe's kind of introduced the thought process that went into the development of the model and sort of how we started to benchmark it in silico, when we started to see these like pretty good in silico benchmark results, it was now kind of time to test this out on and test real proteins and experimentally verify the proteins that this model is making. And so we tested experimentally, designs from a variety of problems which we considered as kind of limited in backbone generation. And so problems where if we had good backbone generation, we could have a really nice solution to these problems. And so the first thing that we went for is we just wanted to like see how far we could push unconditional backbone generation. And this is just sort of measuring how well could a model when it's given a lot of real estate in terms of amino acids, how well can it make just protein domains that fold nicely? And so as depicted here kind of on the top of this screen, as we can actually get really long proteins that when you encode them with a sequence with protein MPNN can be really nicely recapitulated with Alpha Fold. So you can see the Alpha Fold in color overlaid on the RF diffusion design model in gray. And we quantified this kind of on the bottom left plot here showing an Alpha Fold versus design RMSD as a function of length. And you can see that even up to say 400 amino acids, the median protein coming out of RF diffusion is exceptionally designable in the eyes of protein MPNN followed by Alpha Fold, which is really remarkable. And then even up to 600 amino acids, sort of in this length range, you can still find the occasional backbone that RF diffusion produces and then protein MPNN can successfully get Alpha Fold to recapitulate. And getting a high confidence Alpha Fold prediction with this low of RMSD and this confidence is quite a remarkable task, I would say. So this is really good. And so we wanted to kind of benchmark this against another state of the art method for generating backbones. So we chose to benchmark it against Rosetta Fold hallucination. And so you can see in both this shorter protein regime but also much more so in the long protein regime on this Alpha Fold versus design RMSD benchmark, RF diffusion kind of just knocks it out of the park in terms of staying low with Alpha Fold RMSD versus designs and it scales much better than Rosetta Fold hallucination which was really exciting. And then one thing that you've kind of always got to make sure of and verify is that these types of generative models, they're not just memorizing the dataset, they're actually modeling the whole distribution that you want to model. And so we checked this via the TM score. And so for a variety of lengths of proteins sampled from RF diffusion, these are not the Alpha Folds, these are just what RF diffusion was actually trying to build. We just measured the TM score, a similarity metric versus the entire training set in the PDB. And we can see basically across the board that RF diffusion backbones are very, they show very little similarity to anything that it was trained on in the PDB, which is really, really nice. So you can look at all of these designs and basically all of them are just brand new proteins which is really, really cool. So the next thing we wanted to see was, okay, can these unconditional proteins actually recapitulate some of the desirable de novo protein properties that Joe mentioned earlier. So expressing well, being very thermostable, folding well and that sort of thing. So we took just these unconditional 300 amino acid designs here on the left and ordered their sequences, expressed them in E. coli and then looked at their CD spectra and tried to melt them. And what we found was that they indeed exhibited these really nice properties. So they expressed it really high levels and they were well-folded according to CD spectra. And then they were impossible to melt up to boiling them. And for perspective, if you did see a protein melt in the CD melt spectra, you would see sort of the sigmoidal curve with the inflection point denoting where the melting temperature is. And we just don't see that with these unconditional designs. So they're really nicely behaved. So that's super exciting. And then another challenge that we wanted to address is the ability to make scaffold libraries and sample around a particular fold family. This is really useful for design methods that actually utilize large databases or large libraries of backbones, say where you're trying to design a small molecule binder or an enzyme and you need to scaffold, say, a constellation of side chains and you need a lot of backbones to look through. And so the way that we try to tackle this is inputting two pieces of information to the network. This block adjacency tensor, which describes just very coarse-grained or very roughly which secondary structure elements are close to each other in 3D space. And then the second piece of information is just actually encoding what's the secondary structure of each residue. And so these are two very easy things to compute and then send into a model. So we trained a new version of the model that then could condition its predictions, its predictions of X-naught hat on these pieces of information. And specifically, we input that in this 2D template track for Rosetta Fold to condition on. And so when we do this, this is just an example of a trajectory that comes out when you allow the network to condition on the family of folds from Tim Barrels. You can see a nice iterative trajectory that recapitulates a Tim Barrel fold by eye at least. And then here's just some more examples. So starting from this really perfect-looking Tim Barrel, we designed a few more. And importantly, what's cool, I think, is that you're sampling all different sort of confirmations and flexing of this backbone and sampling the whole distribution of Tim Barrel folds, presumably. And when we ordered these sequences, tested them, again, they expressed really nicely, they folded up and they didn't melt. And so that was just really nice to see that we could do this for a particular fold family. And we're not obviously limited to just the Tim Barrel family. We tried this with NTF-2s and this is just some really nice benchmark results from AlphaFold for these different NTF-2 designs. And what we saw for both the Tim Barrels and the NTF-2s and presumably other problems, we're trying to sample a particular fold family with these adjacency and secondary structure tensors, is that the rate of designs coming out of ARP diffusion that meet sort of orderable criteria, this in-silico success rate that we would consider an orderable protein is somewhere on the order of 40 to 50%. And so that's just an extremely efficient way to generate new samples from a fold family that was really exciting to see. Okay, so then the next thing we tried is, it would be really useful to be able to generate symmetric oligomers for all sorts of materials, design problems, vaccine design problems. And so the way that we approach this is taking the point cloud that you send into Rosettafold each time, ARP diffusion each time, take an asymmetric unit's worth of the residues from that Gaussian distribution and then apply all of the symmetry operators from whatever your point symmetry is that you're trying to generate proteins for. And so when you do this, the equivalents of the network will actually make it so that X naught hat, the prediction, is also really, really closely respecting this point symmetry. And then after your prediction and then you interpolate towards it, you get your next set of residues, a little less noisy, then you resymmetrize that again and repeat this process all the way until the final time step. And then what's really nice is that by definition, after this trajectory has been run, you will have a fold of protein that perfectly respects your point symmetry. And then you can just use any sequence design method you want, say protein MPN to symmetrically design the sequence for this oligomer. And this is kind of advantageous beyond previously known deep learning methods for making oligomers because it's sort of by definition constructing these rather than optimizing towards something that eventually becomes approximately symmetric. And so we can perform this protocol for cyclic and dihedral symmetries and really, really large oligomers. And we can have really high success rates in silico. And what's really exciting actually is that this method is able to produce with a really high success rate dihedral symmetries, dihedral oligomers, which were previously unreachable, say with hallucination. And so that was super exciting to see. And because we saw this, this high success rate in silico, we went on this like massive design campaign. So Helen Eisenach and Andrew Borscht just went kind of crazy and purified tons and tons of proteins and then experimentally verified their approximate structure with negative stain EM. And so you can see this in this column, here's the RF diffusion backbone, almost indistinguishable from this is the alpha fold prediction. And then here's some cyclics in the top and in the bottom, there's dihedral symmetries, 2D class averages and 3D reconstructions from negative stain electron microscopy verify their approximate shape and show us that these particles are actually taking on the shape that we expect from these oligomers. So that was just super cool to see. And with the help of Will Scheffler, actually expanded this to even icosahedral symmetries. And so kind of by modeling the minimal number of subunits and interfaces in an icosahedron, performing this diffusion trajectory and then symmetriesing it, performing MPNN, you can get again in this little square here, there's just a depiction of an alpha fold prediction of the C3 symmetric component or the C3 axis of the psychosahedral nanoparticle overlaid really nicely with the RF diffusion backbone. And then of course, experimentally, it was shown that these actually do form the desired particle and the 3D reconstruction was just dead on with the design model. So that was just super cool to see. So with like a few clicks of a button, we're now able to sometimes generate like icosahedral nanoparticles that fold up and work in the wet lab, which is really cool. Okay, so kind of another design challenge that is becoming a classic at this point is this functional motif scaffolding problem. And so the brief description of this is you have some functional motif from maybe a native protein or maybe it's something you've computed, but you either know or you believe it's going to be, it's going to possess some chemical function. And then the goal here is to take it out of this native protein and then find some other protein that's de novo that when it folds up, say this protein here on the right side, when it folds up, it actually recapitulates perfectly the structure of your motif and thus possesses the chemical function of that motif. And so we wanted to see if RF diffusion could perform well at this task. So on the right hand side here, we benchmark RF diffusion against Rosettafold inpainting and Rosettafold hallucination. And this is just a really broad range, all sorts of different motif scaffolding problems. And on the Y axis here, oops, on the Y axis here is just the in silico success rate of those benchmarks that Joe defined earlier. And what was really cool is just across the board for almost all of these challenges, RF diffusion is A, A getting like a reasonable success rate, in some cases really high success rates in silico for scaffolding these sites and also massively performing previously known methods for doing this. So that was really exciting to see. On the left hand side here is just a few motifs in green depicted alongside their RF diffusion backbones and alpha fold predictions in color for problems where it was really hard to get inpainting or Rosettafold hallucination to find backbones to solve this problem. And so, and the reason we care about these metrics for motif scaffolding is that like Joe said, when we see things passing in these really stringent metrics, they actually work in the wet lab. And so one of the benchmark problems was this, this P53 Helix scaffolding problem. And so this problem is important to try and scaffold the P53 Helix into some well-folded protein that binds MDM2 because if you could disrupt the interaction between the native P53 Helix and the MDM2 protein, it could serve as some cancer therapy. And so we set out to try and scaffold this Helix into well-folded proteins that could maybe interact even more strongly with MDM2 than the native P53 which has about a 600 nanomolar affinity. And in this swarm plot in the center here, you can see experimentally when we ordered a set of 96 designs that more than half of them actually bound with at least the affinity of the P53 Helix. And so that was just like really, really cool to see. And a couple of these were actually super tight binders, so less than one nanomolar KD. And so you could actually consider these sort of candidate therapeutics if you wanted for cancer therapy. And so that was just a really nice kind of design campaign and test case for motif scaffolding that we saw. And then another class of motif scaffolding that could be useful to address is this sort of symmetric motif scaffolding. So there's lots of naturally occurring symmetries and functional proteins, whether it be metal binding domains or say viral glycoproteins that are symmetric, it would be really nice to be able to scaffold symmetric proteins that present functional motifs. And so to explore this area, we just chose this metal binding case where we attempted to scaffold a nickel binding site with just four histidine residues, and we decided to attach them to just chunks of ideal helix. And so the goal here was actually, can you find an oligomer that scaffolds this C4 or this square planar nickel binding domain? And so through combining the motif scaffolding protocol with the symmetric oligomer generation protocol, you can actually then generate these trajectories that both find well-packed, nice-looking oligomers, but also that nicely thread through your motif here. And so you can see just an example of one of the alpha fold predictions in color overlaid on an RF diffusion design. And so we ran this a whole bunch of times and it was really cool and just striking to see the diversity of backbones that were coming out to try that could solve this problem, at least in silico. And so on the top here, you see a variety of structures. And what's really cool also is when you MPNM them and you alpha fold them, you get really, really nice close recapitulation even at the side chain level for your histidines. And so this gave us a lot of confidence in their ability to bind this nickel ion. And so we ordered 48 different designs and we had something like a 30 or 40% success rate at binding the metal. And it was really cool to see that Andrew Borscht helped us again here to verify their ligament structure approximately by negative stain EM and their symmetry. All of that looked good. And then it was really cool to see just super tight binders right out of the computer against nickel, both endothermic and exothermic, which was really cool. And then importantly in pink here, when you mutate away the histidines from these ligaments, you see complete abolishing of the binding signal. So it was really those histidines that were doing the binding to the nickel. And then this last class of problems we'll go over is this problem which is broadly relevant for therapeutics is therapeutics and diagnostics is protein binder design. So creating a protein that can bind to some target protein or a target peptide. And the way generally that we modeled this problem is sort of like a multi-chain motif or a scaffolding problem. So you just tell the network that, okay, the motif is going to be the target protein, the protein you're trying to bind to, and it's just gonna be in its own chain. And then the model's job is to design a second chain that's globular and that packs well against this protein. And so when we ran this protocol against a whole, I guess four therapeutically relevant targets, we saw really excitingly a couple or at least one order of magnitude improved experimental success against all of these targets that we had previously done design campaigns for with other methods and saw kind of you needed on the order of say 10,000 designs to actually find binders against. And here right off the bat, we were finding tens of percents of successes experimentally, which was just unheard of and that was really cool. And here's just a depiction of the strongest hits for each of these targets. And you can see a variety, I guess, of binding interactions and kind of ways that the model likes to dock folds against the targets, folds that the model comes up with. And it's just really cool that you can find in a batch of 96 designs now for hemagglutin and A binders, a 28 nanomolar binder where like I said, that used to take like thousands or tens of thousands of attempts, so really efficient again, design campaign for binders now. And this was really exciting. So we recently, Andrew Borscht was able to actually solve a cryoEM structure of our flu binder against flu hemagglutin and A and it was just great to see that it was like atomically accurate. So RF diffusion produced this binder and it was less than an angstrom off from the cryoEM structure. So kind of validating that the structures we see coming out of RF diffusion are often atomically accurate and we can trust them. And yeah, this was just really great to see. And then, yeah, so the last campaign, which was kind of led by Pretham and Susanna from our group was using RF diffusion to try and make binders to therapeutically or diagnostically relevant peptides. And so the protocol here is very similar to the binder design protocol. You just have your peptide structure input to the network and then try and have it build a fold around it. And often as you can kind of see from this movie, for at least these helical peptides, the model really liked to make these grooved scaffolds where the peptides just fit really nicely into the groove. And we thought that boated well and so after MP&N AlphaFold and ordering 96 designs, Pretham and Susanna and our coworkers found picomolar binders to both PTH and BIM peptides. And to our knowledge, this was the first time or the strongest binder to anything that has been computed and experimentally tested without any optimization in the wet lab. So that was really, really cool. And so kind of in conclusion, we've seen somewhat of a step function increase in our ability to make protein backbones with RF diffusion, which has just been so exciting to use and to see. And it pretty concretely, quantitatively outperforms previous methods computationally and experimentally across like a really wide range of tasks. And what's cool is that it really does leverage the advances in protein structure prediction, the architectures, the data representation, the training strategies and even the weights themselves. And so it's just really cool to see this kind of standing on the shoulders of advances in protein structure prediction. And finally, we're still working to try and improve this model even more. So I think there's a ton of different ways that we're continuing to explore, to try and improve it, sampling and training-wise. And so here's just kind of a sneak peek at yet another model that we've recently found and it does even better on our benchmarks in Silico than the bio-archive version of the model in pink. You can see on this RMSD AlphaFold versus Design benchmark, this new blue model in February has been even better. So that's exciting. And we're pretty much, I think we're gonna release, we're definitely gonna release the code when the paper's published. And then we'll probably release any models that we find that are better along with that code to just make sure everybody has access to the best model possible. And with that, that's the end of our talk and super special thanks to our co-authors on this paper. And it's been an exciting ride. And we would welcome any questions from you guys. And yeah, thanks for your attention.